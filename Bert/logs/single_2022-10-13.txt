[2022-10-13 14:33:01] - INFO: ≥…π¶µº»ÎBERT≈‰÷√Œƒº˛ F:\Desktop\◊‘»ª”Ô—‘¥¶¿Ì¥Û◊˜“µ\src\BertWithPretrained\bert_base_chinese\config.json
[2022-10-13 14:33:01] - INFO:  ### Ω´µ±«∞≈‰÷√¥Ú”°µΩ»’÷æŒƒº˛÷– 
[2022-10-13 14:33:01] - INFO: ###  project_dir = F:\Desktop\◊‘»ª”Ô—‘¥¶¿Ì¥Û◊˜“µ\src\BertWithPretrained
[2022-10-13 14:33:01] - INFO: ###  dataset_dir = F:\Desktop\◊‘»ª”Ô—‘¥¶¿Ì¥Û◊˜“µ\src\BertWithPretrained\data\SingleSentenceClassification
[2022-10-13 14:33:01] - INFO: ###  pretrained_model_dir = F:\Desktop\◊‘»ª”Ô—‘¥¶¿Ì¥Û◊˜“µ\src\BertWithPretrained\bert_base_chinese
[2022-10-13 14:33:01] - INFO: ###  vocab_path = F:\Desktop\◊‘»ª”Ô—‘¥¶¿Ì¥Û◊˜“µ\src\BertWithPretrained\bert_base_chinese\vocab.txt
[2022-10-13 14:33:01] - INFO: ###  device = cpu
[2022-10-13 14:33:01] - INFO: ###  train_file_path = F:\Desktop\◊‘»ª”Ô—‘¥¶¿Ì¥Û◊˜“µ\src\BertWithPretrained\data\SingleSentenceClassification\toutiao_train.txt
[2022-10-13 14:33:01] - INFO: ###  val_file_path = F:\Desktop\◊‘»ª”Ô—‘¥¶¿Ì¥Û◊˜“µ\src\BertWithPretrained\data\SingleSentenceClassification\toutiao_val.txt
[2022-10-13 14:33:01] - INFO: ###  test_file_path = F:\Desktop\◊‘»ª”Ô—‘¥¶¿Ì¥Û◊˜“µ\src\BertWithPretrained\data\SingleSentenceClassification\toutiao_test.txt
[2022-10-13 14:33:01] - INFO: ###  model_save_dir = F:\Desktop\◊‘»ª”Ô—‘¥¶¿Ì¥Û◊˜“µ\src\BertWithPretrained\cache
[2022-10-13 14:33:01] - INFO: ###  logs_save_dir = F:\Desktop\◊‘»ª”Ô—‘¥¶¿Ì¥Û◊˜“µ\src\BertWithPretrained\logs
[2022-10-13 14:33:01] - INFO: ###  split_sep = _!_
[2022-10-13 14:33:01] - INFO: ###  is_sample_shuffle = True
[2022-10-13 14:33:01] - INFO: ###  batch_size = 64
[2022-10-13 14:33:01] - INFO: ###  max_sen_len = None
[2022-10-13 14:33:01] - INFO: ###  num_labels = 15
[2022-10-13 14:33:01] - INFO: ###  epochs = 10
[2022-10-13 14:33:01] - INFO: ###  model_val_per_epoch = 2
[2022-10-13 14:33:01] - INFO: ###  vocab_size = 21128
[2022-10-13 14:33:01] - INFO: ###  hidden_size = 768
[2022-10-13 14:33:01] - INFO: ###  num_hidden_layers = 12
[2022-10-13 14:33:01] - INFO: ###  num_attention_heads = 12
[2022-10-13 14:33:01] - INFO: ###  hidden_act = gelu
[2022-10-13 14:33:01] - INFO: ###  intermediate_size = 3072
[2022-10-13 14:33:01] - INFO: ###  pad_token_id = 0
[2022-10-13 14:33:01] - INFO: ###  hidden_dropout_prob = 0.1
[2022-10-13 14:33:01] - INFO: ###  attention_probs_dropout_prob = 0.1
[2022-10-13 14:33:01] - INFO: ###  max_position_embeddings = 512
[2022-10-13 14:33:01] - INFO: ###  type_vocab_size = 2
[2022-10-13 14:33:01] - INFO: ###  initializer_range = 0.02
[2022-10-13 14:33:01] - INFO: ###  directionality = bidi
[2022-10-13 14:33:01] - INFO: ###  pooler_fc_size = 768
[2022-10-13 14:33:01] - INFO: ###  pooler_num_attention_heads = 12
[2022-10-13 14:33:01] - INFO: ###  pooler_num_fc_layers = 3
[2022-10-13 14:33:01] - INFO: ###  pooler_size_per_head = 128
[2022-10-13 14:33:01] - INFO: ###  pooler_type = first_token_transform
[2022-10-13 14:47:12] - INFO: ≥…π¶µº»ÎBERT≈‰÷√Œƒº˛ F:\Desktop\◊‘»ª”Ô—‘¥¶¿Ì¥Û◊˜“µ\src\BertWithPretrained\bert_base_chinese\config.json
[2022-10-13 14:47:12] - INFO:  ### Ω´µ±«∞≈‰÷√¥Ú”°µΩ»’÷æŒƒº˛÷– 
[2022-10-13 14:47:12] - INFO: ###  project_dir = F:\Desktop\◊‘»ª”Ô—‘¥¶¿Ì¥Û◊˜“µ\src\BertWithPretrained
[2022-10-13 14:47:12] - INFO: ###  dataset_dir = F:\Desktop\◊‘»ª”Ô—‘¥¶¿Ì¥Û◊˜“µ\src\BertWithPretrained\data\SingleSentenceClassification
[2022-10-13 14:47:12] - INFO: ###  pretrained_model_dir = F:\Desktop\◊‘»ª”Ô—‘¥¶¿Ì¥Û◊˜“µ\src\BertWithPretrained\bert_base_chinese
[2022-10-13 14:47:12] - INFO: ###  vocab_path = F:\Desktop\◊‘»ª”Ô—‘¥¶¿Ì¥Û◊˜“µ\src\BertWithPretrained\bert_base_chinese\vocab.txt
[2022-10-13 14:47:12] - INFO: ###  device = cpu
[2022-10-13 14:47:12] - INFO: ###  train_file_path = F:\Desktop\◊‘»ª”Ô—‘¥¶¿Ì¥Û◊˜“µ\src\BertWithPretrained\data\SingleSentenceClassification\toutiao_train.txt
[2022-10-13 14:47:12] - INFO: ###  val_file_path = F:\Desktop\◊‘»ª”Ô—‘¥¶¿Ì¥Û◊˜“µ\src\BertWithPretrained\data\SingleSentenceClassification\toutiao_val.txt
[2022-10-13 14:47:12] - INFO: ###  test_file_path = F:\Desktop\◊‘»ª”Ô—‘¥¶¿Ì¥Û◊˜“µ\src\BertWithPretrained\data\SingleSentenceClassification\toutiao_test.txt
[2022-10-13 14:47:12] - INFO: ###  model_save_dir = F:\Desktop\◊‘»ª”Ô—‘¥¶¿Ì¥Û◊˜“µ\src\BertWithPretrained\cache
[2022-10-13 14:47:12] - INFO: ###  logs_save_dir = F:\Desktop\◊‘»ª”Ô—‘¥¶¿Ì¥Û◊˜“µ\src\BertWithPretrained\logs
[2022-10-13 14:47:12] - INFO: ###  split_sep = _!_
[2022-10-13 14:47:12] - INFO: ###  is_sample_shuffle = True
[2022-10-13 14:47:12] - INFO: ###  batch_size = 64
[2022-10-13 14:47:12] - INFO: ###  max_sen_len = None
[2022-10-13 14:47:12] - INFO: ###  num_labels = 15
[2022-10-13 14:47:12] - INFO: ###  epochs = 10
[2022-10-13 14:47:12] - INFO: ###  model_val_per_epoch = 2
[2022-10-13 14:47:12] - INFO: ###  vocab_size = 21128
[2022-10-13 14:47:12] - INFO: ###  hidden_size = 768
[2022-10-13 14:47:12] - INFO: ###  num_hidden_layers = 12
[2022-10-13 14:47:12] - INFO: ###  num_attention_heads = 12
[2022-10-13 14:47:12] - INFO: ###  hidden_act = gelu
[2022-10-13 14:47:12] - INFO: ###  intermediate_size = 3072
[2022-10-13 14:47:12] - INFO: ###  pad_token_id = 0
[2022-10-13 14:47:12] - INFO: ###  hidden_dropout_prob = 0.1
[2022-10-13 14:47:12] - INFO: ###  attention_probs_dropout_prob = 0.1
[2022-10-13 14:47:12] - INFO: ###  max_position_embeddings = 512
[2022-10-13 14:47:12] - INFO: ###  type_vocab_size = 2
[2022-10-13 14:47:12] - INFO: ###  initializer_range = 0.02
[2022-10-13 14:47:12] - INFO: ###  directionality = bidi
[2022-10-13 14:47:12] - INFO: ###  pooler_fc_size = 768
[2022-10-13 14:47:12] - INFO: ###  pooler_num_attention_heads = 12
[2022-10-13 14:47:12] - INFO: ###  pooler_num_fc_layers = 3
[2022-10-13 14:47:12] - INFO: ###  pooler_size_per_head = 128
[2022-10-13 14:47:12] - INFO: ###  pooler_type = first_token_transform
[2022-10-13 14:47:14] - INFO: ## ◊¢“‚£¨’˝‘⁄ π”√±æµÿMyTransformer÷–µƒMyMultiHeadAttention µœ÷
[2022-10-13 14:47:14] - INFO: ª∫¥ÊŒƒº˛ F:\Desktop\◊‘»ª”Ô—‘¥¶¿Ì¥Û◊˜“µ\src\BertWithPretrained\data\SingleSentenceClassification\toutiao_test_None.pt ≤ª¥Ê‘⁄£¨÷ÿ–¬¥¶¿Ì≤¢ª∫¥Ê£°
[2022-10-13 14:47:28] - INFO: ª∫¥ÊŒƒº˛ F:\Desktop\◊‘»ª”Ô—‘¥¶¿Ì¥Û◊˜“µ\src\BertWithPretrained\data\SingleSentenceClassification\toutiao_train_None.pt ≤ª¥Ê‘⁄£¨÷ÿ–¬¥¶¿Ì≤¢ª∫¥Ê£°
[2022-10-13 14:49:05] - INFO: ª∫¥ÊŒƒº˛ F:\Desktop\◊‘»ª”Ô—‘¥¶¿Ì¥Û◊˜“µ\src\BertWithPretrained\data\SingleSentenceClassification\toutiao_val_None.pt ≤ª¥Ê‘⁄£¨÷ÿ–¬¥¶¿Ì≤¢ª∫¥Ê£°
[2022-10-13 14:49:47] - INFO: Epoch: 0, Batch[0/4186], Train loss :2.804, Train acc: 0.094
[2022-10-13 14:52:03] - INFO: Epoch: 0, Batch[10/4186], Train loss :2.204, Train acc: 0.391
[2022-10-13 15:15:38] - INFO: ÊàêÂäüÂØºÂÖ•BERTÈÖçÁΩÆÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/config.json
[2022-10-13 15:15:38] - INFO:  ### Â∞ÜÂΩìÂâçÈÖçÁΩÆÊâìÂç∞Âà∞Êó•ÂøóÊñá‰ª∂‰∏≠ 
[2022-10-13 15:15:38] - INFO: ###  project_dir = /home3/Tpandeng/mxy/BertWithPretrained
[2022-10-13 15:15:38] - INFO: ###  dataset_dir = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification
[2022-10-13 15:15:38] - INFO: ###  pretrained_model_dir = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese
[2022-10-13 15:15:38] - INFO: ###  vocab_path = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/vocab.txt
[2022-10-13 15:15:38] - INFO: ###  device = cuda:0
[2022-10-13 15:15:38] - INFO: ###  train_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_train.txt
[2022-10-13 15:15:38] - INFO: ###  val_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_val.txt
[2022-10-13 15:15:38] - INFO: ###  test_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_test.txt
[2022-10-13 15:15:38] - INFO: ###  model_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/cache
[2022-10-13 15:15:38] - INFO: ###  logs_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/logs
[2022-10-13 15:15:38] - INFO: ###  split_sep = _!_
[2022-10-13 15:15:38] - INFO: ###  is_sample_shuffle = True
[2022-10-13 15:15:38] - INFO: ###  batch_size = 64
[2022-10-13 15:15:38] - INFO: ###  max_sen_len = None
[2022-10-13 15:15:38] - INFO: ###  num_labels = 15
[2022-10-13 15:15:38] - INFO: ###  epochs = 10
[2022-10-13 15:15:38] - INFO: ###  model_val_per_epoch = 2
[2022-10-13 15:15:38] - INFO: ###  vocab_size = 21128
[2022-10-13 15:15:38] - INFO: ###  hidden_size = 768
[2022-10-13 15:15:38] - INFO: ###  num_hidden_layers = 12
[2022-10-13 15:15:38] - INFO: ###  num_attention_heads = 12
[2022-10-13 15:15:38] - INFO: ###  hidden_act = gelu
[2022-10-13 15:15:38] - INFO: ###  intermediate_size = 3072
[2022-10-13 15:15:38] - INFO: ###  pad_token_id = 0
[2022-10-13 15:15:38] - INFO: ###  hidden_dropout_prob = 0.1
[2022-10-13 15:15:38] - INFO: ###  attention_probs_dropout_prob = 0.1
[2022-10-13 15:15:38] - INFO: ###  max_position_embeddings = 512
[2022-10-13 15:15:38] - INFO: ###  type_vocab_size = 2
[2022-10-13 15:15:38] - INFO: ###  initializer_range = 0.02
[2022-10-13 15:15:38] - INFO: ###  directionality = bidi
[2022-10-13 15:15:38] - INFO: ###  pooler_fc_size = 768
[2022-10-13 15:15:38] - INFO: ###  pooler_num_attention_heads = 12
[2022-10-13 15:15:38] - INFO: ###  pooler_num_fc_layers = 3
[2022-10-13 15:15:38] - INFO: ###  pooler_size_per_head = 128
[2022-10-13 15:15:38] - INFO: ###  pooler_type = first_token_transform
[2022-10-13 15:15:41] - INFO: ## Ê≥®ÊÑèÔºåÊ≠£Âú®‰ΩøÁî®Êú¨Âú∞MyTransformer‰∏≠ÁöÑMyMultiHeadAttentionÂÆûÁé∞
[2022-10-13 15:15:48] - INFO: ÁºìÂ≠òÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_test_None.pt Â≠òÂú®ÔºåÁõ¥Êé•ËΩΩÂÖ•ÁºìÂ≠òÊñá‰ª∂ÔºÅ
[2022-10-13 15:15:51] - INFO: ÁºìÂ≠òÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_train_None.pt Â≠òÂú®ÔºåÁõ¥Êé•ËΩΩÂÖ•ÁºìÂ≠òÊñá‰ª∂ÔºÅ
[2022-10-13 15:16:12] - INFO: ÁºìÂ≠òÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_val_None.pt Â≠òÂú®ÔºåÁõ¥Êé•ËΩΩÂÖ•ÁºìÂ≠òÊñá‰ª∂ÔºÅ
[2022-10-13 15:16:23] - INFO: Epoch: 0, Batch[0/4186], Train loss :2.885, Train acc: 0.047
[2022-10-13 15:16:25] - INFO: Epoch: 0, Batch[10/4186], Train loss :2.113, Train acc: 0.531
[2022-10-13 15:16:27] - INFO: Epoch: 0, Batch[20/4186], Train loss :1.251, Train acc: 0.797
[2022-10-13 15:16:29] - INFO: Epoch: 0, Batch[30/4186], Train loss :1.002, Train acc: 0.766
[2022-10-13 15:16:32] - INFO: Epoch: 0, Batch[40/4186], Train loss :0.856, Train acc: 0.766
[2022-10-13 15:16:34] - INFO: Epoch: 0, Batch[50/4186], Train loss :0.812, Train acc: 0.797
[2022-10-13 15:16:36] - INFO: Epoch: 0, Batch[60/4186], Train loss :0.598, Train acc: 0.859
[2022-10-13 15:16:38] - INFO: Epoch: 0, Batch[70/4186], Train loss :0.553, Train acc: 0.875
[2022-10-13 15:16:40] - INFO: Epoch: 0, Batch[80/4186], Train loss :0.573, Train acc: 0.828
[2022-10-13 15:16:42] - INFO: Epoch: 0, Batch[90/4186], Train loss :0.576, Train acc: 0.844
[2022-10-13 15:16:45] - INFO: Epoch: 0, Batch[100/4186], Train loss :0.648, Train acc: 0.891
[2022-10-13 15:16:47] - INFO: Epoch: 0, Batch[110/4186], Train loss :0.491, Train acc: 0.922
[2022-10-13 15:16:49] - INFO: Epoch: 0, Batch[120/4186], Train loss :0.892, Train acc: 0.781
[2022-10-13 15:16:51] - INFO: Epoch: 0, Batch[130/4186], Train loss :0.756, Train acc: 0.797
[2022-10-13 15:16:54] - INFO: Epoch: 0, Batch[140/4186], Train loss :0.506, Train acc: 0.859
[2022-10-13 15:16:56] - INFO: Epoch: 0, Batch[150/4186], Train loss :0.723, Train acc: 0.781
[2022-10-13 15:16:58] - INFO: Epoch: 0, Batch[160/4186], Train loss :0.899, Train acc: 0.734
[2022-10-13 15:17:00] - INFO: Epoch: 0, Batch[170/4186], Train loss :0.773, Train acc: 0.781
[2022-10-13 15:17:03] - INFO: Epoch: 0, Batch[180/4186], Train loss :0.587, Train acc: 0.844
[2022-10-13 15:17:05] - INFO: Epoch: 0, Batch[190/4186], Train loss :0.724, Train acc: 0.781
[2022-10-13 15:17:07] - INFO: Epoch: 0, Batch[200/4186], Train loss :0.672, Train acc: 0.828
[2022-10-13 15:17:09] - INFO: Epoch: 0, Batch[210/4186], Train loss :0.592, Train acc: 0.844
[2022-10-13 15:17:12] - INFO: Epoch: 0, Batch[220/4186], Train loss :0.468, Train acc: 0.891
[2022-10-13 15:17:14] - INFO: Epoch: 0, Batch[230/4186], Train loss :0.915, Train acc: 0.766
[2022-10-13 15:17:16] - INFO: Epoch: 0, Batch[240/4186], Train loss :0.546, Train acc: 0.859
[2022-10-13 15:17:19] - INFO: Epoch: 0, Batch[250/4186], Train loss :0.536, Train acc: 0.844
[2022-10-13 15:17:21] - INFO: Epoch: 0, Batch[260/4186], Train loss :0.634, Train acc: 0.828
[2022-10-13 15:17:23] - INFO: Epoch: 0, Batch[270/4186], Train loss :0.503, Train acc: 0.844
[2022-10-13 15:17:25] - INFO: Epoch: 0, Batch[280/4186], Train loss :0.392, Train acc: 0.938
[2022-10-13 15:17:27] - INFO: Epoch: 0, Batch[290/4186], Train loss :0.474, Train acc: 0.875
[2022-10-13 15:17:30] - INFO: Epoch: 0, Batch[300/4186], Train loss :0.351, Train acc: 0.906
[2022-10-13 15:17:32] - INFO: Epoch: 0, Batch[310/4186], Train loss :0.362, Train acc: 0.922
[2022-10-13 15:17:34] - INFO: Epoch: 0, Batch[320/4186], Train loss :0.551, Train acc: 0.891
[2022-10-13 15:17:37] - INFO: Epoch: 0, Batch[330/4186], Train loss :0.575, Train acc: 0.844
[2022-10-13 15:17:39] - INFO: Epoch: 0, Batch[340/4186], Train loss :0.636, Train acc: 0.844
[2022-10-13 15:17:41] - INFO: Epoch: 0, Batch[350/4186], Train loss :0.589, Train acc: 0.844
[2022-10-13 15:17:44] - INFO: Epoch: 0, Batch[360/4186], Train loss :0.499, Train acc: 0.844
[2022-10-13 15:17:46] - INFO: Epoch: 0, Batch[370/4186], Train loss :0.721, Train acc: 0.781
[2022-10-13 15:17:48] - INFO: Epoch: 0, Batch[380/4186], Train loss :0.633, Train acc: 0.828
[2022-10-13 15:17:51] - INFO: Epoch: 0, Batch[390/4186], Train loss :0.329, Train acc: 0.938
[2022-10-13 15:17:53] - INFO: Epoch: 0, Batch[400/4186], Train loss :0.583, Train acc: 0.875
[2022-10-13 15:17:55] - INFO: Epoch: 0, Batch[410/4186], Train loss :0.410, Train acc: 0.891
[2022-10-13 15:17:58] - INFO: Epoch: 0, Batch[420/4186], Train loss :0.413, Train acc: 0.891
[2022-10-13 15:18:00] - INFO: Epoch: 0, Batch[430/4186], Train loss :0.884, Train acc: 0.750
[2022-10-13 15:18:02] - INFO: Epoch: 0, Batch[440/4186], Train loss :0.338, Train acc: 0.906
[2022-10-13 15:18:05] - INFO: Epoch: 0, Batch[450/4186], Train loss :0.356, Train acc: 0.875
[2022-10-13 15:18:07] - INFO: Epoch: 0, Batch[460/4186], Train loss :0.864, Train acc: 0.797
[2022-10-13 15:18:09] - INFO: Epoch: 0, Batch[470/4186], Train loss :0.485, Train acc: 0.812
[2022-10-13 15:18:12] - INFO: Epoch: 0, Batch[480/4186], Train loss :0.852, Train acc: 0.750
[2022-10-13 15:18:14] - INFO: Epoch: 0, Batch[490/4186], Train loss :0.871, Train acc: 0.750
[2022-10-13 15:18:17] - INFO: Epoch: 0, Batch[500/4186], Train loss :0.573, Train acc: 0.844
[2022-10-13 15:18:19] - INFO: Epoch: 0, Batch[510/4186], Train loss :0.491, Train acc: 0.859
[2022-10-13 15:18:21] - INFO: Epoch: 0, Batch[520/4186], Train loss :0.481, Train acc: 0.844
[2022-10-13 15:18:23] - INFO: Epoch: 0, Batch[530/4186], Train loss :0.782, Train acc: 0.766
[2022-10-13 15:18:26] - INFO: Epoch: 0, Batch[540/4186], Train loss :0.719, Train acc: 0.812
[2022-10-13 15:18:28] - INFO: Epoch: 0, Batch[550/4186], Train loss :0.367, Train acc: 0.859
[2022-10-13 15:18:30] - INFO: Epoch: 0, Batch[560/4186], Train loss :0.737, Train acc: 0.812
[2022-10-13 15:18:33] - INFO: Epoch: 0, Batch[570/4186], Train loss :0.579, Train acc: 0.828
[2022-10-13 15:18:35] - INFO: Epoch: 0, Batch[580/4186], Train loss :0.622, Train acc: 0.828
[2022-10-13 15:18:37] - INFO: Epoch: 0, Batch[590/4186], Train loss :0.556, Train acc: 0.828
[2022-10-13 15:18:40] - INFO: Epoch: 0, Batch[600/4186], Train loss :0.412, Train acc: 0.812
[2022-10-13 15:18:42] - INFO: Epoch: 0, Batch[610/4186], Train loss :0.734, Train acc: 0.812
[2022-10-13 15:18:44] - INFO: Epoch: 0, Batch[620/4186], Train loss :0.427, Train acc: 0.875
[2022-10-13 15:18:46] - INFO: Epoch: 0, Batch[630/4186], Train loss :0.625, Train acc: 0.797
[2022-10-13 15:18:49] - INFO: Epoch: 0, Batch[640/4186], Train loss :0.526, Train acc: 0.891
[2022-10-13 15:18:51] - INFO: Epoch: 0, Batch[650/4186], Train loss :0.434, Train acc: 0.875
[2022-10-13 15:18:53] - INFO: Epoch: 0, Batch[660/4186], Train loss :0.536, Train acc: 0.844
[2022-10-13 15:18:56] - INFO: Epoch: 0, Batch[670/4186], Train loss :0.376, Train acc: 0.906
[2022-10-13 15:18:58] - INFO: Epoch: 0, Batch[680/4186], Train loss :0.403, Train acc: 0.859
[2022-10-13 15:19:00] - INFO: Epoch: 0, Batch[690/4186], Train loss :0.451, Train acc: 0.875
[2022-10-13 15:19:02] - INFO: Epoch: 0, Batch[700/4186], Train loss :0.635, Train acc: 0.812
[2022-10-13 15:19:05] - INFO: Epoch: 0, Batch[710/4186], Train loss :0.419, Train acc: 0.859
[2022-10-13 15:19:07] - INFO: Epoch: 0, Batch[720/4186], Train loss :0.529, Train acc: 0.875
[2022-10-13 15:19:09] - INFO: Epoch: 0, Batch[730/4186], Train loss :0.797, Train acc: 0.797
[2022-10-13 15:19:12] - INFO: Epoch: 0, Batch[740/4186], Train loss :0.587, Train acc: 0.812
[2022-10-13 15:19:14] - INFO: Epoch: 0, Batch[750/4186], Train loss :0.427, Train acc: 0.891
[2022-10-13 15:19:16] - INFO: Epoch: 0, Batch[760/4186], Train loss :0.425, Train acc: 0.906
[2022-10-13 15:19:19] - INFO: Epoch: 0, Batch[770/4186], Train loss :0.323, Train acc: 0.875
[2022-10-13 15:19:21] - INFO: Epoch: 0, Batch[780/4186], Train loss :0.486, Train acc: 0.875
[2022-10-13 15:19:23] - INFO: Epoch: 0, Batch[790/4186], Train loss :0.372, Train acc: 0.891
[2022-10-13 15:19:26] - INFO: Epoch: 0, Batch[800/4186], Train loss :0.309, Train acc: 0.953
[2022-10-13 15:19:28] - INFO: Epoch: 0, Batch[810/4186], Train loss :0.694, Train acc: 0.781
[2022-10-13 15:19:30] - INFO: Epoch: 0, Batch[820/4186], Train loss :0.491, Train acc: 0.875
[2022-10-13 15:19:33] - INFO: Epoch: 0, Batch[830/4186], Train loss :0.562, Train acc: 0.844
[2022-10-13 15:19:35] - INFO: Epoch: 0, Batch[840/4186], Train loss :0.269, Train acc: 0.922
[2022-10-13 15:19:37] - INFO: Epoch: 0, Batch[850/4186], Train loss :0.849, Train acc: 0.812
[2022-10-13 15:19:39] - INFO: Epoch: 0, Batch[860/4186], Train loss :0.502, Train acc: 0.828
[2022-10-13 15:19:42] - INFO: Epoch: 0, Batch[870/4186], Train loss :0.496, Train acc: 0.812
[2022-10-13 15:19:44] - INFO: Epoch: 0, Batch[880/4186], Train loss :0.756, Train acc: 0.828
[2022-10-13 15:19:46] - INFO: Epoch: 0, Batch[890/4186], Train loss :0.348, Train acc: 0.922
[2022-10-13 15:19:49] - INFO: Epoch: 0, Batch[900/4186], Train loss :0.410, Train acc: 0.844
[2022-10-13 15:19:51] - INFO: Epoch: 0, Batch[910/4186], Train loss :0.659, Train acc: 0.781
[2022-10-13 15:19:53] - INFO: Epoch: 0, Batch[920/4186], Train loss :0.285, Train acc: 0.938
[2022-10-13 15:19:56] - INFO: Epoch: 0, Batch[930/4186], Train loss :0.522, Train acc: 0.859
[2022-10-13 15:19:58] - INFO: Epoch: 0, Batch[940/4186], Train loss :0.439, Train acc: 0.859
[2022-10-13 15:20:00] - INFO: Epoch: 0, Batch[950/4186], Train loss :0.392, Train acc: 0.844
[2022-10-13 15:20:03] - INFO: Epoch: 0, Batch[960/4186], Train loss :0.491, Train acc: 0.906
[2022-10-13 15:20:05] - INFO: Epoch: 0, Batch[970/4186], Train loss :0.500, Train acc: 0.891
[2022-10-13 15:20:07] - INFO: Epoch: 0, Batch[980/4186], Train loss :0.416, Train acc: 0.859
[2022-10-13 15:20:09] - INFO: Epoch: 0, Batch[990/4186], Train loss :0.371, Train acc: 0.906
[2022-10-13 15:20:12] - INFO: Epoch: 0, Batch[1000/4186], Train loss :0.564, Train acc: 0.844
[2022-10-13 15:20:14] - INFO: Epoch: 0, Batch[1010/4186], Train loss :0.494, Train acc: 0.812
[2022-10-13 15:20:16] - INFO: Epoch: 0, Batch[1020/4186], Train loss :0.468, Train acc: 0.875
[2022-10-13 15:20:19] - INFO: Epoch: 0, Batch[1030/4186], Train loss :0.561, Train acc: 0.859
[2022-10-13 15:20:21] - INFO: Epoch: 0, Batch[1040/4186], Train loss :0.470, Train acc: 0.844
[2022-10-13 15:20:23] - INFO: Epoch: 0, Batch[1050/4186], Train loss :0.412, Train acc: 0.891
[2022-10-13 15:20:25] - INFO: Epoch: 0, Batch[1060/4186], Train loss :0.465, Train acc: 0.844
[2022-10-13 15:20:28] - INFO: Epoch: 0, Batch[1070/4186], Train loss :0.542, Train acc: 0.812
[2022-10-13 15:20:30] - INFO: Epoch: 0, Batch[1080/4186], Train loss :0.735, Train acc: 0.781
[2022-10-13 15:20:32] - INFO: Epoch: 0, Batch[1090/4186], Train loss :0.324, Train acc: 0.922
[2022-10-13 15:20:35] - INFO: Epoch: 0, Batch[1100/4186], Train loss :0.337, Train acc: 0.922
[2022-10-13 15:20:37] - INFO: Epoch: 0, Batch[1110/4186], Train loss :0.376, Train acc: 0.906
[2022-10-13 15:20:39] - INFO: Epoch: 0, Batch[1120/4186], Train loss :0.595, Train acc: 0.844
[2022-10-13 15:20:42] - INFO: Epoch: 0, Batch[1130/4186], Train loss :0.460, Train acc: 0.875
[2022-10-13 15:20:44] - INFO: Epoch: 0, Batch[1140/4186], Train loss :0.502, Train acc: 0.812
[2022-10-13 15:20:46] - INFO: Epoch: 0, Batch[1150/4186], Train loss :0.817, Train acc: 0.797
[2022-10-13 15:20:48] - INFO: Epoch: 0, Batch[1160/4186], Train loss :0.685, Train acc: 0.766
[2022-10-13 15:20:51] - INFO: Epoch: 0, Batch[1170/4186], Train loss :0.434, Train acc: 0.906
[2022-10-13 15:20:53] - INFO: Epoch: 0, Batch[1180/4186], Train loss :0.616, Train acc: 0.812
[2022-10-13 15:20:55] - INFO: Epoch: 0, Batch[1190/4186], Train loss :0.621, Train acc: 0.828
[2022-10-13 15:20:58] - INFO: Epoch: 0, Batch[1200/4186], Train loss :0.441, Train acc: 0.844
[2022-10-13 15:21:00] - INFO: Epoch: 0, Batch[1210/4186], Train loss :0.405, Train acc: 0.859
[2022-10-13 15:21:02] - INFO: Epoch: 0, Batch[1220/4186], Train loss :0.403, Train acc: 0.875
[2022-10-13 15:21:05] - INFO: Epoch: 0, Batch[1230/4186], Train loss :0.412, Train acc: 0.875
[2022-10-13 15:21:07] - INFO: Epoch: 0, Batch[1240/4186], Train loss :0.826, Train acc: 0.812
[2022-10-13 15:21:09] - INFO: Epoch: 0, Batch[1250/4186], Train loss :0.316, Train acc: 0.875
[2022-10-13 15:21:12] - INFO: Epoch: 0, Batch[1260/4186], Train loss :0.505, Train acc: 0.875
[2022-10-13 15:21:14] - INFO: Epoch: 0, Batch[1270/4186], Train loss :0.355, Train acc: 0.891
[2022-10-13 15:21:16] - INFO: Epoch: 0, Batch[1280/4186], Train loss :0.438, Train acc: 0.844
[2022-10-13 15:21:19] - INFO: Epoch: 0, Batch[1290/4186], Train loss :0.347, Train acc: 0.875
[2022-10-13 15:21:21] - INFO: Epoch: 0, Batch[1300/4186], Train loss :0.461, Train acc: 0.844
[2022-10-13 15:21:23] - INFO: Epoch: 0, Batch[1310/4186], Train loss :0.718, Train acc: 0.828
[2022-10-13 15:21:26] - INFO: Epoch: 0, Batch[1320/4186], Train loss :0.272, Train acc: 0.891
[2022-10-13 15:21:28] - INFO: Epoch: 0, Batch[1330/4186], Train loss :0.382, Train acc: 0.891
[2022-10-13 15:21:30] - INFO: Epoch: 0, Batch[1340/4186], Train loss :0.602, Train acc: 0.844
[2022-10-13 15:21:33] - INFO: Epoch: 0, Batch[1350/4186], Train loss :0.412, Train acc: 0.859
[2022-10-13 15:21:35] - INFO: Epoch: 0, Batch[1360/4186], Train loss :0.542, Train acc: 0.797
[2022-10-13 15:21:37] - INFO: Epoch: 0, Batch[1370/4186], Train loss :0.359, Train acc: 0.906
[2022-10-13 15:21:40] - INFO: Epoch: 0, Batch[1380/4186], Train loss :0.583, Train acc: 0.812
[2022-10-13 15:21:42] - INFO: Epoch: 0, Batch[1390/4186], Train loss :0.808, Train acc: 0.781
[2022-10-13 15:21:44] - INFO: Epoch: 0, Batch[1400/4186], Train loss :0.710, Train acc: 0.812
[2022-10-13 15:21:46] - INFO: Epoch: 0, Batch[1410/4186], Train loss :0.392, Train acc: 0.906
[2022-10-13 15:21:49] - INFO: Epoch: 0, Batch[1420/4186], Train loss :0.495, Train acc: 0.875
[2022-10-13 15:21:51] - INFO: Epoch: 0, Batch[1430/4186], Train loss :0.600, Train acc: 0.812
[2022-10-13 15:21:53] - INFO: Epoch: 0, Batch[1440/4186], Train loss :0.373, Train acc: 0.875
[2022-10-13 15:21:55] - INFO: Epoch: 0, Batch[1450/4186], Train loss :0.448, Train acc: 0.875
[2022-10-13 15:21:58] - INFO: Epoch: 0, Batch[1460/4186], Train loss :0.480, Train acc: 0.859
[2022-10-13 15:22:00] - INFO: Epoch: 0, Batch[1470/4186], Train loss :0.369, Train acc: 0.922
[2022-10-13 15:22:02] - INFO: Epoch: 0, Batch[1480/4186], Train loss :0.387, Train acc: 0.922
[2022-10-13 15:22:05] - INFO: Epoch: 0, Batch[1490/4186], Train loss :0.352, Train acc: 0.922
[2022-10-13 15:22:07] - INFO: Epoch: 0, Batch[1500/4186], Train loss :0.373, Train acc: 0.906
[2022-10-13 15:22:10] - INFO: Epoch: 0, Batch[1510/4186], Train loss :0.368, Train acc: 0.906
[2022-10-13 15:22:12] - INFO: Epoch: 0, Batch[1520/4186], Train loss :0.505, Train acc: 0.844
[2022-10-13 15:22:15] - INFO: Epoch: 0, Batch[1530/4186], Train loss :0.468, Train acc: 0.859
[2022-10-13 15:22:17] - INFO: Epoch: 0, Batch[1540/4186], Train loss :0.445, Train acc: 0.906
[2022-10-13 15:22:19] - INFO: Epoch: 0, Batch[1550/4186], Train loss :0.423, Train acc: 0.906
[2022-10-13 15:22:22] - INFO: Epoch: 0, Batch[1560/4186], Train loss :0.493, Train acc: 0.844
[2022-10-13 15:22:24] - INFO: Epoch: 0, Batch[1570/4186], Train loss :0.750, Train acc: 0.812
[2022-10-13 15:22:26] - INFO: Epoch: 0, Batch[1580/4186], Train loss :0.752, Train acc: 0.781
[2022-10-13 15:22:28] - INFO: Epoch: 0, Batch[1590/4186], Train loss :0.532, Train acc: 0.844
[2022-10-13 15:22:31] - INFO: Epoch: 0, Batch[1600/4186], Train loss :0.434, Train acc: 0.844
[2022-10-13 15:22:33] - INFO: Epoch: 0, Batch[1610/4186], Train loss :0.309, Train acc: 0.922
[2022-10-13 15:22:36] - INFO: Epoch: 0, Batch[1620/4186], Train loss :0.481, Train acc: 0.844
[2022-10-13 15:22:38] - INFO: Epoch: 0, Batch[1630/4186], Train loss :0.606, Train acc: 0.797
[2022-10-13 15:22:40] - INFO: Epoch: 0, Batch[1640/4186], Train loss :0.471, Train acc: 0.828
[2022-10-13 15:22:42] - INFO: Epoch: 0, Batch[1650/4186], Train loss :0.575, Train acc: 0.844
[2022-10-13 15:22:45] - INFO: Epoch: 0, Batch[1660/4186], Train loss :0.373, Train acc: 0.859
[2022-10-13 15:22:47] - INFO: Epoch: 0, Batch[1670/4186], Train loss :0.688, Train acc: 0.828
[2022-10-13 15:22:50] - INFO: Epoch: 0, Batch[1680/4186], Train loss :0.517, Train acc: 0.859
[2022-10-13 15:22:52] - INFO: Epoch: 0, Batch[1690/4186], Train loss :0.346, Train acc: 0.922
[2022-10-13 15:22:54] - INFO: Epoch: 0, Batch[1700/4186], Train loss :0.299, Train acc: 0.938
[2022-10-13 15:22:56] - INFO: Epoch: 0, Batch[1710/4186], Train loss :0.340, Train acc: 0.875
[2022-10-13 15:22:59] - INFO: Epoch: 0, Batch[1720/4186], Train loss :0.289, Train acc: 0.922
[2022-10-13 15:23:01] - INFO: Epoch: 0, Batch[1730/4186], Train loss :0.442, Train acc: 0.875
[2022-10-13 15:23:03] - INFO: Epoch: 0, Batch[1740/4186], Train loss :0.707, Train acc: 0.812
[2022-10-13 15:23:05] - INFO: Epoch: 0, Batch[1750/4186], Train loss :0.444, Train acc: 0.891
[2022-10-13 15:23:08] - INFO: Epoch: 0, Batch[1760/4186], Train loss :0.260, Train acc: 0.938
[2022-10-13 15:23:10] - INFO: Epoch: 0, Batch[1770/4186], Train loss :0.531, Train acc: 0.859
[2022-10-13 15:23:12] - INFO: Epoch: 0, Batch[1780/4186], Train loss :0.306, Train acc: 0.906
[2022-10-13 15:23:15] - INFO: Epoch: 0, Batch[1790/4186], Train loss :0.598, Train acc: 0.828
[2022-10-13 15:23:17] - INFO: Epoch: 0, Batch[1800/4186], Train loss :0.573, Train acc: 0.844
[2022-10-13 15:23:19] - INFO: Epoch: 0, Batch[1810/4186], Train loss :0.609, Train acc: 0.828
[2022-10-13 15:23:22] - INFO: Epoch: 0, Batch[1820/4186], Train loss :0.317, Train acc: 0.906
[2022-10-13 15:23:24] - INFO: Epoch: 0, Batch[1830/4186], Train loss :0.471, Train acc: 0.859
[2022-10-13 15:23:26] - INFO: Epoch: 0, Batch[1840/4186], Train loss :0.264, Train acc: 0.938
[2022-10-13 15:23:29] - INFO: Epoch: 0, Batch[1850/4186], Train loss :0.346, Train acc: 0.859
[2022-10-13 15:23:31] - INFO: Epoch: 0, Batch[1860/4186], Train loss :0.497, Train acc: 0.875
[2022-10-13 15:23:33] - INFO: Epoch: 0, Batch[1870/4186], Train loss :0.174, Train acc: 0.953
[2022-10-13 15:23:35] - INFO: Epoch: 0, Batch[1880/4186], Train loss :0.538, Train acc: 0.859
[2022-10-13 15:23:38] - INFO: Epoch: 0, Batch[1890/4186], Train loss :0.322, Train acc: 0.938
[2022-10-13 15:23:40] - INFO: Epoch: 0, Batch[1900/4186], Train loss :0.357, Train acc: 0.906
[2022-10-13 15:23:43] - INFO: Epoch: 0, Batch[1910/4186], Train loss :0.618, Train acc: 0.828
[2022-10-13 15:23:45] - INFO: Epoch: 0, Batch[1920/4186], Train loss :0.410, Train acc: 0.859
[2022-10-13 15:23:47] - INFO: Epoch: 0, Batch[1930/4186], Train loss :0.663, Train acc: 0.828
[2022-10-13 15:23:50] - INFO: Epoch: 0, Batch[1940/4186], Train loss :0.289, Train acc: 0.875
[2022-10-13 15:23:52] - INFO: Epoch: 0, Batch[1950/4186], Train loss :0.472, Train acc: 0.906
[2022-10-13 15:23:55] - INFO: Epoch: 0, Batch[1960/4186], Train loss :0.382, Train acc: 0.875
[2022-10-13 15:23:57] - INFO: Epoch: 0, Batch[1970/4186], Train loss :0.468, Train acc: 0.875
[2022-10-13 15:23:59] - INFO: Epoch: 0, Batch[1980/4186], Train loss :0.392, Train acc: 0.844
[2022-10-13 15:24:02] - INFO: Epoch: 0, Batch[1990/4186], Train loss :0.600, Train acc: 0.844
[2022-10-13 15:24:04] - INFO: Epoch: 0, Batch[2000/4186], Train loss :0.505, Train acc: 0.875
[2022-10-13 15:24:07] - INFO: Epoch: 0, Batch[2010/4186], Train loss :0.476, Train acc: 0.891
[2022-10-13 15:24:09] - INFO: Epoch: 0, Batch[2020/4186], Train loss :0.317, Train acc: 0.891
[2022-10-13 15:24:11] - INFO: Epoch: 0, Batch[2030/4186], Train loss :0.466, Train acc: 0.906
[2022-10-13 15:24:13] - INFO: Epoch: 0, Batch[2040/4186], Train loss :0.689, Train acc: 0.828
[2022-10-13 15:24:15] - INFO: Epoch: 0, Batch[2050/4186], Train loss :0.470, Train acc: 0.844
[2022-10-13 15:24:18] - INFO: Epoch: 0, Batch[2060/4186], Train loss :0.195, Train acc: 0.953
[2022-10-13 15:24:20] - INFO: Epoch: 0, Batch[2070/4186], Train loss :0.400, Train acc: 0.828
[2022-10-13 15:24:22] - INFO: Epoch: 0, Batch[2080/4186], Train loss :0.409, Train acc: 0.844
[2022-10-13 15:24:25] - INFO: Epoch: 0, Batch[2090/4186], Train loss :0.530, Train acc: 0.859
[2022-10-13 15:24:27] - INFO: Epoch: 0, Batch[2100/4186], Train loss :0.298, Train acc: 0.906
[2022-10-13 15:24:30] - INFO: Epoch: 0, Batch[2110/4186], Train loss :0.263, Train acc: 0.938
[2022-10-13 15:24:32] - INFO: Epoch: 0, Batch[2120/4186], Train loss :0.239, Train acc: 0.906
[2022-10-13 15:24:35] - INFO: Epoch: 0, Batch[2130/4186], Train loss :0.669, Train acc: 0.797
[2022-10-13 15:24:37] - INFO: Epoch: 0, Batch[2140/4186], Train loss :0.462, Train acc: 0.844
[2022-10-13 15:24:39] - INFO: Epoch: 0, Batch[2150/4186], Train loss :0.519, Train acc: 0.891
[2022-10-13 15:24:42] - INFO: Epoch: 0, Batch[2160/4186], Train loss :0.507, Train acc: 0.875
[2022-10-13 15:24:44] - INFO: Epoch: 0, Batch[2170/4186], Train loss :0.407, Train acc: 0.891
[2022-10-13 15:24:46] - INFO: Epoch: 0, Batch[2180/4186], Train loss :0.585, Train acc: 0.859
[2022-10-13 15:24:49] - INFO: Epoch: 0, Batch[2190/4186], Train loss :0.629, Train acc: 0.844
[2022-10-13 15:24:51] - INFO: Epoch: 0, Batch[2200/4186], Train loss :0.375, Train acc: 0.891
[2022-10-13 15:24:53] - INFO: Epoch: 0, Batch[2210/4186], Train loss :0.349, Train acc: 0.922
[2022-10-13 15:24:56] - INFO: Epoch: 0, Batch[2220/4186], Train loss :0.360, Train acc: 0.875
[2022-10-13 15:24:58] - INFO: Epoch: 0, Batch[2230/4186], Train loss :0.617, Train acc: 0.844
[2022-10-13 15:25:01] - INFO: Epoch: 0, Batch[2240/4186], Train loss :0.594, Train acc: 0.828
[2022-10-13 15:25:03] - INFO: Epoch: 0, Batch[2250/4186], Train loss :0.638, Train acc: 0.797
[2022-10-13 15:25:05] - INFO: Epoch: 0, Batch[2260/4186], Train loss :0.418, Train acc: 0.859
[2022-10-13 15:25:08] - INFO: Epoch: 0, Batch[2270/4186], Train loss :0.377, Train acc: 0.875
[2022-10-13 15:25:10] - INFO: Epoch: 0, Batch[2280/4186], Train loss :0.419, Train acc: 0.891
[2022-10-13 15:25:12] - INFO: Epoch: 0, Batch[2290/4186], Train loss :0.301, Train acc: 0.938
[2022-10-13 15:25:15] - INFO: Epoch: 0, Batch[2300/4186], Train loss :0.186, Train acc: 0.938
[2022-10-13 15:25:17] - INFO: Epoch: 0, Batch[2310/4186], Train loss :0.488, Train acc: 0.844
[2022-10-13 15:25:19] - INFO: Epoch: 0, Batch[2320/4186], Train loss :0.450, Train acc: 0.859
[2022-10-13 15:25:22] - INFO: Epoch: 0, Batch[2330/4186], Train loss :0.278, Train acc: 0.906
[2022-10-13 15:25:24] - INFO: Epoch: 0, Batch[2340/4186], Train loss :0.416, Train acc: 0.906
[2022-10-13 15:25:26] - INFO: Epoch: 0, Batch[2350/4186], Train loss :0.564, Train acc: 0.844
[2022-10-13 15:25:29] - INFO: Epoch: 0, Batch[2360/4186], Train loss :0.615, Train acc: 0.781
[2022-10-13 15:25:31] - INFO: Epoch: 0, Batch[2370/4186], Train loss :0.553, Train acc: 0.828
[2022-10-13 15:25:33] - INFO: Epoch: 0, Batch[2380/4186], Train loss :0.310, Train acc: 0.891
[2022-10-13 15:25:36] - INFO: Epoch: 0, Batch[2390/4186], Train loss :0.408, Train acc: 0.844
[2022-10-13 15:25:38] - INFO: Epoch: 0, Batch[2400/4186], Train loss :0.610, Train acc: 0.797
[2022-10-13 15:25:40] - INFO: Epoch: 0, Batch[2410/4186], Train loss :0.460, Train acc: 0.828
[2022-10-13 15:25:43] - INFO: Epoch: 0, Batch[2420/4186], Train loss :0.804, Train acc: 0.797
[2022-10-13 15:25:45] - INFO: Epoch: 0, Batch[2430/4186], Train loss :0.245, Train acc: 0.938
[2022-10-13 15:25:47] - INFO: Epoch: 0, Batch[2440/4186], Train loss :0.424, Train acc: 0.875
[2022-10-13 15:25:50] - INFO: Epoch: 0, Batch[2450/4186], Train loss :0.553, Train acc: 0.844
[2022-10-13 15:25:52] - INFO: Epoch: 0, Batch[2460/4186], Train loss :0.345, Train acc: 0.891
[2022-10-13 15:25:55] - INFO: Epoch: 0, Batch[2470/4186], Train loss :0.248, Train acc: 0.922
[2022-10-13 15:25:57] - INFO: Epoch: 0, Batch[2480/4186], Train loss :0.417, Train acc: 0.859
[2022-10-13 15:25:59] - INFO: Epoch: 0, Batch[2490/4186], Train loss :0.666, Train acc: 0.797
[2022-10-13 15:26:01] - INFO: Epoch: 0, Batch[2500/4186], Train loss :0.423, Train acc: 0.859
[2022-10-13 15:26:04] - INFO: Epoch: 0, Batch[2510/4186], Train loss :0.217, Train acc: 0.953
[2022-10-13 15:26:06] - INFO: Epoch: 0, Batch[2520/4186], Train loss :0.486, Train acc: 0.828
[2022-10-13 15:26:08] - INFO: Epoch: 0, Batch[2530/4186], Train loss :0.483, Train acc: 0.844
[2022-10-13 15:26:11] - INFO: Epoch: 0, Batch[2540/4186], Train loss :0.249, Train acc: 0.953
[2022-10-13 15:26:13] - INFO: Epoch: 0, Batch[2550/4186], Train loss :0.553, Train acc: 0.797
[2022-10-13 15:26:15] - INFO: Epoch: 0, Batch[2560/4186], Train loss :0.641, Train acc: 0.859
[2022-10-13 15:26:18] - INFO: Epoch: 0, Batch[2570/4186], Train loss :0.563, Train acc: 0.828
[2022-10-13 15:26:20] - INFO: Epoch: 0, Batch[2580/4186], Train loss :0.298, Train acc: 0.906
[2022-10-13 15:26:22] - INFO: Epoch: 0, Batch[2590/4186], Train loss :0.477, Train acc: 0.859
[2022-10-13 15:26:24] - INFO: Epoch: 0, Batch[2600/4186], Train loss :0.466, Train acc: 0.859
[2022-10-13 15:26:27] - INFO: Epoch: 0, Batch[2610/4186], Train loss :0.616, Train acc: 0.859
[2022-10-13 15:26:29] - INFO: Epoch: 0, Batch[2620/4186], Train loss :0.579, Train acc: 0.859
[2022-10-13 15:26:32] - INFO: Epoch: 0, Batch[2630/4186], Train loss :0.494, Train acc: 0.844
[2022-10-13 15:26:34] - INFO: Epoch: 0, Batch[2640/4186], Train loss :0.469, Train acc: 0.859
[2022-10-13 15:26:36] - INFO: Epoch: 0, Batch[2650/4186], Train loss :0.443, Train acc: 0.875
[2022-10-13 15:26:39] - INFO: Epoch: 0, Batch[2660/4186], Train loss :0.432, Train acc: 0.859
[2022-10-13 15:26:41] - INFO: Epoch: 0, Batch[2670/4186], Train loss :0.570, Train acc: 0.844
[2022-10-13 15:26:43] - INFO: Epoch: 0, Batch[2680/4186], Train loss :0.414, Train acc: 0.906
[2022-10-13 15:26:45] - INFO: Epoch: 0, Batch[2690/4186], Train loss :0.488, Train acc: 0.812
[2022-10-13 15:26:48] - INFO: Epoch: 0, Batch[2700/4186], Train loss :0.314, Train acc: 0.875
[2022-10-13 15:26:50] - INFO: Epoch: 0, Batch[2710/4186], Train loss :0.318, Train acc: 0.906
[2022-10-13 15:26:52] - INFO: Epoch: 0, Batch[2720/4186], Train loss :0.424, Train acc: 0.906
[2022-10-13 15:26:55] - INFO: Epoch: 0, Batch[2730/4186], Train loss :0.364, Train acc: 0.922
[2022-10-13 15:26:57] - INFO: Epoch: 0, Batch[2740/4186], Train loss :0.154, Train acc: 0.938
[2022-10-13 15:26:59] - INFO: Epoch: 0, Batch[2750/4186], Train loss :0.482, Train acc: 0.875
[2022-10-13 15:27:01] - INFO: Epoch: 0, Batch[2760/4186], Train loss :0.415, Train acc: 0.859
[2022-10-13 15:27:04] - INFO: Epoch: 0, Batch[2770/4186], Train loss :0.403, Train acc: 0.906
[2022-10-13 15:27:06] - INFO: Epoch: 0, Batch[2780/4186], Train loss :0.400, Train acc: 0.844
[2022-10-13 15:27:08] - INFO: Epoch: 0, Batch[2790/4186], Train loss :0.695, Train acc: 0.828
[2022-10-13 15:27:11] - INFO: Epoch: 0, Batch[2800/4186], Train loss :0.492, Train acc: 0.906
[2022-10-13 15:27:13] - INFO: Epoch: 0, Batch[2810/4186], Train loss :0.656, Train acc: 0.812
[2022-10-13 15:27:15] - INFO: Epoch: 0, Batch[2820/4186], Train loss :0.247, Train acc: 0.953
[2022-10-13 15:27:17] - INFO: Epoch: 0, Batch[2830/4186], Train loss :0.598, Train acc: 0.844
[2022-10-13 15:27:20] - INFO: Epoch: 0, Batch[2840/4186], Train loss :0.302, Train acc: 0.859
[2022-10-13 15:27:22] - INFO: Epoch: 0, Batch[2850/4186], Train loss :0.402, Train acc: 0.859
[2022-10-13 15:27:24] - INFO: Epoch: 0, Batch[2860/4186], Train loss :0.364, Train acc: 0.891
[2022-10-13 15:27:27] - INFO: Epoch: 0, Batch[2870/4186], Train loss :0.410, Train acc: 0.875
[2022-10-13 15:27:29] - INFO: Epoch: 0, Batch[2880/4186], Train loss :0.649, Train acc: 0.828
[2022-10-13 15:27:31] - INFO: Epoch: 0, Batch[2890/4186], Train loss :0.307, Train acc: 0.906
[2022-10-13 15:27:34] - INFO: Epoch: 0, Batch[2900/4186], Train loss :0.291, Train acc: 0.922
[2022-10-13 15:27:36] - INFO: Epoch: 0, Batch[2910/4186], Train loss :0.488, Train acc: 0.875
[2022-10-13 15:27:38] - INFO: Epoch: 0, Batch[2920/4186], Train loss :0.326, Train acc: 0.906
[2022-10-13 15:27:41] - INFO: Epoch: 0, Batch[2930/4186], Train loss :0.376, Train acc: 0.891
[2022-10-13 15:27:43] - INFO: Epoch: 0, Batch[2940/4186], Train loss :0.527, Train acc: 0.844
[2022-10-13 15:27:45] - INFO: Epoch: 0, Batch[2950/4186], Train loss :0.687, Train acc: 0.859
[2022-10-13 15:27:48] - INFO: Epoch: 0, Batch[2960/4186], Train loss :0.496, Train acc: 0.844
[2022-10-13 15:27:50] - INFO: Epoch: 0, Batch[2970/4186], Train loss :0.551, Train acc: 0.859
[2022-10-13 15:27:52] - INFO: Epoch: 0, Batch[2980/4186], Train loss :0.334, Train acc: 0.906
[2022-10-13 15:27:54] - INFO: Epoch: 0, Batch[2990/4186], Train loss :0.491, Train acc: 0.859
[2022-10-13 15:27:57] - INFO: Epoch: 0, Batch[3000/4186], Train loss :0.515, Train acc: 0.812
[2022-10-13 15:27:59] - INFO: Epoch: 0, Batch[3010/4186], Train loss :0.511, Train acc: 0.844
[2022-10-13 15:28:02] - INFO: Epoch: 0, Batch[3020/4186], Train loss :0.432, Train acc: 0.859
[2022-10-13 15:28:04] - INFO: Epoch: 0, Batch[3030/4186], Train loss :0.724, Train acc: 0.812
[2022-10-13 15:28:06] - INFO: Epoch: 0, Batch[3040/4186], Train loss :0.383, Train acc: 0.906
[2022-10-13 15:28:08] - INFO: Epoch: 0, Batch[3050/4186], Train loss :0.306, Train acc: 0.938
[2022-10-13 15:28:11] - INFO: Epoch: 0, Batch[3060/4186], Train loss :0.166, Train acc: 0.953
[2022-10-13 15:28:13] - INFO: Epoch: 0, Batch[3070/4186], Train loss :0.224, Train acc: 0.938
[2022-10-13 15:28:15] - INFO: Epoch: 0, Batch[3080/4186], Train loss :0.585, Train acc: 0.781
[2022-10-13 15:28:18] - INFO: Epoch: 0, Batch[3090/4186], Train loss :0.462, Train acc: 0.891
[2022-10-13 15:28:20] - INFO: Epoch: 0, Batch[3100/4186], Train loss :0.626, Train acc: 0.844
[2022-10-13 15:28:22] - INFO: Epoch: 0, Batch[3110/4186], Train loss :0.436, Train acc: 0.875
[2022-10-13 15:28:25] - INFO: Epoch: 0, Batch[3120/4186], Train loss :0.514, Train acc: 0.875
[2022-10-13 15:28:27] - INFO: Epoch: 0, Batch[3130/4186], Train loss :0.493, Train acc: 0.844
[2022-10-13 15:28:29] - INFO: Epoch: 0, Batch[3140/4186], Train loss :0.600, Train acc: 0.875
[2022-10-13 15:28:32] - INFO: Epoch: 0, Batch[3150/4186], Train loss :0.402, Train acc: 0.922
[2022-10-13 15:28:34] - INFO: Epoch: 0, Batch[3160/4186], Train loss :0.471, Train acc: 0.859
[2022-10-13 15:28:36] - INFO: Epoch: 0, Batch[3170/4186], Train loss :0.294, Train acc: 0.906
[2022-10-13 15:28:39] - INFO: Epoch: 0, Batch[3180/4186], Train loss :0.349, Train acc: 0.891
[2022-10-13 15:28:41] - INFO: Epoch: 0, Batch[3190/4186], Train loss :0.429, Train acc: 0.875
[2022-10-13 15:28:43] - INFO: Epoch: 0, Batch[3200/4186], Train loss :0.639, Train acc: 0.828
[2022-10-13 15:28:46] - INFO: Epoch: 0, Batch[3210/4186], Train loss :0.767, Train acc: 0.844
[2022-10-13 15:28:48] - INFO: Epoch: 0, Batch[3220/4186], Train loss :0.354, Train acc: 0.922
[2022-10-13 15:28:50] - INFO: Epoch: 0, Batch[3230/4186], Train loss :0.405, Train acc: 0.875
[2022-10-13 15:28:53] - INFO: Epoch: 0, Batch[3240/4186], Train loss :0.198, Train acc: 0.938
[2022-10-13 15:28:55] - INFO: Epoch: 0, Batch[3250/4186], Train loss :0.284, Train acc: 0.906
[2022-10-13 15:28:58] - INFO: Epoch: 0, Batch[3260/4186], Train loss :0.332, Train acc: 0.875
[2022-10-13 15:29:00] - INFO: Epoch: 0, Batch[3270/4186], Train loss :0.355, Train acc: 0.891
[2022-10-13 15:29:03] - INFO: Epoch: 0, Batch[3280/4186], Train loss :0.548, Train acc: 0.875
[2022-10-13 15:29:05] - INFO: Epoch: 0, Batch[3290/4186], Train loss :0.442, Train acc: 0.828
[2022-10-13 15:29:07] - INFO: Epoch: 0, Batch[3300/4186], Train loss :0.416, Train acc: 0.891
[2022-10-13 15:29:09] - INFO: Epoch: 0, Batch[3310/4186], Train loss :0.611, Train acc: 0.812
[2022-10-13 15:29:12] - INFO: Epoch: 0, Batch[3320/4186], Train loss :0.521, Train acc: 0.828
[2022-10-13 15:29:14] - INFO: Epoch: 0, Batch[3330/4186], Train loss :0.373, Train acc: 0.875
[2022-10-13 15:29:16] - INFO: Epoch: 0, Batch[3340/4186], Train loss :0.364, Train acc: 0.922
[2022-10-13 15:29:19] - INFO: Epoch: 0, Batch[3350/4186], Train loss :0.221, Train acc: 0.953
[2022-10-13 15:29:21] - INFO: Epoch: 0, Batch[3360/4186], Train loss :0.318, Train acc: 0.891
[2022-10-13 15:29:23] - INFO: Epoch: 0, Batch[3370/4186], Train loss :0.213, Train acc: 0.953
[2022-10-13 15:29:26] - INFO: Epoch: 0, Batch[3380/4186], Train loss :0.233, Train acc: 0.969
[2022-10-13 15:29:28] - INFO: Epoch: 0, Batch[3390/4186], Train loss :0.739, Train acc: 0.781
[2022-10-13 15:29:31] - INFO: Epoch: 0, Batch[3400/4186], Train loss :0.290, Train acc: 0.906
[2022-10-13 15:29:33] - INFO: Epoch: 0, Batch[3410/4186], Train loss :0.497, Train acc: 0.797
[2022-10-13 15:29:35] - INFO: Epoch: 0, Batch[3420/4186], Train loss :0.570, Train acc: 0.859
[2022-10-13 15:29:38] - INFO: Epoch: 0, Batch[3430/4186], Train loss :0.491, Train acc: 0.859
[2022-10-13 15:29:40] - INFO: Epoch: 0, Batch[3440/4186], Train loss :0.436, Train acc: 0.859
[2022-10-13 15:29:42] - INFO: Epoch: 0, Batch[3450/4186], Train loss :0.413, Train acc: 0.906
[2022-10-13 15:29:44] - INFO: Epoch: 0, Batch[3460/4186], Train loss :0.276, Train acc: 0.922
[2022-10-13 15:29:47] - INFO: Epoch: 0, Batch[3470/4186], Train loss :0.386, Train acc: 0.906
[2022-10-13 15:29:49] - INFO: Epoch: 0, Batch[3480/4186], Train loss :0.518, Train acc: 0.828
[2022-10-13 15:29:51] - INFO: Epoch: 0, Batch[3490/4186], Train loss :0.487, Train acc: 0.875
[2022-10-13 15:29:54] - INFO: Epoch: 0, Batch[3500/4186], Train loss :0.555, Train acc: 0.875
[2022-10-13 15:29:56] - INFO: Epoch: 0, Batch[3510/4186], Train loss :0.283, Train acc: 0.938
[2022-10-13 15:29:58] - INFO: Epoch: 0, Batch[3520/4186], Train loss :0.815, Train acc: 0.781
[2022-10-13 15:30:00] - INFO: Epoch: 0, Batch[3530/4186], Train loss :0.255, Train acc: 0.922
[2022-10-13 15:30:03] - INFO: Epoch: 0, Batch[3540/4186], Train loss :0.596, Train acc: 0.844
[2022-10-13 15:30:05] - INFO: Epoch: 0, Batch[3550/4186], Train loss :0.475, Train acc: 0.828
[2022-10-13 15:30:07] - INFO: Epoch: 0, Batch[3560/4186], Train loss :0.601, Train acc: 0.859
[2022-10-13 15:30:09] - INFO: Epoch: 0, Batch[3570/4186], Train loss :0.382, Train acc: 0.891
[2022-10-13 15:30:12] - INFO: Epoch: 0, Batch[3580/4186], Train loss :0.450, Train acc: 0.875
[2022-10-13 15:30:14] - INFO: Epoch: 0, Batch[3590/4186], Train loss :0.614, Train acc: 0.828
[2022-10-13 15:30:16] - INFO: Epoch: 0, Batch[3600/4186], Train loss :0.402, Train acc: 0.875
[2022-10-13 15:30:19] - INFO: Epoch: 0, Batch[3610/4186], Train loss :0.280, Train acc: 0.922
[2022-10-13 15:30:21] - INFO: Epoch: 0, Batch[3620/4186], Train loss :0.351, Train acc: 0.922
[2022-10-13 15:30:23] - INFO: Epoch: 0, Batch[3630/4186], Train loss :0.178, Train acc: 0.953
[2022-10-13 15:30:26] - INFO: Epoch: 0, Batch[3640/4186], Train loss :0.454, Train acc: 0.875
[2022-10-13 15:30:28] - INFO: Epoch: 0, Batch[3650/4186], Train loss :0.527, Train acc: 0.859
[2022-10-13 15:30:30] - INFO: Epoch: 0, Batch[3660/4186], Train loss :0.435, Train acc: 0.891
[2022-10-13 15:30:32] - INFO: Epoch: 0, Batch[3670/4186], Train loss :0.485, Train acc: 0.875
[2022-10-13 15:30:35] - INFO: Epoch: 0, Batch[3680/4186], Train loss :0.426, Train acc: 0.891
[2022-10-13 15:30:37] - INFO: Epoch: 0, Batch[3690/4186], Train loss :0.335, Train acc: 0.891
[2022-10-13 15:30:39] - INFO: Epoch: 0, Batch[3700/4186], Train loss :0.431, Train acc: 0.844
[2022-10-13 15:30:42] - INFO: Epoch: 0, Batch[3710/4186], Train loss :0.631, Train acc: 0.844
[2022-10-13 15:30:44] - INFO: Epoch: 0, Batch[3720/4186], Train loss :0.316, Train acc: 0.891
[2022-10-13 15:30:46] - INFO: Epoch: 0, Batch[3730/4186], Train loss :0.261, Train acc: 0.938
[2022-10-13 15:30:48] - INFO: Epoch: 0, Batch[3740/4186], Train loss :0.431, Train acc: 0.906
[2022-10-13 15:30:51] - INFO: Epoch: 0, Batch[3750/4186], Train loss :0.552, Train acc: 0.891
[2022-10-13 15:30:53] - INFO: Epoch: 0, Batch[3760/4186], Train loss :0.349, Train acc: 0.891
[2022-10-13 15:30:55] - INFO: Epoch: 0, Batch[3770/4186], Train loss :0.315, Train acc: 0.891
[2022-10-13 15:30:57] - INFO: Epoch: 0, Batch[3780/4186], Train loss :0.276, Train acc: 0.922
[2022-10-13 15:31:00] - INFO: Epoch: 0, Batch[3790/4186], Train loss :0.295, Train acc: 0.875
[2022-10-13 15:31:02] - INFO: Epoch: 0, Batch[3800/4186], Train loss :0.452, Train acc: 0.875
[2022-10-13 15:31:04] - INFO: Epoch: 0, Batch[3810/4186], Train loss :0.518, Train acc: 0.859
[2022-10-13 15:31:07] - INFO: Epoch: 0, Batch[3820/4186], Train loss :0.584, Train acc: 0.828
[2022-10-13 15:31:09] - INFO: Epoch: 0, Batch[3830/4186], Train loss :0.474, Train acc: 0.891
[2022-10-13 15:31:11] - INFO: Epoch: 0, Batch[3840/4186], Train loss :0.327, Train acc: 0.906
[2022-10-13 15:31:14] - INFO: Epoch: 0, Batch[3850/4186], Train loss :0.347, Train acc: 0.906
[2022-10-13 15:31:16] - INFO: Epoch: 0, Batch[3860/4186], Train loss :0.528, Train acc: 0.844
[2022-10-13 15:31:18] - INFO: Epoch: 0, Batch[3870/4186], Train loss :0.595, Train acc: 0.859
[2022-10-13 15:31:21] - INFO: Epoch: 0, Batch[3880/4186], Train loss :0.563, Train acc: 0.859
[2022-10-13 15:31:23] - INFO: Epoch: 0, Batch[3890/4186], Train loss :0.643, Train acc: 0.875
[2022-10-13 15:31:25] - INFO: Epoch: 0, Batch[3900/4186], Train loss :0.504, Train acc: 0.844
[2022-10-13 15:31:27] - INFO: Epoch: 0, Batch[3910/4186], Train loss :0.118, Train acc: 0.984
[2022-10-13 15:31:30] - INFO: Epoch: 0, Batch[3920/4186], Train loss :0.559, Train acc: 0.859
[2022-10-13 15:31:32] - INFO: Epoch: 0, Batch[3930/4186], Train loss :0.264, Train acc: 0.922
[2022-10-13 15:31:34] - INFO: Epoch: 0, Batch[3940/4186], Train loss :0.462, Train acc: 0.859
[2022-10-13 15:31:37] - INFO: Epoch: 0, Batch[3950/4186], Train loss :0.376, Train acc: 0.906
[2022-10-13 15:31:39] - INFO: Epoch: 0, Batch[3960/4186], Train loss :0.156, Train acc: 0.969
[2022-10-13 15:31:41] - INFO: Epoch: 0, Batch[3970/4186], Train loss :0.407, Train acc: 0.828
[2022-10-13 15:31:44] - INFO: Epoch: 0, Batch[3980/4186], Train loss :0.538, Train acc: 0.859
[2022-10-13 15:31:46] - INFO: Epoch: 0, Batch[3990/4186], Train loss :0.456, Train acc: 0.891
[2022-10-13 15:31:48] - INFO: Epoch: 0, Batch[4000/4186], Train loss :0.437, Train acc: 0.859
[2022-10-13 15:31:51] - INFO: Epoch: 0, Batch[4010/4186], Train loss :0.221, Train acc: 0.938
[2022-10-13 15:31:53] - INFO: Epoch: 0, Batch[4020/4186], Train loss :0.527, Train acc: 0.891
[2022-10-13 15:31:55] - INFO: Epoch: 0, Batch[4030/4186], Train loss :0.415, Train acc: 0.875
[2022-10-13 15:31:57] - INFO: Epoch: 0, Batch[4040/4186], Train loss :0.275, Train acc: 0.906
[2022-10-13 15:32:00] - INFO: Epoch: 0, Batch[4050/4186], Train loss :0.364, Train acc: 0.828
[2022-10-13 15:32:02] - INFO: Epoch: 0, Batch[4060/4186], Train loss :0.165, Train acc: 0.938
[2022-10-13 15:32:04] - INFO: Epoch: 0, Batch[4070/4186], Train loss :0.279, Train acc: 0.906
[2022-10-13 15:32:07] - INFO: Epoch: 0, Batch[4080/4186], Train loss :0.441, Train acc: 0.875
[2022-10-13 15:32:09] - INFO: Epoch: 0, Batch[4090/4186], Train loss :0.377, Train acc: 0.891
[2022-10-13 15:32:11] - INFO: Epoch: 0, Batch[4100/4186], Train loss :0.434, Train acc: 0.859
[2022-10-13 15:32:14] - INFO: Epoch: 0, Batch[4110/4186], Train loss :0.522, Train acc: 0.859
[2022-10-13 15:32:16] - INFO: Epoch: 0, Batch[4120/4186], Train loss :0.362, Train acc: 0.891
[2022-10-13 15:32:18] - INFO: Epoch: 0, Batch[4130/4186], Train loss :0.404, Train acc: 0.875
[2022-10-13 15:32:20] - INFO: Epoch: 0, Batch[4140/4186], Train loss :0.375, Train acc: 0.906
[2022-10-13 15:32:23] - INFO: Epoch: 0, Batch[4150/4186], Train loss :0.576, Train acc: 0.859
[2022-10-13 15:32:25] - INFO: Epoch: 0, Batch[4160/4186], Train loss :0.461, Train acc: 0.828
[2022-10-13 15:32:27] - INFO: Epoch: 0, Batch[4170/4186], Train loss :0.536, Train acc: 0.844
[2022-10-13 15:32:30] - INFO: Epoch: 0, Batch[4180/4186], Train loss :0.573, Train acc: 0.828
[2022-10-13 15:32:31] - INFO: Epoch: 0, Train loss: 0.483, Epoch time = 972.105s
[2022-10-13 15:32:31] - INFO: Epoch: 1, Batch[0/4186], Train loss :0.239, Train acc: 0.906
[2022-10-13 15:32:33] - INFO: Epoch: 1, Batch[10/4186], Train loss :0.209, Train acc: 0.953
[2022-10-13 15:32:36] - INFO: Epoch: 1, Batch[20/4186], Train loss :0.350, Train acc: 0.828
[2022-10-13 15:32:38] - INFO: Epoch: 1, Batch[30/4186], Train loss :0.347, Train acc: 0.891
[2022-10-13 15:32:40] - INFO: Epoch: 1, Batch[40/4186], Train loss :0.352, Train acc: 0.891
[2022-10-13 15:32:42] - INFO: Epoch: 1, Batch[50/4186], Train loss :0.227, Train acc: 0.922
[2022-10-13 15:32:45] - INFO: Epoch: 1, Batch[60/4186], Train loss :0.307, Train acc: 0.922
[2022-10-13 15:32:47] - INFO: Epoch: 1, Batch[70/4186], Train loss :0.294, Train acc: 0.953
[2022-10-13 15:32:50] - INFO: Epoch: 1, Batch[80/4186], Train loss :0.328, Train acc: 0.875
[2022-10-13 15:32:52] - INFO: Epoch: 1, Batch[90/4186], Train loss :0.271, Train acc: 0.922
[2022-10-13 15:32:54] - INFO: Epoch: 1, Batch[100/4186], Train loss :0.351, Train acc: 0.875
[2022-10-13 15:32:57] - INFO: Epoch: 1, Batch[110/4186], Train loss :0.126, Train acc: 0.953
[2022-10-13 15:32:59] - INFO: Epoch: 1, Batch[120/4186], Train loss :0.280, Train acc: 0.922
[2022-10-13 15:33:01] - INFO: Epoch: 1, Batch[130/4186], Train loss :0.146, Train acc: 0.969
[2022-10-13 15:33:03] - INFO: Epoch: 1, Batch[140/4186], Train loss :0.389, Train acc: 0.891
[2022-10-13 15:33:06] - INFO: Epoch: 1, Batch[150/4186], Train loss :0.303, Train acc: 0.906
[2022-10-13 15:33:08] - INFO: Epoch: 1, Batch[160/4186], Train loss :0.634, Train acc: 0.828
[2022-10-13 15:33:10] - INFO: Epoch: 1, Batch[170/4186], Train loss :0.311, Train acc: 0.891
[2022-10-13 15:33:13] - INFO: Epoch: 1, Batch[180/4186], Train loss :0.303, Train acc: 0.938
[2022-10-13 15:33:15] - INFO: Epoch: 1, Batch[190/4186], Train loss :0.304, Train acc: 0.922
[2022-10-13 15:33:17] - INFO: Epoch: 1, Batch[200/4186], Train loss :0.206, Train acc: 0.922
[2022-10-13 15:33:20] - INFO: Epoch: 1, Batch[210/4186], Train loss :0.298, Train acc: 0.891
[2022-10-13 15:33:22] - INFO: Epoch: 1, Batch[220/4186], Train loss :0.390, Train acc: 0.875
[2022-10-13 15:33:24] - INFO: Epoch: 1, Batch[230/4186], Train loss :0.338, Train acc: 0.891
[2022-10-13 15:33:27] - INFO: Epoch: 1, Batch[240/4186], Train loss :0.457, Train acc: 0.906
[2022-10-13 15:33:29] - INFO: Epoch: 1, Batch[250/4186], Train loss :0.411, Train acc: 0.906
[2022-10-13 15:33:32] - INFO: Epoch: 1, Batch[260/4186], Train loss :0.270, Train acc: 0.906
[2022-10-13 15:33:34] - INFO: Epoch: 1, Batch[270/4186], Train loss :0.199, Train acc: 0.922
[2022-10-13 15:33:36] - INFO: Epoch: 1, Batch[280/4186], Train loss :0.407, Train acc: 0.875
[2022-10-13 15:33:39] - INFO: Epoch: 1, Batch[290/4186], Train loss :0.298, Train acc: 0.891
[2022-10-13 15:33:41] - INFO: Epoch: 1, Batch[300/4186], Train loss :0.374, Train acc: 0.891
[2022-10-13 15:33:43] - INFO: Epoch: 1, Batch[310/4186], Train loss :0.305, Train acc: 0.875
[2022-10-13 15:33:46] - INFO: Epoch: 1, Batch[320/4186], Train loss :0.085, Train acc: 0.984
[2022-10-13 15:33:48] - INFO: Epoch: 1, Batch[330/4186], Train loss :0.267, Train acc: 0.922
[2022-10-13 15:33:50] - INFO: Epoch: 1, Batch[340/4186], Train loss :0.278, Train acc: 0.891
[2022-10-13 15:33:53] - INFO: Epoch: 1, Batch[350/4186], Train loss :0.326, Train acc: 0.922
[2022-10-13 15:33:55] - INFO: Epoch: 1, Batch[360/4186], Train loss :0.275, Train acc: 0.922
[2022-10-13 15:33:57] - INFO: Epoch: 1, Batch[370/4186], Train loss :0.358, Train acc: 0.906
[2022-10-13 15:33:59] - INFO: Epoch: 1, Batch[380/4186], Train loss :0.135, Train acc: 0.953
[2022-10-13 15:34:02] - INFO: Epoch: 1, Batch[390/4186], Train loss :0.320, Train acc: 0.891
[2022-10-13 15:34:04] - INFO: Epoch: 1, Batch[400/4186], Train loss :0.282, Train acc: 0.922
[2022-10-13 15:34:06] - INFO: Epoch: 1, Batch[410/4186], Train loss :0.323, Train acc: 0.891
[2022-10-13 15:34:09] - INFO: Epoch: 1, Batch[420/4186], Train loss :0.286, Train acc: 0.938
[2022-10-13 15:34:11] - INFO: Epoch: 1, Batch[430/4186], Train loss :0.272, Train acc: 0.922
[2022-10-13 15:34:13] - INFO: Epoch: 1, Batch[440/4186], Train loss :0.353, Train acc: 0.922
[2022-10-13 15:34:16] - INFO: Epoch: 1, Batch[450/4186], Train loss :0.338, Train acc: 0.875
[2022-10-13 15:34:18] - INFO: Epoch: 1, Batch[460/4186], Train loss :0.265, Train acc: 0.906
[2022-10-13 15:34:21] - INFO: Epoch: 1, Batch[470/4186], Train loss :0.452, Train acc: 0.828
[2022-10-13 15:34:23] - INFO: Epoch: 1, Batch[480/4186], Train loss :0.510, Train acc: 0.859
[2022-10-13 15:34:25] - INFO: Epoch: 1, Batch[490/4186], Train loss :0.247, Train acc: 0.906
[2022-10-13 15:34:28] - INFO: Epoch: 1, Batch[500/4186], Train loss :0.499, Train acc: 0.891
[2022-10-13 15:34:30] - INFO: Epoch: 1, Batch[510/4186], Train loss :0.530, Train acc: 0.844
[2022-10-13 15:34:32] - INFO: Epoch: 1, Batch[520/4186], Train loss :0.250, Train acc: 0.875
[2022-10-13 15:34:35] - INFO: Epoch: 1, Batch[530/4186], Train loss :0.193, Train acc: 0.953
[2022-10-13 15:34:37] - INFO: Epoch: 1, Batch[540/4186], Train loss :0.258, Train acc: 0.953
[2022-10-13 15:34:39] - INFO: Epoch: 1, Batch[550/4186], Train loss :0.394, Train acc: 0.906
[2022-10-13 15:34:42] - INFO: Epoch: 1, Batch[560/4186], Train loss :0.338, Train acc: 0.922
[2022-10-13 15:34:44] - INFO: Epoch: 1, Batch[570/4186], Train loss :0.391, Train acc: 0.891
[2022-10-13 15:34:46] - INFO: Epoch: 1, Batch[580/4186], Train loss :0.381, Train acc: 0.891
[2022-10-13 15:34:48] - INFO: Epoch: 1, Batch[590/4186], Train loss :0.254, Train acc: 0.953
[2022-10-13 15:34:51] - INFO: Epoch: 1, Batch[600/4186], Train loss :0.279, Train acc: 0.922
[2022-10-13 15:34:53] - INFO: Epoch: 1, Batch[610/4186], Train loss :0.459, Train acc: 0.859
[2022-10-13 15:34:55] - INFO: Epoch: 1, Batch[620/4186], Train loss :0.353, Train acc: 0.891
[2022-10-13 15:34:58] - INFO: Epoch: 1, Batch[630/4186], Train loss :0.286, Train acc: 0.906
[2022-10-13 15:35:00] - INFO: Epoch: 1, Batch[640/4186], Train loss :0.331, Train acc: 0.922
[2022-10-13 15:35:02] - INFO: Epoch: 1, Batch[650/4186], Train loss :0.285, Train acc: 0.891
[2022-10-13 15:35:05] - INFO: Epoch: 1, Batch[660/4186], Train loss :0.520, Train acc: 0.844
[2022-10-13 15:35:07] - INFO: Epoch: 1, Batch[670/4186], Train loss :0.133, Train acc: 0.953
[2022-10-13 15:35:09] - INFO: Epoch: 1, Batch[680/4186], Train loss :0.224, Train acc: 0.875
[2022-10-13 15:35:12] - INFO: Epoch: 1, Batch[690/4186], Train loss :0.236, Train acc: 0.906
[2022-10-13 15:35:14] - INFO: Epoch: 1, Batch[700/4186], Train loss :0.313, Train acc: 0.922
[2022-10-13 15:35:16] - INFO: Epoch: 1, Batch[710/4186], Train loss :0.291, Train acc: 0.906
[2022-10-13 15:35:19] - INFO: Epoch: 1, Batch[720/4186], Train loss :0.154, Train acc: 0.938
[2022-10-13 15:35:21] - INFO: Epoch: 1, Batch[730/4186], Train loss :0.572, Train acc: 0.844
[2022-10-13 15:35:23] - INFO: Epoch: 1, Batch[740/4186], Train loss :0.225, Train acc: 0.922
[2022-10-13 15:35:25] - INFO: Epoch: 1, Batch[750/4186], Train loss :0.142, Train acc: 0.953
[2022-10-13 15:35:28] - INFO: Epoch: 1, Batch[760/4186], Train loss :0.217, Train acc: 0.922
[2022-10-13 15:35:30] - INFO: Epoch: 1, Batch[770/4186], Train loss :0.571, Train acc: 0.828
[2022-10-13 15:35:32] - INFO: Epoch: 1, Batch[780/4186], Train loss :0.343, Train acc: 0.922
[2022-10-13 15:35:34] - INFO: Epoch: 1, Batch[790/4186], Train loss :0.263, Train acc: 0.906
[2022-10-13 15:35:37] - INFO: Epoch: 1, Batch[800/4186], Train loss :0.452, Train acc: 0.812
[2022-10-13 15:35:39] - INFO: Epoch: 1, Batch[810/4186], Train loss :0.498, Train acc: 0.844
[2022-10-13 15:35:41] - INFO: Epoch: 1, Batch[820/4186], Train loss :0.440, Train acc: 0.859
[2022-10-13 15:35:44] - INFO: Epoch: 1, Batch[830/4186], Train loss :0.228, Train acc: 0.938
[2022-10-13 15:35:46] - INFO: Epoch: 1, Batch[840/4186], Train loss :0.345, Train acc: 0.906
[2022-10-13 15:35:48] - INFO: Epoch: 1, Batch[850/4186], Train loss :0.412, Train acc: 0.859
[2022-10-13 15:35:51] - INFO: Epoch: 1, Batch[860/4186], Train loss :0.557, Train acc: 0.828
[2022-10-13 15:35:53] - INFO: Epoch: 1, Batch[870/4186], Train loss :0.414, Train acc: 0.875
[2022-10-13 15:35:55] - INFO: Epoch: 1, Batch[880/4186], Train loss :0.211, Train acc: 0.938
[2022-10-13 15:35:58] - INFO: Epoch: 1, Batch[890/4186], Train loss :0.502, Train acc: 0.875
[2022-10-13 15:36:00] - INFO: Epoch: 1, Batch[900/4186], Train loss :0.441, Train acc: 0.875
[2022-10-13 15:36:02] - INFO: Epoch: 1, Batch[910/4186], Train loss :0.308, Train acc: 0.875
[2022-10-13 15:36:05] - INFO: Epoch: 1, Batch[920/4186], Train loss :0.477, Train acc: 0.875
[2022-10-13 15:36:07] - INFO: Epoch: 1, Batch[930/4186], Train loss :0.333, Train acc: 0.859
[2022-10-13 15:36:09] - INFO: Epoch: 1, Batch[940/4186], Train loss :0.384, Train acc: 0.906
[2022-10-13 15:36:12] - INFO: Epoch: 1, Batch[950/4186], Train loss :0.212, Train acc: 0.906
[2022-10-13 15:36:14] - INFO: Epoch: 1, Batch[960/4186], Train loss :0.286, Train acc: 0.922
[2022-10-13 15:36:16] - INFO: Epoch: 1, Batch[970/4186], Train loss :0.238, Train acc: 0.906
[2022-10-13 15:36:19] - INFO: Epoch: 1, Batch[980/4186], Train loss :0.276, Train acc: 0.906
[2022-10-13 15:36:21] - INFO: Epoch: 1, Batch[990/4186], Train loss :0.729, Train acc: 0.797
[2022-10-13 15:36:23] - INFO: Epoch: 1, Batch[1000/4186], Train loss :0.424, Train acc: 0.891
[2022-10-13 15:36:25] - INFO: Epoch: 1, Batch[1010/4186], Train loss :0.350, Train acc: 0.875
[2022-10-13 15:36:28] - INFO: Epoch: 1, Batch[1020/4186], Train loss :0.141, Train acc: 0.953
[2022-10-13 15:36:30] - INFO: Epoch: 1, Batch[1030/4186], Train loss :0.473, Train acc: 0.812
[2022-10-13 15:36:32] - INFO: Epoch: 1, Batch[1040/4186], Train loss :0.535, Train acc: 0.859
[2022-10-13 15:36:35] - INFO: Epoch: 1, Batch[1050/4186], Train loss :0.325, Train acc: 0.875
[2022-10-13 15:36:37] - INFO: Epoch: 1, Batch[1060/4186], Train loss :0.548, Train acc: 0.844
[2022-10-13 15:36:40] - INFO: Epoch: 1, Batch[1070/4186], Train loss :0.355, Train acc: 0.922
[2022-10-13 15:36:42] - INFO: Epoch: 1, Batch[1080/4186], Train loss :0.452, Train acc: 0.812
[2022-10-13 15:36:44] - INFO: Epoch: 1, Batch[1090/4186], Train loss :0.633, Train acc: 0.812
[2022-10-13 15:36:47] - INFO: Epoch: 1, Batch[1100/4186], Train loss :0.268, Train acc: 0.906
[2022-10-13 15:36:49] - INFO: Epoch: 1, Batch[1110/4186], Train loss :0.401, Train acc: 0.844
[2022-10-13 15:36:51] - INFO: Epoch: 1, Batch[1120/4186], Train loss :0.290, Train acc: 0.922
[2022-10-13 15:36:53] - INFO: Epoch: 1, Batch[1130/4186], Train loss :0.389, Train acc: 0.891
[2022-10-13 15:36:56] - INFO: Epoch: 1, Batch[1140/4186], Train loss :0.323, Train acc: 0.906
[2022-10-13 15:36:58] - INFO: Epoch: 1, Batch[1150/4186], Train loss :0.084, Train acc: 0.969
[2022-10-13 15:37:00] - INFO: Epoch: 1, Batch[1160/4186], Train loss :0.241, Train acc: 0.938
[2022-10-13 15:37:02] - INFO: Epoch: 1, Batch[1170/4186], Train loss :0.393, Train acc: 0.891
[2022-10-13 15:37:05] - INFO: Epoch: 1, Batch[1180/4186], Train loss :0.287, Train acc: 0.938
[2022-10-13 15:37:07] - INFO: Epoch: 1, Batch[1190/4186], Train loss :0.493, Train acc: 0.859
[2022-10-13 15:37:09] - INFO: Epoch: 1, Batch[1200/4186], Train loss :0.426, Train acc: 0.859
[2022-10-13 15:37:12] - INFO: Epoch: 1, Batch[1210/4186], Train loss :0.377, Train acc: 0.906
[2022-10-13 15:37:14] - INFO: Epoch: 1, Batch[1220/4186], Train loss :0.307, Train acc: 0.891
[2022-10-13 15:37:17] - INFO: Epoch: 1, Batch[1230/4186], Train loss :0.344, Train acc: 0.906
[2022-10-13 15:37:19] - INFO: Epoch: 1, Batch[1240/4186], Train loss :0.200, Train acc: 0.938
[2022-10-13 15:37:21] - INFO: Epoch: 1, Batch[1250/4186], Train loss :0.304, Train acc: 0.906
[2022-10-13 15:37:23] - INFO: Epoch: 1, Batch[1260/4186], Train loss :0.417, Train acc: 0.891
[2022-10-13 15:37:26] - INFO: Epoch: 1, Batch[1270/4186], Train loss :0.423, Train acc: 0.906
[2022-10-13 15:37:28] - INFO: Epoch: 1, Batch[1280/4186], Train loss :0.366, Train acc: 0.891
[2022-10-13 15:37:30] - INFO: Epoch: 1, Batch[1290/4186], Train loss :0.441, Train acc: 0.891
[2022-10-13 15:37:33] - INFO: Epoch: 1, Batch[1300/4186], Train loss :0.469, Train acc: 0.828
[2022-10-13 15:37:35] - INFO: Epoch: 1, Batch[1310/4186], Train loss :0.144, Train acc: 0.938
[2022-10-13 15:37:37] - INFO: Epoch: 1, Batch[1320/4186], Train loss :0.529, Train acc: 0.859
[2022-10-13 15:37:40] - INFO: Epoch: 1, Batch[1330/4186], Train loss :0.336, Train acc: 0.859
[2022-10-13 15:37:42] - INFO: Epoch: 1, Batch[1340/4186], Train loss :0.233, Train acc: 0.969
[2022-10-13 15:37:44] - INFO: Epoch: 1, Batch[1350/4186], Train loss :0.226, Train acc: 0.922
[2022-10-13 15:37:47] - INFO: Epoch: 1, Batch[1360/4186], Train loss :0.143, Train acc: 0.938
[2022-10-13 15:37:49] - INFO: Epoch: 1, Batch[1370/4186], Train loss :0.562, Train acc: 0.844
[2022-10-13 15:37:51] - INFO: Epoch: 1, Batch[1380/4186], Train loss :0.496, Train acc: 0.875
[2022-10-13 15:37:54] - INFO: Epoch: 1, Batch[1390/4186], Train loss :0.436, Train acc: 0.922
[2022-10-13 15:37:56] - INFO: Epoch: 1, Batch[1400/4186], Train loss :0.335, Train acc: 0.906
[2022-10-13 15:37:58] - INFO: Epoch: 1, Batch[1410/4186], Train loss :0.564, Train acc: 0.844
[2022-10-13 15:38:01] - INFO: Epoch: 1, Batch[1420/4186], Train loss :0.398, Train acc: 0.875
[2022-10-13 15:38:03] - INFO: Epoch: 1, Batch[1430/4186], Train loss :0.332, Train acc: 0.938
[2022-10-13 15:38:05] - INFO: Epoch: 1, Batch[1440/4186], Train loss :0.359, Train acc: 0.875
[2022-10-13 15:38:08] - INFO: Epoch: 1, Batch[1450/4186], Train loss :0.487, Train acc: 0.844
[2022-10-13 15:38:10] - INFO: Epoch: 1, Batch[1460/4186], Train loss :0.343, Train acc: 0.922
[2022-10-13 15:38:12] - INFO: Epoch: 1, Batch[1470/4186], Train loss :0.297, Train acc: 0.891
[2022-10-13 15:38:15] - INFO: Epoch: 1, Batch[1480/4186], Train loss :0.367, Train acc: 0.906
[2022-10-13 15:38:17] - INFO: Epoch: 1, Batch[1490/4186], Train loss :0.298, Train acc: 0.906
[2022-10-13 15:38:19] - INFO: Epoch: 1, Batch[1500/4186], Train loss :0.302, Train acc: 0.875
[2022-10-13 15:38:22] - INFO: Epoch: 1, Batch[1510/4186], Train loss :0.545, Train acc: 0.812
[2022-10-13 15:38:24] - INFO: Epoch: 1, Batch[1520/4186], Train loss :0.250, Train acc: 0.922
[2022-10-13 15:38:27] - INFO: Epoch: 1, Batch[1530/4186], Train loss :0.271, Train acc: 0.922
[2022-10-13 15:38:29] - INFO: Epoch: 1, Batch[1540/4186], Train loss :0.505, Train acc: 0.828
[2022-10-13 15:38:31] - INFO: Epoch: 1, Batch[1550/4186], Train loss :0.226, Train acc: 0.938
[2022-10-13 15:38:33] - INFO: Epoch: 1, Batch[1560/4186], Train loss :0.378, Train acc: 0.891
[2022-10-13 15:38:36] - INFO: Epoch: 1, Batch[1570/4186], Train loss :0.203, Train acc: 0.938
[2022-10-13 15:38:38] - INFO: Epoch: 1, Batch[1580/4186], Train loss :0.221, Train acc: 0.922
[2022-10-13 15:38:40] - INFO: Epoch: 1, Batch[1590/4186], Train loss :0.357, Train acc: 0.859
[2022-10-13 15:38:43] - INFO: Epoch: 1, Batch[1600/4186], Train loss :0.216, Train acc: 0.938
[2022-10-13 15:38:45] - INFO: Epoch: 1, Batch[1610/4186], Train loss :0.476, Train acc: 0.875
[2022-10-13 15:38:47] - INFO: Epoch: 1, Batch[1620/4186], Train loss :0.314, Train acc: 0.906
[2022-10-13 15:38:50] - INFO: Epoch: 1, Batch[1630/4186], Train loss :0.309, Train acc: 0.891
[2022-10-13 15:38:52] - INFO: Epoch: 1, Batch[1640/4186], Train loss :0.282, Train acc: 0.906
[2022-10-13 15:38:55] - INFO: Epoch: 1, Batch[1650/4186], Train loss :0.403, Train acc: 0.906
[2022-10-13 15:38:57] - INFO: Epoch: 1, Batch[1660/4186], Train loss :0.266, Train acc: 0.922
[2022-10-13 15:38:59] - INFO: Epoch: 1, Batch[1670/4186], Train loss :0.236, Train acc: 0.922
[2022-10-13 15:39:02] - INFO: Epoch: 1, Batch[1680/4186], Train loss :0.160, Train acc: 0.953
[2022-10-13 15:39:04] - INFO: Epoch: 1, Batch[1690/4186], Train loss :0.268, Train acc: 0.953
[2022-10-13 15:39:06] - INFO: Epoch: 1, Batch[1700/4186], Train loss :0.273, Train acc: 0.906
[2022-10-13 15:39:09] - INFO: Epoch: 1, Batch[1710/4186], Train loss :0.347, Train acc: 0.906
[2022-10-13 15:39:11] - INFO: Epoch: 1, Batch[1720/4186], Train loss :0.288, Train acc: 0.906
[2022-10-13 15:39:14] - INFO: Epoch: 1, Batch[1730/4186], Train loss :0.129, Train acc: 0.953
[2022-10-13 15:39:16] - INFO: Epoch: 1, Batch[1740/4186], Train loss :0.306, Train acc: 0.938
[2022-10-13 15:39:19] - INFO: Epoch: 1, Batch[1750/4186], Train loss :0.183, Train acc: 0.953
[2022-10-13 15:39:21] - INFO: Epoch: 1, Batch[1760/4186], Train loss :0.257, Train acc: 0.922
[2022-10-13 15:39:23] - INFO: Epoch: 1, Batch[1770/4186], Train loss :0.457, Train acc: 0.859
[2022-10-13 15:39:26] - INFO: Epoch: 1, Batch[1780/4186], Train loss :0.329, Train acc: 0.906
[2022-10-13 15:39:28] - INFO: Epoch: 1, Batch[1790/4186], Train loss :0.303, Train acc: 0.875
[2022-10-13 15:39:30] - INFO: Epoch: 1, Batch[1800/4186], Train loss :0.523, Train acc: 0.859
[2022-10-13 15:39:33] - INFO: Epoch: 1, Batch[1810/4186], Train loss :0.137, Train acc: 0.969
[2022-10-13 15:39:35] - INFO: Epoch: 1, Batch[1820/4186], Train loss :0.301, Train acc: 0.875
[2022-10-13 15:39:37] - INFO: Epoch: 1, Batch[1830/4186], Train loss :0.257, Train acc: 0.922
[2022-10-13 15:39:39] - INFO: Epoch: 1, Batch[1840/4186], Train loss :0.693, Train acc: 0.766
[2022-10-13 15:39:42] - INFO: Epoch: 1, Batch[1850/4186], Train loss :0.408, Train acc: 0.875
[2022-10-13 15:39:44] - INFO: Epoch: 1, Batch[1860/4186], Train loss :0.462, Train acc: 0.828
[2022-10-13 15:39:46] - INFO: Epoch: 1, Batch[1870/4186], Train loss :0.289, Train acc: 0.891
[2022-10-13 15:39:49] - INFO: Epoch: 1, Batch[1880/4186], Train loss :0.272, Train acc: 0.922
[2022-10-13 15:39:51] - INFO: Epoch: 1, Batch[1890/4186], Train loss :0.381, Train acc: 0.891
[2022-10-13 15:39:53] - INFO: Epoch: 1, Batch[1900/4186], Train loss :0.201, Train acc: 0.953
[2022-10-13 15:39:56] - INFO: Epoch: 1, Batch[1910/4186], Train loss :0.408, Train acc: 0.922
[2022-10-13 15:39:58] - INFO: Epoch: 1, Batch[1920/4186], Train loss :0.308, Train acc: 0.922
[2022-10-13 15:40:00] - INFO: Epoch: 1, Batch[1930/4186], Train loss :0.201, Train acc: 0.953
[2022-10-13 15:40:03] - INFO: Epoch: 1, Batch[1940/4186], Train loss :0.454, Train acc: 0.891
[2022-10-13 15:40:05] - INFO: Epoch: 1, Batch[1950/4186], Train loss :0.333, Train acc: 0.875
[2022-10-13 15:40:07] - INFO: Epoch: 1, Batch[1960/4186], Train loss :0.276, Train acc: 0.875
[2022-10-13 15:40:10] - INFO: Epoch: 1, Batch[1970/4186], Train loss :0.217, Train acc: 0.938
[2022-10-13 15:40:12] - INFO: Epoch: 1, Batch[1980/4186], Train loss :0.318, Train acc: 0.891
[2022-10-13 15:40:14] - INFO: Epoch: 1, Batch[1990/4186], Train loss :0.266, Train acc: 0.891
[2022-10-13 15:40:17] - INFO: Epoch: 1, Batch[2000/4186], Train loss :0.335, Train acc: 0.906
[2022-10-13 15:40:19] - INFO: Epoch: 1, Batch[2010/4186], Train loss :0.263, Train acc: 0.891
[2022-10-13 15:40:21] - INFO: Epoch: 1, Batch[2020/4186], Train loss :0.441, Train acc: 0.875
[2022-10-13 15:40:23] - INFO: Epoch: 1, Batch[2030/4186], Train loss :0.266, Train acc: 0.938
[2022-10-13 15:40:26] - INFO: Epoch: 1, Batch[2040/4186], Train loss :0.347, Train acc: 0.891
[2022-10-13 15:40:28] - INFO: Epoch: 1, Batch[2050/4186], Train loss :0.486, Train acc: 0.828
[2022-10-13 15:40:30] - INFO: Epoch: 1, Batch[2060/4186], Train loss :0.339, Train acc: 0.922
[2022-10-13 15:40:33] - INFO: Epoch: 1, Batch[2070/4186], Train loss :0.302, Train acc: 0.859
[2022-10-13 15:40:35] - INFO: Epoch: 1, Batch[2080/4186], Train loss :0.403, Train acc: 0.875
[2022-10-13 15:40:37] - INFO: Epoch: 1, Batch[2090/4186], Train loss :0.309, Train acc: 0.906
[2022-10-13 15:40:40] - INFO: Epoch: 1, Batch[2100/4186], Train loss :0.283, Train acc: 0.906
[2022-10-13 15:40:42] - INFO: Epoch: 1, Batch[2110/4186], Train loss :0.509, Train acc: 0.859
[2022-10-13 15:40:44] - INFO: Epoch: 1, Batch[2120/4186], Train loss :0.355, Train acc: 0.891
[2022-10-13 15:40:47] - INFO: Epoch: 1, Batch[2130/4186], Train loss :0.418, Train acc: 0.875
[2022-10-13 15:40:49] - INFO: Epoch: 1, Batch[2140/4186], Train loss :0.244, Train acc: 0.922
[2022-10-13 15:40:51] - INFO: Epoch: 1, Batch[2150/4186], Train loss :0.189, Train acc: 0.969
[2022-10-13 15:40:54] - INFO: Epoch: 1, Batch[2160/4186], Train loss :0.177, Train acc: 0.938
[2022-10-13 15:40:56] - INFO: Epoch: 1, Batch[2170/4186], Train loss :0.414, Train acc: 0.859
[2022-10-13 15:40:58] - INFO: Epoch: 1, Batch[2180/4186], Train loss :0.359, Train acc: 0.906
[2022-10-13 15:41:01] - INFO: Epoch: 1, Batch[2190/4186], Train loss :0.383, Train acc: 0.891
[2022-10-13 15:41:03] - INFO: Epoch: 1, Batch[2200/4186], Train loss :0.482, Train acc: 0.812
[2022-10-13 15:41:05] - INFO: Epoch: 1, Batch[2210/4186], Train loss :0.387, Train acc: 0.875
[2022-10-13 15:41:07] - INFO: Epoch: 1, Batch[2220/4186], Train loss :0.160, Train acc: 0.922
[2022-10-13 15:41:10] - INFO: Epoch: 1, Batch[2230/4186], Train loss :0.220, Train acc: 0.922
[2022-10-13 15:41:12] - INFO: Epoch: 1, Batch[2240/4186], Train loss :0.419, Train acc: 0.875
[2022-10-13 15:41:14] - INFO: Epoch: 1, Batch[2250/4186], Train loss :0.466, Train acc: 0.891
[2022-10-13 15:41:17] - INFO: Epoch: 1, Batch[2260/4186], Train loss :0.340, Train acc: 0.906
[2022-10-13 15:41:19] - INFO: Epoch: 1, Batch[2270/4186], Train loss :0.554, Train acc: 0.797
[2022-10-13 15:41:21] - INFO: Epoch: 1, Batch[2280/4186], Train loss :0.436, Train acc: 0.859
[2022-10-13 15:41:24] - INFO: Epoch: 1, Batch[2290/4186], Train loss :0.354, Train acc: 0.844
[2022-10-13 15:41:26] - INFO: Epoch: 1, Batch[2300/4186], Train loss :0.516, Train acc: 0.891
[2022-10-13 15:41:28] - INFO: Epoch: 1, Batch[2310/4186], Train loss :0.420, Train acc: 0.875
[2022-10-13 15:41:30] - INFO: Epoch: 1, Batch[2320/4186], Train loss :0.700, Train acc: 0.812
[2022-10-13 15:41:33] - INFO: Epoch: 1, Batch[2330/4186], Train loss :0.181, Train acc: 0.938
[2022-10-13 15:41:35] - INFO: Epoch: 1, Batch[2340/4186], Train loss :0.465, Train acc: 0.875
[2022-10-13 15:41:37] - INFO: Epoch: 1, Batch[2350/4186], Train loss :0.322, Train acc: 0.906
[2022-10-13 15:41:40] - INFO: Epoch: 1, Batch[2360/4186], Train loss :0.472, Train acc: 0.859
[2022-10-13 15:41:42] - INFO: Epoch: 1, Batch[2370/4186], Train loss :0.215, Train acc: 0.922
[2022-10-13 15:41:44] - INFO: Epoch: 1, Batch[2380/4186], Train loss :0.290, Train acc: 0.906
[2022-10-13 15:41:47] - INFO: Epoch: 1, Batch[2390/4186], Train loss :0.185, Train acc: 0.938
[2022-10-13 15:41:49] - INFO: Epoch: 1, Batch[2400/4186], Train loss :0.413, Train acc: 0.844
[2022-10-13 15:41:51] - INFO: Epoch: 1, Batch[2410/4186], Train loss :0.321, Train acc: 0.906
[2022-10-13 15:41:53] - INFO: Epoch: 1, Batch[2420/4186], Train loss :0.317, Train acc: 0.859
[2022-10-13 15:41:56] - INFO: Epoch: 1, Batch[2430/4186], Train loss :0.372, Train acc: 0.906
[2022-10-13 15:41:58] - INFO: Epoch: 1, Batch[2440/4186], Train loss :0.185, Train acc: 0.969
[2022-10-13 15:42:00] - INFO: Epoch: 1, Batch[2450/4186], Train loss :0.455, Train acc: 0.859
[2022-10-13 15:42:03] - INFO: Epoch: 1, Batch[2460/4186], Train loss :0.186, Train acc: 0.938
[2022-10-13 15:42:05] - INFO: Epoch: 1, Batch[2470/4186], Train loss :0.308, Train acc: 0.922
[2022-10-13 15:42:07] - INFO: Epoch: 1, Batch[2480/4186], Train loss :0.374, Train acc: 0.891
[2022-10-13 15:42:10] - INFO: Epoch: 1, Batch[2490/4186], Train loss :0.421, Train acc: 0.875
[2022-10-13 15:42:12] - INFO: Epoch: 1, Batch[2500/4186], Train loss :0.233, Train acc: 0.922
[2022-10-13 15:42:14] - INFO: Epoch: 1, Batch[2510/4186], Train loss :0.381, Train acc: 0.906
[2022-10-13 15:42:17] - INFO: Epoch: 1, Batch[2520/4186], Train loss :0.268, Train acc: 0.906
[2022-10-13 15:42:19] - INFO: Epoch: 1, Batch[2530/4186], Train loss :0.175, Train acc: 0.953
[2022-10-13 15:42:21] - INFO: Epoch: 1, Batch[2540/4186], Train loss :0.205, Train acc: 0.922
[2022-10-13 15:42:24] - INFO: Epoch: 1, Batch[2550/4186], Train loss :0.205, Train acc: 0.922
[2022-10-13 15:42:26] - INFO: Epoch: 1, Batch[2560/4186], Train loss :0.528, Train acc: 0.828
[2022-10-13 15:42:28] - INFO: Epoch: 1, Batch[2570/4186], Train loss :0.303, Train acc: 0.922
[2022-10-13 15:42:31] - INFO: Epoch: 1, Batch[2580/4186], Train loss :0.480, Train acc: 0.859
[2022-10-13 15:42:33] - INFO: Epoch: 1, Batch[2590/4186], Train loss :0.322, Train acc: 0.875
[2022-10-13 15:42:35] - INFO: Epoch: 1, Batch[2600/4186], Train loss :0.197, Train acc: 0.953
[2022-10-13 15:42:38] - INFO: Epoch: 1, Batch[2610/4186], Train loss :0.225, Train acc: 0.906
[2022-10-13 15:42:40] - INFO: Epoch: 1, Batch[2620/4186], Train loss :0.457, Train acc: 0.859
[2022-10-13 15:42:42] - INFO: Epoch: 1, Batch[2630/4186], Train loss :0.481, Train acc: 0.828
[2022-10-13 15:42:45] - INFO: Epoch: 1, Batch[2640/4186], Train loss :0.543, Train acc: 0.797
[2022-10-13 15:42:47] - INFO: Epoch: 1, Batch[2650/4186], Train loss :0.324, Train acc: 0.938
[2022-10-13 15:42:49] - INFO: Epoch: 1, Batch[2660/4186], Train loss :0.543, Train acc: 0.812
[2022-10-13 15:42:52] - INFO: Epoch: 1, Batch[2670/4186], Train loss :0.368, Train acc: 0.891
[2022-10-13 15:42:54] - INFO: Epoch: 1, Batch[2680/4186], Train loss :0.146, Train acc: 0.953
[2022-10-13 15:42:56] - INFO: Epoch: 1, Batch[2690/4186], Train loss :0.336, Train acc: 0.875
[2022-10-13 15:42:58] - INFO: Epoch: 1, Batch[2700/4186], Train loss :0.378, Train acc: 0.922
[2022-10-13 15:43:01] - INFO: Epoch: 1, Batch[2710/4186], Train loss :0.382, Train acc: 0.859
[2022-10-13 15:43:03] - INFO: Epoch: 1, Batch[2720/4186], Train loss :0.203, Train acc: 0.922
[2022-10-13 15:43:05] - INFO: Epoch: 1, Batch[2730/4186], Train loss :0.467, Train acc: 0.859
[2022-10-13 15:43:08] - INFO: Epoch: 1, Batch[2740/4186], Train loss :0.331, Train acc: 0.938
[2022-10-13 15:43:10] - INFO: Epoch: 1, Batch[2750/4186], Train loss :0.459, Train acc: 0.844
[2022-10-13 15:43:12] - INFO: Epoch: 1, Batch[2760/4186], Train loss :0.396, Train acc: 0.891
[2022-10-13 15:43:15] - INFO: Epoch: 1, Batch[2770/4186], Train loss :0.326, Train acc: 0.891
[2022-10-13 15:43:17] - INFO: Epoch: 1, Batch[2780/4186], Train loss :0.298, Train acc: 0.922
[2022-10-13 15:43:19] - INFO: Epoch: 1, Batch[2790/4186], Train loss :0.328, Train acc: 0.891
[2022-10-13 15:43:22] - INFO: Epoch: 1, Batch[2800/4186], Train loss :0.352, Train acc: 0.938
[2022-10-13 15:43:24] - INFO: Epoch: 1, Batch[2810/4186], Train loss :0.316, Train acc: 0.922
[2022-10-13 15:43:27] - INFO: Epoch: 1, Batch[2820/4186], Train loss :0.281, Train acc: 0.953
[2022-10-13 15:43:29] - INFO: Epoch: 1, Batch[2830/4186], Train loss :0.152, Train acc: 0.953
[2022-10-13 15:43:31] - INFO: Epoch: 1, Batch[2840/4186], Train loss :0.310, Train acc: 0.906
[2022-10-13 15:43:33] - INFO: Epoch: 1, Batch[2850/4186], Train loss :0.275, Train acc: 0.906
[2022-10-13 15:43:36] - INFO: Epoch: 1, Batch[2860/4186], Train loss :0.465, Train acc: 0.891
[2022-10-13 15:43:38] - INFO: Epoch: 1, Batch[2870/4186], Train loss :0.441, Train acc: 0.844
[2022-10-13 15:43:41] - INFO: Epoch: 1, Batch[2880/4186], Train loss :0.573, Train acc: 0.844
[2022-10-13 15:43:43] - INFO: Epoch: 1, Batch[2890/4186], Train loss :0.223, Train acc: 0.969
[2022-10-13 15:43:45] - INFO: Epoch: 1, Batch[2900/4186], Train loss :0.435, Train acc: 0.906
[2022-10-13 15:43:47] - INFO: Epoch: 1, Batch[2910/4186], Train loss :0.205, Train acc: 0.906
[2022-10-13 15:43:50] - INFO: Epoch: 1, Batch[2920/4186], Train loss :0.482, Train acc: 0.891
[2022-10-13 15:43:52] - INFO: Epoch: 1, Batch[2930/4186], Train loss :0.225, Train acc: 0.922
[2022-10-13 15:43:54] - INFO: Epoch: 1, Batch[2940/4186], Train loss :0.335, Train acc: 0.906
[2022-10-13 15:43:57] - INFO: Epoch: 1, Batch[2950/4186], Train loss :0.425, Train acc: 0.844
[2022-10-13 15:43:59] - INFO: Epoch: 1, Batch[2960/4186], Train loss :0.429, Train acc: 0.859
[2022-10-13 15:44:01] - INFO: Epoch: 1, Batch[2970/4186], Train loss :0.474, Train acc: 0.859
[2022-10-13 15:44:03] - INFO: Epoch: 1, Batch[2980/4186], Train loss :0.249, Train acc: 0.938
[2022-10-13 15:44:06] - INFO: Epoch: 1, Batch[2990/4186], Train loss :0.502, Train acc: 0.891
[2022-10-13 15:44:08] - INFO: Epoch: 1, Batch[3000/4186], Train loss :0.388, Train acc: 0.906
[2022-10-13 15:44:10] - INFO: Epoch: 1, Batch[3010/4186], Train loss :0.506, Train acc: 0.844
[2022-10-13 15:44:13] - INFO: Epoch: 1, Batch[3020/4186], Train loss :0.132, Train acc: 0.969
[2022-10-13 15:44:15] - INFO: Epoch: 1, Batch[3030/4186], Train loss :0.429, Train acc: 0.875
[2022-10-13 15:44:17] - INFO: Epoch: 1, Batch[3040/4186], Train loss :0.392, Train acc: 0.828
[2022-10-13 15:44:19] - INFO: Epoch: 1, Batch[3050/4186], Train loss :0.332, Train acc: 0.891
[2022-10-13 15:44:22] - INFO: Epoch: 1, Batch[3060/4186], Train loss :0.235, Train acc: 0.953
[2022-10-13 15:44:24] - INFO: Epoch: 1, Batch[3070/4186], Train loss :0.262, Train acc: 0.906
[2022-10-13 15:44:26] - INFO: Epoch: 1, Batch[3080/4186], Train loss :0.386, Train acc: 0.859
[2022-10-13 15:44:29] - INFO: Epoch: 1, Batch[3090/4186], Train loss :0.310, Train acc: 0.906
[2022-10-13 15:44:31] - INFO: Epoch: 1, Batch[3100/4186], Train loss :0.151, Train acc: 0.969
[2022-10-13 15:44:33] - INFO: Epoch: 1, Batch[3110/4186], Train loss :0.207, Train acc: 0.953
[2022-10-13 15:44:36] - INFO: Epoch: 1, Batch[3120/4186], Train loss :0.385, Train acc: 0.891
[2022-10-13 15:44:38] - INFO: Epoch: 1, Batch[3130/4186], Train loss :0.317, Train acc: 0.875
[2022-10-13 15:44:40] - INFO: Epoch: 1, Batch[3140/4186], Train loss :0.365, Train acc: 0.906
[2022-10-13 15:44:43] - INFO: Epoch: 1, Batch[3150/4186], Train loss :0.243, Train acc: 0.922
[2022-10-13 15:44:45] - INFO: Epoch: 1, Batch[3160/4186], Train loss :0.270, Train acc: 0.891
[2022-10-13 15:44:47] - INFO: Epoch: 1, Batch[3170/4186], Train loss :0.275, Train acc: 0.906
[2022-10-13 15:44:49] - INFO: Epoch: 1, Batch[3180/4186], Train loss :0.243, Train acc: 0.922
[2022-10-13 15:44:52] - INFO: Epoch: 1, Batch[3190/4186], Train loss :0.454, Train acc: 0.859
[2022-10-13 15:44:54] - INFO: Epoch: 1, Batch[3200/4186], Train loss :0.258, Train acc: 0.906
[2022-10-13 15:44:56] - INFO: Epoch: 1, Batch[3210/4186], Train loss :0.512, Train acc: 0.812
[2022-10-13 15:44:59] - INFO: Epoch: 1, Batch[3220/4186], Train loss :0.594, Train acc: 0.828
[2022-10-13 15:45:01] - INFO: Epoch: 1, Batch[3230/4186], Train loss :0.270, Train acc: 0.953
[2022-10-13 15:45:03] - INFO: Epoch: 1, Batch[3240/4186], Train loss :0.464, Train acc: 0.844
[2022-10-13 15:45:06] - INFO: Epoch: 1, Batch[3250/4186], Train loss :0.311, Train acc: 0.891
[2022-10-13 15:45:08] - INFO: Epoch: 1, Batch[3260/4186], Train loss :0.357, Train acc: 0.891
[2022-10-13 15:45:10] - INFO: Epoch: 1, Batch[3270/4186], Train loss :0.224, Train acc: 0.922
[2022-10-13 15:45:13] - INFO: Epoch: 1, Batch[3280/4186], Train loss :0.388, Train acc: 0.875
[2022-10-13 15:45:15] - INFO: Epoch: 1, Batch[3290/4186], Train loss :0.368, Train acc: 0.891
[2022-10-13 15:45:17] - INFO: Epoch: 1, Batch[3300/4186], Train loss :0.614, Train acc: 0.844
[2022-10-13 15:45:20] - INFO: Epoch: 1, Batch[3310/4186], Train loss :0.310, Train acc: 0.922
[2022-10-13 15:45:22] - INFO: Epoch: 1, Batch[3320/4186], Train loss :0.427, Train acc: 0.875
[2022-10-13 15:45:24] - INFO: Epoch: 1, Batch[3330/4186], Train loss :0.376, Train acc: 0.906
[2022-10-13 15:45:26] - INFO: Epoch: 1, Batch[3340/4186], Train loss :0.509, Train acc: 0.812
[2022-10-13 15:45:29] - INFO: Epoch: 1, Batch[3350/4186], Train loss :0.441, Train acc: 0.859
[2022-10-13 15:45:31] - INFO: Epoch: 1, Batch[3360/4186], Train loss :0.496, Train acc: 0.891
[2022-10-13 15:45:33] - INFO: Epoch: 1, Batch[3370/4186], Train loss :0.189, Train acc: 0.969
[2022-10-13 15:45:36] - INFO: Epoch: 1, Batch[3380/4186], Train loss :0.417, Train acc: 0.875
[2022-10-13 15:45:38] - INFO: Epoch: 1, Batch[3390/4186], Train loss :0.338, Train acc: 0.922
[2022-10-13 15:45:41] - INFO: Epoch: 1, Batch[3400/4186], Train loss :0.689, Train acc: 0.828
[2022-10-13 15:45:43] - INFO: Epoch: 1, Batch[3410/4186], Train loss :0.314, Train acc: 0.891
[2022-10-13 15:45:45] - INFO: Epoch: 1, Batch[3420/4186], Train loss :0.376, Train acc: 0.906
[2022-10-13 15:45:48] - INFO: Epoch: 1, Batch[3430/4186], Train loss :0.417, Train acc: 0.875
[2022-10-13 15:45:50] - INFO: Epoch: 1, Batch[3440/4186], Train loss :0.176, Train acc: 0.953
[2022-10-13 15:45:52] - INFO: Epoch: 1, Batch[3450/4186], Train loss :0.509, Train acc: 0.859
[2022-10-13 15:45:54] - INFO: Epoch: 1, Batch[3460/4186], Train loss :0.230, Train acc: 0.938
[2022-10-13 15:45:57] - INFO: Epoch: 1, Batch[3470/4186], Train loss :0.492, Train acc: 0.812
[2022-10-13 15:45:59] - INFO: Epoch: 1, Batch[3480/4186], Train loss :0.133, Train acc: 0.953
[2022-10-13 15:46:01] - INFO: Epoch: 1, Batch[3490/4186], Train loss :0.427, Train acc: 0.828
[2022-10-13 15:46:04] - INFO: Epoch: 1, Batch[3500/4186], Train loss :0.626, Train acc: 0.812
[2022-10-13 15:46:06] - INFO: Epoch: 1, Batch[3510/4186], Train loss :0.505, Train acc: 0.812
[2022-10-13 15:46:08] - INFO: Epoch: 1, Batch[3520/4186], Train loss :0.296, Train acc: 0.875
[2022-10-13 15:46:11] - INFO: Epoch: 1, Batch[3530/4186], Train loss :0.261, Train acc: 0.922
[2022-10-13 15:46:13] - INFO: Epoch: 1, Batch[3540/4186], Train loss :0.494, Train acc: 0.828
[2022-10-13 15:46:15] - INFO: Epoch: 1, Batch[3550/4186], Train loss :0.250, Train acc: 0.953
[2022-10-13 15:46:18] - INFO: Epoch: 1, Batch[3560/4186], Train loss :0.431, Train acc: 0.875
[2022-10-13 15:46:20] - INFO: Epoch: 1, Batch[3570/4186], Train loss :0.555, Train acc: 0.844
[2022-10-13 15:46:22] - INFO: Epoch: 1, Batch[3580/4186], Train loss :0.181, Train acc: 0.953
[2022-10-13 15:46:24] - INFO: Epoch: 1, Batch[3590/4186], Train loss :0.237, Train acc: 0.906
[2022-10-13 15:46:27] - INFO: Epoch: 1, Batch[3600/4186], Train loss :0.302, Train acc: 0.953
[2022-10-13 15:46:29] - INFO: Epoch: 1, Batch[3610/4186], Train loss :0.303, Train acc: 0.875
[2022-10-13 15:46:31] - INFO: Epoch: 1, Batch[3620/4186], Train loss :0.320, Train acc: 0.891
[2022-10-13 15:46:34] - INFO: Epoch: 1, Batch[3630/4186], Train loss :0.545, Train acc: 0.859
[2022-10-13 15:46:36] - INFO: Epoch: 1, Batch[3640/4186], Train loss :0.471, Train acc: 0.859
[2022-10-13 15:46:39] - INFO: Epoch: 1, Batch[3650/4186], Train loss :0.374, Train acc: 0.922
[2022-10-13 15:46:41] - INFO: Epoch: 1, Batch[3660/4186], Train loss :0.573, Train acc: 0.875
[2022-10-13 15:46:44] - INFO: Epoch: 1, Batch[3670/4186], Train loss :0.489, Train acc: 0.875
[2022-10-13 15:46:46] - INFO: Epoch: 1, Batch[3680/4186], Train loss :0.251, Train acc: 0.891
[2022-10-13 15:46:48] - INFO: Epoch: 1, Batch[3690/4186], Train loss :0.293, Train acc: 0.922
[2022-10-13 15:46:51] - INFO: Epoch: 1, Batch[3700/4186], Train loss :0.290, Train acc: 0.891
[2022-10-13 15:46:53] - INFO: Epoch: 1, Batch[3710/4186], Train loss :0.567, Train acc: 0.875
[2022-10-13 15:46:55] - INFO: Epoch: 1, Batch[3720/4186], Train loss :0.486, Train acc: 0.875
[2022-10-13 15:46:58] - INFO: Epoch: 1, Batch[3730/4186], Train loss :0.339, Train acc: 0.875
[2022-10-13 15:47:00] - INFO: Epoch: 1, Batch[3740/4186], Train loss :0.192, Train acc: 0.953
[2022-10-13 15:47:02] - INFO: Epoch: 1, Batch[3750/4186], Train loss :0.389, Train acc: 0.875
[2022-10-13 15:47:05] - INFO: Epoch: 1, Batch[3760/4186], Train loss :0.269, Train acc: 0.891
[2022-10-13 15:47:07] - INFO: Epoch: 1, Batch[3770/4186], Train loss :0.747, Train acc: 0.781
[2022-10-13 15:47:09] - INFO: Epoch: 1, Batch[3780/4186], Train loss :0.271, Train acc: 0.922
[2022-10-13 15:47:12] - INFO: Epoch: 1, Batch[3790/4186], Train loss :0.280, Train acc: 0.875
[2022-10-13 15:47:14] - INFO: Epoch: 1, Batch[3800/4186], Train loss :0.426, Train acc: 0.844
[2022-10-13 15:47:16] - INFO: Epoch: 1, Batch[3810/4186], Train loss :0.408, Train acc: 0.906
[2022-10-13 15:47:19] - INFO: Epoch: 1, Batch[3820/4186], Train loss :0.209, Train acc: 0.922
[2022-10-13 15:47:21] - INFO: Epoch: 1, Batch[3830/4186], Train loss :0.180, Train acc: 0.938
[2022-10-13 15:47:23] - INFO: Epoch: 1, Batch[3840/4186], Train loss :0.471, Train acc: 0.875
[2022-10-13 15:47:26] - INFO: Epoch: 1, Batch[3850/4186], Train loss :0.196, Train acc: 0.938
[2022-10-13 15:47:29] - INFO: Epoch: 1, Batch[3860/4186], Train loss :0.345, Train acc: 0.922
[2022-10-13 15:47:31] - INFO: Epoch: 1, Batch[3870/4186], Train loss :0.245, Train acc: 0.938
[2022-10-13 15:47:33] - INFO: Epoch: 1, Batch[3880/4186], Train loss :0.292, Train acc: 0.891
[2022-10-13 15:47:35] - INFO: Epoch: 1, Batch[3890/4186], Train loss :0.293, Train acc: 0.906
[2022-10-13 15:47:38] - INFO: Epoch: 1, Batch[3900/4186], Train loss :0.292, Train acc: 0.922
[2022-10-13 15:47:41] - INFO: Epoch: 1, Batch[3910/4186], Train loss :0.474, Train acc: 0.875
[2022-10-13 15:47:43] - INFO: Epoch: 1, Batch[3920/4186], Train loss :0.441, Train acc: 0.844
[2022-10-13 15:47:45] - INFO: Epoch: 1, Batch[3930/4186], Train loss :0.310, Train acc: 0.875
[2022-10-13 15:47:48] - INFO: Epoch: 1, Batch[3940/4186], Train loss :0.473, Train acc: 0.875
[2022-10-13 15:47:50] - INFO: Epoch: 1, Batch[3950/4186], Train loss :0.334, Train acc: 0.906
[2022-10-13 15:47:52] - INFO: Epoch: 1, Batch[3960/4186], Train loss :0.293, Train acc: 0.922
[2022-10-13 15:47:55] - INFO: Epoch: 1, Batch[3970/4186], Train loss :0.442, Train acc: 0.906
[2022-10-13 15:47:57] - INFO: Epoch: 1, Batch[3980/4186], Train loss :0.450, Train acc: 0.891
[2022-10-13 15:47:59] - INFO: Epoch: 1, Batch[3990/4186], Train loss :0.248, Train acc: 0.922
[2022-10-13 15:48:01] - INFO: Epoch: 1, Batch[4000/4186], Train loss :0.345, Train acc: 0.922
[2022-10-13 15:48:04] - INFO: Epoch: 1, Batch[4010/4186], Train loss :0.314, Train acc: 0.891
[2022-10-13 15:48:06] - INFO: Epoch: 1, Batch[4020/4186], Train loss :0.267, Train acc: 0.906
[2022-10-13 15:48:09] - INFO: Epoch: 1, Batch[4030/4186], Train loss :0.400, Train acc: 0.922
[2022-10-13 15:48:11] - INFO: Epoch: 1, Batch[4040/4186], Train loss :0.252, Train acc: 0.891
[2022-10-13 15:48:13] - INFO: Epoch: 1, Batch[4050/4186], Train loss :0.249, Train acc: 0.922
[2022-10-13 15:48:16] - INFO: Epoch: 1, Batch[4060/4186], Train loss :0.416, Train acc: 0.891
[2022-10-13 15:48:18] - INFO: Epoch: 1, Batch[4070/4186], Train loss :0.388, Train acc: 0.891
[2022-10-13 15:48:20] - INFO: Epoch: 1, Batch[4080/4186], Train loss :0.360, Train acc: 0.891
[2022-10-13 15:48:23] - INFO: Epoch: 1, Batch[4090/4186], Train loss :0.595, Train acc: 0.828
[2022-10-13 15:48:25] - INFO: Epoch: 1, Batch[4100/4186], Train loss :0.429, Train acc: 0.891
[2022-10-13 15:48:27] - INFO: Epoch: 1, Batch[4110/4186], Train loss :0.486, Train acc: 0.844
[2022-10-13 15:48:30] - INFO: Epoch: 1, Batch[4120/4186], Train loss :0.442, Train acc: 0.859
[2022-10-13 15:48:32] - INFO: Epoch: 1, Batch[4130/4186], Train loss :0.185, Train acc: 0.969
[2022-10-13 15:48:34] - INFO: Epoch: 1, Batch[4140/4186], Train loss :0.487, Train acc: 0.859
[2022-10-13 15:48:36] - INFO: Epoch: 1, Batch[4150/4186], Train loss :0.362, Train acc: 0.922
[2022-10-13 15:48:39] - INFO: Epoch: 1, Batch[4160/4186], Train loss :0.252, Train acc: 0.906
[2022-10-13 15:48:41] - INFO: Epoch: 1, Batch[4170/4186], Train loss :0.506, Train acc: 0.859
[2022-10-13 15:48:43] - INFO: Epoch: 1, Batch[4180/4186], Train loss :0.400, Train acc: 0.891
[2022-10-13 15:48:44] - INFO: Epoch: 1, Train loss: 0.345, Epoch time = 973.728s
[2022-10-13 15:50:14] - INFO: Accuracy on val 0.889
[2022-10-13 15:50:15] - INFO: Epoch: 2, Batch[0/4186], Train loss :0.229, Train acc: 0.969
[2022-10-13 15:50:17] - INFO: Epoch: 2, Batch[10/4186], Train loss :0.159, Train acc: 0.938
[2022-10-13 15:50:19] - INFO: Epoch: 2, Batch[20/4186], Train loss :0.246, Train acc: 0.938
[2022-10-13 15:50:22] - INFO: Epoch: 2, Batch[30/4186], Train loss :0.148, Train acc: 0.969
[2022-10-13 15:50:24] - INFO: Epoch: 2, Batch[40/4186], Train loss :0.301, Train acc: 0.953
[2022-10-13 15:50:26] - INFO: Epoch: 2, Batch[50/4186], Train loss :0.230, Train acc: 0.938
[2022-10-13 15:50:28] - INFO: Epoch: 2, Batch[60/4186], Train loss :0.499, Train acc: 0.844
[2022-10-13 15:50:31] - INFO: Epoch: 2, Batch[70/4186], Train loss :0.260, Train acc: 0.922
[2022-10-13 15:50:33] - INFO: Epoch: 2, Batch[80/4186], Train loss :0.548, Train acc: 0.859
[2022-10-13 15:50:35] - INFO: Epoch: 2, Batch[90/4186], Train loss :0.209, Train acc: 0.938
[2022-10-13 15:50:38] - INFO: Epoch: 2, Batch[100/4186], Train loss :0.357, Train acc: 0.906
[2022-10-13 15:50:40] - INFO: Epoch: 2, Batch[110/4186], Train loss :0.282, Train acc: 0.922
[2022-10-13 15:50:42] - INFO: Epoch: 2, Batch[120/4186], Train loss :0.228, Train acc: 0.906
[2022-10-13 15:50:45] - INFO: Epoch: 2, Batch[130/4186], Train loss :0.427, Train acc: 0.859
[2022-10-13 15:50:47] - INFO: Epoch: 2, Batch[140/4186], Train loss :0.125, Train acc: 0.953
[2022-10-13 15:50:49] - INFO: Epoch: 2, Batch[150/4186], Train loss :0.197, Train acc: 0.953
[2022-10-13 15:50:51] - INFO: Epoch: 2, Batch[160/4186], Train loss :0.058, Train acc: 1.000
[2022-10-13 15:50:54] - INFO: Epoch: 2, Batch[170/4186], Train loss :0.375, Train acc: 0.891
[2022-10-13 15:50:56] - INFO: Epoch: 2, Batch[180/4186], Train loss :0.326, Train acc: 0.906
[2022-10-13 15:50:58] - INFO: Epoch: 2, Batch[190/4186], Train loss :0.118, Train acc: 0.984
[2022-10-13 15:51:01] - INFO: Epoch: 2, Batch[200/4186], Train loss :0.300, Train acc: 0.906
[2022-10-13 15:51:03] - INFO: Epoch: 2, Batch[210/4186], Train loss :0.097, Train acc: 0.969
[2022-10-13 15:51:05] - INFO: Epoch: 2, Batch[220/4186], Train loss :0.247, Train acc: 0.938
[2022-10-13 15:51:08] - INFO: Epoch: 2, Batch[230/4186], Train loss :0.310, Train acc: 0.938
[2022-10-13 15:51:10] - INFO: Epoch: 2, Batch[240/4186], Train loss :0.154, Train acc: 0.938
[2022-10-13 15:51:12] - INFO: Epoch: 2, Batch[250/4186], Train loss :0.254, Train acc: 0.922
[2022-10-13 15:51:14] - INFO: Epoch: 2, Batch[260/4186], Train loss :0.245, Train acc: 0.906
[2022-10-13 15:51:17] - INFO: Epoch: 2, Batch[270/4186], Train loss :0.447, Train acc: 0.891
[2022-10-13 15:51:19] - INFO: Epoch: 2, Batch[280/4186], Train loss :0.397, Train acc: 0.891
[2022-10-13 15:51:21] - INFO: Epoch: 2, Batch[290/4186], Train loss :0.296, Train acc: 0.906
[2022-10-13 15:51:24] - INFO: Epoch: 2, Batch[300/4186], Train loss :0.171, Train acc: 0.969
[2022-10-13 15:51:26] - INFO: Epoch: 2, Batch[310/4186], Train loss :0.319, Train acc: 0.906
[2022-10-13 15:51:28] - INFO: Epoch: 2, Batch[320/4186], Train loss :0.208, Train acc: 0.906
[2022-10-13 15:51:31] - INFO: Epoch: 2, Batch[330/4186], Train loss :0.372, Train acc: 0.844
[2022-10-13 15:51:33] - INFO: Epoch: 2, Batch[340/4186], Train loss :0.190, Train acc: 0.953
[2022-10-13 15:51:35] - INFO: Epoch: 2, Batch[350/4186], Train loss :0.314, Train acc: 0.906
[2022-10-13 15:51:37] - INFO: Epoch: 2, Batch[360/4186], Train loss :0.286, Train acc: 0.906
[2022-10-13 15:51:40] - INFO: Epoch: 2, Batch[370/4186], Train loss :0.247, Train acc: 0.953
[2022-10-13 15:51:42] - INFO: Epoch: 2, Batch[380/4186], Train loss :0.520, Train acc: 0.875
[2022-10-13 15:51:44] - INFO: Epoch: 2, Batch[390/4186], Train loss :0.167, Train acc: 0.938
[2022-10-13 15:51:47] - INFO: Epoch: 2, Batch[400/4186], Train loss :0.249, Train acc: 0.906
[2022-10-13 15:51:49] - INFO: Epoch: 2, Batch[410/4186], Train loss :0.186, Train acc: 0.953
[2022-10-13 15:51:51] - INFO: Epoch: 2, Batch[420/4186], Train loss :0.341, Train acc: 0.922
[2022-10-13 15:51:54] - INFO: Epoch: 2, Batch[430/4186], Train loss :0.144, Train acc: 0.953
[2022-10-13 15:51:56] - INFO: Epoch: 2, Batch[440/4186], Train loss :0.227, Train acc: 0.953
[2022-10-13 15:51:58] - INFO: Epoch: 2, Batch[450/4186], Train loss :0.376, Train acc: 0.859
[2022-10-13 15:52:01] - INFO: Epoch: 2, Batch[460/4186], Train loss :0.362, Train acc: 0.891
[2022-10-13 15:52:03] - INFO: Epoch: 2, Batch[470/4186], Train loss :0.120, Train acc: 0.969
[2022-10-13 15:52:05] - INFO: Epoch: 2, Batch[480/4186], Train loss :0.227, Train acc: 0.938
[2022-10-13 15:52:08] - INFO: Epoch: 2, Batch[490/4186], Train loss :0.211, Train acc: 0.922
[2022-10-13 15:52:10] - INFO: Epoch: 2, Batch[500/4186], Train loss :0.181, Train acc: 0.953
[2022-10-13 15:52:12] - INFO: Epoch: 2, Batch[510/4186], Train loss :0.216, Train acc: 0.969
[2022-10-13 15:52:15] - INFO: Epoch: 2, Batch[520/4186], Train loss :0.324, Train acc: 0.922
[2022-10-13 15:52:17] - INFO: Epoch: 2, Batch[530/4186], Train loss :0.445, Train acc: 0.891
[2022-10-13 15:52:19] - INFO: Epoch: 2, Batch[540/4186], Train loss :0.149, Train acc: 0.969
[2022-10-13 15:52:22] - INFO: Epoch: 2, Batch[550/4186], Train loss :0.211, Train acc: 0.922
[2022-10-13 15:52:24] - INFO: Epoch: 2, Batch[560/4186], Train loss :0.208, Train acc: 0.922
[2022-10-13 15:52:26] - INFO: Epoch: 2, Batch[570/4186], Train loss :0.241, Train acc: 0.922
[2022-10-13 15:52:29] - INFO: Epoch: 2, Batch[580/4186], Train loss :0.251, Train acc: 0.891
[2022-10-13 15:52:31] - INFO: Epoch: 2, Batch[590/4186], Train loss :0.228, Train acc: 0.938
[2022-10-13 15:52:33] - INFO: Epoch: 2, Batch[600/4186], Train loss :0.582, Train acc: 0.875
[2022-10-13 15:52:36] - INFO: Epoch: 2, Batch[610/4186], Train loss :0.384, Train acc: 0.859
[2022-10-13 15:52:38] - INFO: Epoch: 2, Batch[620/4186], Train loss :0.333, Train acc: 0.891
[2022-10-13 15:52:40] - INFO: Epoch: 2, Batch[630/4186], Train loss :0.160, Train acc: 0.953
[2022-10-13 15:52:42] - INFO: Epoch: 2, Batch[640/4186], Train loss :0.293, Train acc: 0.922
[2022-10-13 15:52:45] - INFO: Epoch: 2, Batch[650/4186], Train loss :0.266, Train acc: 0.891
[2022-10-13 15:52:47] - INFO: Epoch: 2, Batch[660/4186], Train loss :0.178, Train acc: 0.938
[2022-10-13 15:52:49] - INFO: Epoch: 2, Batch[670/4186], Train loss :0.452, Train acc: 0.875
[2022-10-13 15:52:52] - INFO: Epoch: 2, Batch[680/4186], Train loss :0.337, Train acc: 0.844
[2022-10-13 15:52:54] - INFO: Epoch: 2, Batch[690/4186], Train loss :0.271, Train acc: 0.906
[2022-10-13 15:52:56] - INFO: Epoch: 2, Batch[700/4186], Train loss :0.341, Train acc: 0.891
[2022-10-13 15:52:59] - INFO: Epoch: 2, Batch[710/4186], Train loss :0.162, Train acc: 0.938
[2022-10-13 15:53:01] - INFO: Epoch: 2, Batch[720/4186], Train loss :0.046, Train acc: 0.984
[2022-10-13 15:53:03] - INFO: Epoch: 2, Batch[730/4186], Train loss :0.151, Train acc: 0.938
[2022-10-13 15:53:06] - INFO: Epoch: 2, Batch[740/4186], Train loss :0.187, Train acc: 0.938
[2022-10-13 15:53:08] - INFO: Epoch: 2, Batch[750/4186], Train loss :0.203, Train acc: 0.953
[2022-10-13 15:53:10] - INFO: Epoch: 2, Batch[760/4186], Train loss :0.317, Train acc: 0.891
[2022-10-13 15:53:12] - INFO: Epoch: 2, Batch[770/4186], Train loss :0.160, Train acc: 0.938
[2022-10-13 15:53:15] - INFO: Epoch: 2, Batch[780/4186], Train loss :0.151, Train acc: 0.953
[2022-10-13 15:53:17] - INFO: Epoch: 2, Batch[790/4186], Train loss :0.134, Train acc: 0.969
[2022-10-13 15:53:19] - INFO: Epoch: 2, Batch[800/4186], Train loss :0.291, Train acc: 0.922
[2022-10-13 15:53:22] - INFO: Epoch: 2, Batch[810/4186], Train loss :0.279, Train acc: 0.906
[2022-10-13 15:53:24] - INFO: Epoch: 2, Batch[820/4186], Train loss :0.175, Train acc: 0.922
[2022-10-13 15:53:26] - INFO: Epoch: 2, Batch[830/4186], Train loss :0.330, Train acc: 0.859
[2022-10-13 15:53:28] - INFO: Epoch: 2, Batch[840/4186], Train loss :0.201, Train acc: 0.953
[2022-10-13 15:53:31] - INFO: Epoch: 2, Batch[850/4186], Train loss :0.160, Train acc: 0.953
[2022-10-13 15:53:33] - INFO: Epoch: 2, Batch[860/4186], Train loss :0.271, Train acc: 0.891
[2022-10-13 15:53:36] - INFO: Epoch: 2, Batch[870/4186], Train loss :0.302, Train acc: 0.891
[2022-10-13 15:53:38] - INFO: Epoch: 2, Batch[880/4186], Train loss :0.138, Train acc: 0.938
[2022-10-13 15:53:40] - INFO: Epoch: 2, Batch[890/4186], Train loss :0.116, Train acc: 0.969
[2022-10-13 15:53:42] - INFO: Epoch: 2, Batch[900/4186], Train loss :0.237, Train acc: 0.953
[2022-10-13 15:53:45] - INFO: Epoch: 2, Batch[910/4186], Train loss :0.516, Train acc: 0.844
[2022-10-13 15:53:47] - INFO: Epoch: 2, Batch[920/4186], Train loss :0.308, Train acc: 0.922
[2022-10-13 15:53:49] - INFO: Epoch: 2, Batch[930/4186], Train loss :0.150, Train acc: 0.953
[2022-10-13 15:53:52] - INFO: Epoch: 2, Batch[940/4186], Train loss :0.195, Train acc: 0.969
[2022-10-13 15:53:54] - INFO: Epoch: 2, Batch[950/4186], Train loss :0.259, Train acc: 0.922
[2022-10-13 15:53:56] - INFO: Epoch: 2, Batch[960/4186], Train loss :0.251, Train acc: 0.906
[2022-10-13 15:53:58] - INFO: Epoch: 2, Batch[970/4186], Train loss :0.452, Train acc: 0.875
[2022-10-13 15:54:01] - INFO: Epoch: 2, Batch[980/4186], Train loss :0.235, Train acc: 0.906
[2022-10-13 15:54:03] - INFO: Epoch: 2, Batch[990/4186], Train loss :0.247, Train acc: 0.938
[2022-10-13 15:54:05] - INFO: Epoch: 2, Batch[1000/4186], Train loss :0.173, Train acc: 0.953
[2022-10-13 15:54:07] - INFO: Epoch: 2, Batch[1010/4186], Train loss :0.351, Train acc: 0.938
[2022-10-13 15:54:10] - INFO: Epoch: 2, Batch[1020/4186], Train loss :0.154, Train acc: 0.953
[2022-10-13 15:54:12] - INFO: Epoch: 2, Batch[1030/4186], Train loss :0.236, Train acc: 0.922
[2022-10-13 15:54:14] - INFO: Epoch: 2, Batch[1040/4186], Train loss :0.412, Train acc: 0.875
[2022-10-13 15:54:17] - INFO: Epoch: 2, Batch[1050/4186], Train loss :0.375, Train acc: 0.891
[2022-10-13 15:54:19] - INFO: Epoch: 2, Batch[1060/4186], Train loss :0.176, Train acc: 0.953
[2022-10-13 15:54:21] - INFO: Epoch: 2, Batch[1070/4186], Train loss :0.322, Train acc: 0.891
[2022-10-13 15:54:23] - INFO: Epoch: 2, Batch[1080/4186], Train loss :0.258, Train acc: 0.922
[2022-10-13 15:54:26] - INFO: Epoch: 2, Batch[1090/4186], Train loss :0.403, Train acc: 0.875
[2022-10-13 15:54:28] - INFO: Epoch: 2, Batch[1100/4186], Train loss :0.158, Train acc: 0.922
[2022-10-13 15:54:30] - INFO: Epoch: 2, Batch[1110/4186], Train loss :0.217, Train acc: 0.953
[2022-10-13 15:54:33] - INFO: Epoch: 2, Batch[1120/4186], Train loss :0.239, Train acc: 0.938
[2022-10-13 15:54:35] - INFO: Epoch: 2, Batch[1130/4186], Train loss :0.240, Train acc: 0.953
[2022-10-13 15:54:37] - INFO: Epoch: 2, Batch[1140/4186], Train loss :0.267, Train acc: 0.938
[2022-10-13 15:54:40] - INFO: Epoch: 2, Batch[1150/4186], Train loss :0.400, Train acc: 0.891
[2022-10-13 15:54:42] - INFO: Epoch: 2, Batch[1160/4186], Train loss :0.274, Train acc: 0.922
[2022-10-13 15:54:45] - INFO: Epoch: 2, Batch[1170/4186], Train loss :0.510, Train acc: 0.875
[2022-10-13 15:54:47] - INFO: Epoch: 2, Batch[1180/4186], Train loss :0.093, Train acc: 0.969
[2022-10-13 15:54:49] - INFO: Epoch: 2, Batch[1190/4186], Train loss :0.297, Train acc: 0.922
[2022-10-13 15:54:52] - INFO: Epoch: 2, Batch[1200/4186], Train loss :0.269, Train acc: 0.906
[2022-10-13 15:54:54] - INFO: Epoch: 2, Batch[1210/4186], Train loss :0.154, Train acc: 0.938
[2022-10-13 15:54:56] - INFO: Epoch: 2, Batch[1220/4186], Train loss :0.056, Train acc: 0.984
[2022-10-13 15:54:59] - INFO: Epoch: 2, Batch[1230/4186], Train loss :0.274, Train acc: 0.922
[2022-10-13 15:55:01] - INFO: Epoch: 2, Batch[1240/4186], Train loss :0.179, Train acc: 0.953
[2022-10-13 15:55:03] - INFO: Epoch: 2, Batch[1250/4186], Train loss :0.332, Train acc: 0.875
[2022-10-13 15:55:05] - INFO: Epoch: 2, Batch[1260/4186], Train loss :0.177, Train acc: 0.938
[2022-10-13 15:55:08] - INFO: Epoch: 2, Batch[1270/4186], Train loss :0.338, Train acc: 0.891
[2022-10-13 15:55:10] - INFO: Epoch: 2, Batch[1280/4186], Train loss :0.355, Train acc: 0.922
[2022-10-13 15:55:12] - INFO: Epoch: 2, Batch[1290/4186], Train loss :0.230, Train acc: 0.938
[2022-10-13 15:55:15] - INFO: Epoch: 2, Batch[1300/4186], Train loss :0.362, Train acc: 0.922
[2022-10-13 15:55:17] - INFO: Epoch: 2, Batch[1310/4186], Train loss :0.212, Train acc: 0.922
[2022-10-13 15:55:19] - INFO: Epoch: 2, Batch[1320/4186], Train loss :0.241, Train acc: 0.906
[2022-10-13 15:55:22] - INFO: Epoch: 2, Batch[1330/4186], Train loss :0.164, Train acc: 0.953
[2022-10-13 15:55:24] - INFO: Epoch: 2, Batch[1340/4186], Train loss :0.164, Train acc: 0.922
[2022-10-13 15:55:26] - INFO: Epoch: 2, Batch[1350/4186], Train loss :0.314, Train acc: 0.906
[2022-10-13 15:55:29] - INFO: Epoch: 2, Batch[1360/4186], Train loss :0.181, Train acc: 0.953
[2022-10-13 15:55:31] - INFO: Epoch: 2, Batch[1370/4186], Train loss :0.287, Train acc: 0.906
[2022-10-13 15:55:33] - INFO: Epoch: 2, Batch[1380/4186], Train loss :0.211, Train acc: 0.922
[2022-10-13 15:55:35] - INFO: Epoch: 2, Batch[1390/4186], Train loss :0.158, Train acc: 0.938
[2022-10-13 15:55:38] - INFO: Epoch: 2, Batch[1400/4186], Train loss :0.165, Train acc: 0.969
[2022-10-13 15:55:40] - INFO: Epoch: 2, Batch[1410/4186], Train loss :0.183, Train acc: 0.938
[2022-10-13 15:55:42] - INFO: Epoch: 2, Batch[1420/4186], Train loss :0.381, Train acc: 0.891
[2022-10-13 15:55:44] - INFO: Epoch: 2, Batch[1430/4186], Train loss :0.372, Train acc: 0.906
[2022-10-13 15:55:47] - INFO: Epoch: 2, Batch[1440/4186], Train loss :0.307, Train acc: 0.906
[2022-10-13 15:55:49] - INFO: Epoch: 2, Batch[1450/4186], Train loss :0.328, Train acc: 0.906
[2022-10-13 15:55:51] - INFO: Epoch: 2, Batch[1460/4186], Train loss :0.344, Train acc: 0.891
[2022-10-13 15:55:54] - INFO: Epoch: 2, Batch[1470/4186], Train loss :0.168, Train acc: 0.953
[2022-10-13 15:55:56] - INFO: Epoch: 2, Batch[1480/4186], Train loss :0.203, Train acc: 0.938
[2022-10-13 15:55:58] - INFO: Epoch: 2, Batch[1490/4186], Train loss :0.248, Train acc: 0.922
[2022-10-13 15:56:01] - INFO: Epoch: 2, Batch[1500/4186], Train loss :0.209, Train acc: 0.891
[2022-10-13 15:56:03] - INFO: Epoch: 2, Batch[1510/4186], Train loss :0.364, Train acc: 0.906
[2022-10-13 15:56:05] - INFO: Epoch: 2, Batch[1520/4186], Train loss :0.160, Train acc: 0.922
[2022-10-13 15:56:07] - INFO: Epoch: 2, Batch[1530/4186], Train loss :0.228, Train acc: 0.922
[2022-10-13 15:56:10] - INFO: Epoch: 2, Batch[1540/4186], Train loss :0.128, Train acc: 0.953
[2022-10-13 15:56:13] - INFO: Epoch: 2, Batch[1550/4186], Train loss :0.380, Train acc: 0.891
[2022-10-13 15:56:15] - INFO: Epoch: 2, Batch[1560/4186], Train loss :0.204, Train acc: 0.922
[2022-10-13 15:56:18] - INFO: Epoch: 2, Batch[1570/4186], Train loss :0.127, Train acc: 0.938
[2022-10-13 15:56:20] - INFO: Epoch: 2, Batch[1580/4186], Train loss :0.205, Train acc: 0.938
[2022-10-13 15:56:22] - INFO: Epoch: 2, Batch[1590/4186], Train loss :0.395, Train acc: 0.875
[2022-10-13 15:56:25] - INFO: Epoch: 2, Batch[1600/4186], Train loss :0.323, Train acc: 0.922
[2022-10-13 15:56:27] - INFO: Epoch: 2, Batch[1610/4186], Train loss :0.190, Train acc: 0.953
[2022-10-13 15:56:29] - INFO: Epoch: 2, Batch[1620/4186], Train loss :0.335, Train acc: 0.891
[2022-10-13 15:56:32] - INFO: Epoch: 2, Batch[1630/4186], Train loss :0.230, Train acc: 0.953
[2022-10-13 15:56:34] - INFO: Epoch: 2, Batch[1640/4186], Train loss :0.342, Train acc: 0.859
[2022-10-13 15:56:36] - INFO: Epoch: 2, Batch[1650/4186], Train loss :0.348, Train acc: 0.891
[2022-10-13 15:56:39] - INFO: Epoch: 2, Batch[1660/4186], Train loss :0.170, Train acc: 0.938
[2022-10-13 15:56:41] - INFO: Epoch: 2, Batch[1670/4186], Train loss :0.109, Train acc: 0.984
[2022-10-13 15:56:43] - INFO: Epoch: 2, Batch[1680/4186], Train loss :0.218, Train acc: 0.953
[2022-10-13 15:56:46] - INFO: Epoch: 2, Batch[1690/4186], Train loss :0.143, Train acc: 0.953
[2022-10-13 15:56:48] - INFO: Epoch: 2, Batch[1700/4186], Train loss :0.289, Train acc: 0.906
[2022-10-13 15:56:50] - INFO: Epoch: 2, Batch[1710/4186], Train loss :0.234, Train acc: 0.938
[2022-10-13 15:56:52] - INFO: Epoch: 2, Batch[1720/4186], Train loss :0.159, Train acc: 0.953
[2022-10-13 15:56:55] - INFO: Epoch: 2, Batch[1730/4186], Train loss :0.211, Train acc: 0.938
[2022-10-13 15:56:57] - INFO: Epoch: 2, Batch[1740/4186], Train loss :0.488, Train acc: 0.875
[2022-10-13 15:56:59] - INFO: Epoch: 2, Batch[1750/4186], Train loss :0.212, Train acc: 0.906
[2022-10-13 15:57:02] - INFO: Epoch: 2, Batch[1760/4186], Train loss :0.323, Train acc: 0.922
[2022-10-13 15:57:04] - INFO: Epoch: 2, Batch[1770/4186], Train loss :0.285, Train acc: 0.891
[2022-10-13 15:57:06] - INFO: Epoch: 2, Batch[1780/4186], Train loss :0.143, Train acc: 0.938
[2022-10-13 15:57:09] - INFO: Epoch: 2, Batch[1790/4186], Train loss :0.125, Train acc: 0.953
[2022-10-13 15:57:11] - INFO: Epoch: 2, Batch[1800/4186], Train loss :0.340, Train acc: 0.891
[2022-10-13 15:57:13] - INFO: Epoch: 2, Batch[1810/4186], Train loss :0.330, Train acc: 0.859
[2022-10-13 15:57:16] - INFO: Epoch: 2, Batch[1820/4186], Train loss :0.233, Train acc: 0.906
[2022-10-13 15:57:18] - INFO: Epoch: 2, Batch[1830/4186], Train loss :0.242, Train acc: 0.891
[2022-10-13 15:57:20] - INFO: Epoch: 2, Batch[1840/4186], Train loss :0.247, Train acc: 0.891
[2022-10-13 15:57:23] - INFO: Epoch: 2, Batch[1850/4186], Train loss :0.318, Train acc: 0.891
[2022-10-13 15:57:25] - INFO: Epoch: 2, Batch[1860/4186], Train loss :0.237, Train acc: 0.922
[2022-10-13 15:57:27] - INFO: Epoch: 2, Batch[1870/4186], Train loss :0.294, Train acc: 0.906
[2022-10-13 15:57:29] - INFO: Epoch: 2, Batch[1880/4186], Train loss :0.261, Train acc: 0.922
[2022-10-13 15:57:32] - INFO: Epoch: 2, Batch[1890/4186], Train loss :0.256, Train acc: 0.906
[2022-10-13 15:57:34] - INFO: Epoch: 2, Batch[1900/4186], Train loss :0.237, Train acc: 0.906
[2022-10-13 15:57:36] - INFO: Epoch: 2, Batch[1910/4186], Train loss :0.435, Train acc: 0.875
[2022-10-13 15:57:39] - INFO: Epoch: 2, Batch[1920/4186], Train loss :0.262, Train acc: 0.922
[2022-10-13 15:57:41] - INFO: Epoch: 2, Batch[1930/4186], Train loss :0.359, Train acc: 0.906
[2022-10-13 15:57:43] - INFO: Epoch: 2, Batch[1940/4186], Train loss :0.462, Train acc: 0.859
[2022-10-13 15:57:46] - INFO: Epoch: 2, Batch[1950/4186], Train loss :0.224, Train acc: 0.953
[2022-10-13 15:57:48] - INFO: Epoch: 2, Batch[1960/4186], Train loss :0.411, Train acc: 0.891
[2022-10-13 15:57:50] - INFO: Epoch: 2, Batch[1970/4186], Train loss :0.136, Train acc: 0.953
[2022-10-13 15:57:53] - INFO: Epoch: 2, Batch[1980/4186], Train loss :0.257, Train acc: 0.922
[2022-10-13 15:57:55] - INFO: Epoch: 2, Batch[1990/4186], Train loss :0.317, Train acc: 0.891
[2022-10-13 15:57:57] - INFO: Epoch: 2, Batch[2000/4186], Train loss :0.155, Train acc: 0.984
[2022-10-13 15:57:59] - INFO: Epoch: 2, Batch[2010/4186], Train loss :0.228, Train acc: 0.938
[2022-10-13 15:58:02] - INFO: Epoch: 2, Batch[2020/4186], Train loss :0.276, Train acc: 0.906
[2022-10-13 15:58:04] - INFO: Epoch: 2, Batch[2030/4186], Train loss :0.186, Train acc: 0.938
[2022-10-13 15:58:06] - INFO: Epoch: 2, Batch[2040/4186], Train loss :0.334, Train acc: 0.875
[2022-10-13 15:58:09] - INFO: Epoch: 2, Batch[2050/4186], Train loss :0.180, Train acc: 0.969
[2022-10-13 15:58:11] - INFO: Epoch: 2, Batch[2060/4186], Train loss :0.574, Train acc: 0.859
[2022-10-13 15:58:13] - INFO: Epoch: 2, Batch[2070/4186], Train loss :0.408, Train acc: 0.859
[2022-10-13 15:58:16] - INFO: Epoch: 2, Batch[2080/4186], Train loss :0.228, Train acc: 0.906
[2022-10-13 15:58:18] - INFO: Epoch: 2, Batch[2090/4186], Train loss :0.250, Train acc: 0.922
[2022-10-13 15:58:21] - INFO: Epoch: 2, Batch[2100/4186], Train loss :0.406, Train acc: 0.859
[2022-10-13 15:58:23] - INFO: Epoch: 2, Batch[2110/4186], Train loss :0.167, Train acc: 0.953
[2022-10-13 15:58:25] - INFO: Epoch: 2, Batch[2120/4186], Train loss :0.188, Train acc: 0.938
[2022-10-13 15:58:28] - INFO: Epoch: 2, Batch[2130/4186], Train loss :0.214, Train acc: 0.922
[2022-10-13 15:58:30] - INFO: Epoch: 2, Batch[2140/4186], Train loss :0.202, Train acc: 0.938
[2022-10-13 15:58:33] - INFO: Epoch: 2, Batch[2150/4186], Train loss :0.251, Train acc: 0.891
[2022-10-13 15:58:35] - INFO: Epoch: 2, Batch[2160/4186], Train loss :0.329, Train acc: 0.891
[2022-10-13 15:58:37] - INFO: Epoch: 2, Batch[2170/4186], Train loss :0.320, Train acc: 0.859
[2022-10-13 15:58:39] - INFO: Epoch: 2, Batch[2180/4186], Train loss :0.313, Train acc: 0.875
[2022-10-13 15:58:42] - INFO: Epoch: 2, Batch[2190/4186], Train loss :0.248, Train acc: 0.953
[2022-10-13 15:58:44] - INFO: Epoch: 2, Batch[2200/4186], Train loss :0.098, Train acc: 0.969
[2022-10-13 15:58:47] - INFO: Epoch: 2, Batch[2210/4186], Train loss :0.189, Train acc: 0.922
[2022-10-13 15:58:49] - INFO: Epoch: 2, Batch[2220/4186], Train loss :0.378, Train acc: 0.875
[2022-10-13 15:58:52] - INFO: Epoch: 2, Batch[2230/4186], Train loss :0.390, Train acc: 0.938
[2022-10-13 15:58:54] - INFO: Epoch: 2, Batch[2240/4186], Train loss :0.198, Train acc: 0.938
[2022-10-13 15:58:56] - INFO: Epoch: 2, Batch[2250/4186], Train loss :0.452, Train acc: 0.891
[2022-10-13 15:58:59] - INFO: Epoch: 2, Batch[2260/4186], Train loss :0.141, Train acc: 0.953
[2022-10-13 15:59:01] - INFO: Epoch: 2, Batch[2270/4186], Train loss :0.300, Train acc: 0.922
[2022-10-13 15:59:03] - INFO: Epoch: 2, Batch[2280/4186], Train loss :0.446, Train acc: 0.891
[2022-10-13 15:59:05] - INFO: Epoch: 2, Batch[2290/4186], Train loss :0.347, Train acc: 0.875
[2022-10-13 15:59:08] - INFO: Epoch: 2, Batch[2300/4186], Train loss :0.295, Train acc: 0.922
[2022-10-13 15:59:10] - INFO: Epoch: 2, Batch[2310/4186], Train loss :0.249, Train acc: 0.906
[2022-10-13 15:59:13] - INFO: Epoch: 2, Batch[2320/4186], Train loss :0.330, Train acc: 0.906
[2022-10-13 15:59:15] - INFO: Epoch: 2, Batch[2330/4186], Train loss :0.257, Train acc: 0.891
[2022-10-13 15:59:17] - INFO: Epoch: 2, Batch[2340/4186], Train loss :0.480, Train acc: 0.844
[2022-10-13 15:59:19] - INFO: Epoch: 2, Batch[2350/4186], Train loss :0.158, Train acc: 0.953
[2022-10-13 15:59:22] - INFO: Epoch: 2, Batch[2360/4186], Train loss :0.296, Train acc: 0.938
[2022-10-13 15:59:24] - INFO: Epoch: 2, Batch[2370/4186], Train loss :0.222, Train acc: 0.922
[2022-10-13 15:59:27] - INFO: Epoch: 2, Batch[2380/4186], Train loss :0.258, Train acc: 0.922
[2022-10-13 15:59:29] - INFO: Epoch: 2, Batch[2390/4186], Train loss :0.447, Train acc: 0.859
[2022-10-13 15:59:31] - INFO: Epoch: 2, Batch[2400/4186], Train loss :0.368, Train acc: 0.875
[2022-10-13 15:59:34] - INFO: Epoch: 2, Batch[2410/4186], Train loss :0.393, Train acc: 0.906
[2022-10-13 15:59:36] - INFO: Epoch: 2, Batch[2420/4186], Train loss :0.356, Train acc: 0.922
[2022-10-13 15:59:38] - INFO: Epoch: 2, Batch[2430/4186], Train loss :0.402, Train acc: 0.844
[2022-10-13 15:59:41] - INFO: Epoch: 2, Batch[2440/4186], Train loss :0.314, Train acc: 0.859
[2022-10-13 15:59:43] - INFO: Epoch: 2, Batch[2450/4186], Train loss :0.343, Train acc: 0.906
[2022-10-13 15:59:45] - INFO: Epoch: 2, Batch[2460/4186], Train loss :0.454, Train acc: 0.875
[2022-10-13 15:59:48] - INFO: Epoch: 2, Batch[2470/4186], Train loss :0.640, Train acc: 0.828
[2022-10-13 15:59:50] - INFO: Epoch: 2, Batch[2480/4186], Train loss :0.152, Train acc: 0.984
[2022-10-13 15:59:52] - INFO: Epoch: 2, Batch[2490/4186], Train loss :0.248, Train acc: 0.891
[2022-10-13 15:59:55] - INFO: Epoch: 2, Batch[2500/4186], Train loss :0.254, Train acc: 0.938
[2022-10-13 15:59:57] - INFO: Epoch: 2, Batch[2510/4186], Train loss :0.377, Train acc: 0.891
[2022-10-13 16:00:00] - INFO: Epoch: 2, Batch[2520/4186], Train loss :0.212, Train acc: 0.938
[2022-10-13 16:00:02] - INFO: Epoch: 2, Batch[2530/4186], Train loss :0.207, Train acc: 0.922
[2022-10-13 16:00:04] - INFO: Epoch: 2, Batch[2540/4186], Train loss :0.539, Train acc: 0.844
[2022-10-13 16:00:07] - INFO: Epoch: 2, Batch[2550/4186], Train loss :0.173, Train acc: 0.953
[2022-10-13 16:00:09] - INFO: Epoch: 2, Batch[2560/4186], Train loss :0.231, Train acc: 0.953
[2022-10-13 16:00:11] - INFO: Epoch: 2, Batch[2570/4186], Train loss :0.387, Train acc: 0.891
[2022-10-13 16:00:14] - INFO: Epoch: 2, Batch[2580/4186], Train loss :0.191, Train acc: 0.969
[2022-10-13 16:00:16] - INFO: Epoch: 2, Batch[2590/4186], Train loss :0.290, Train acc: 0.922
[2022-10-13 16:00:18] - INFO: Epoch: 2, Batch[2600/4186], Train loss :0.205, Train acc: 0.953
[2022-10-13 16:00:21] - INFO: Epoch: 2, Batch[2610/4186], Train loss :0.176, Train acc: 0.953
[2022-10-13 16:00:23] - INFO: Epoch: 2, Batch[2620/4186], Train loss :0.259, Train acc: 0.938
[2022-10-13 16:00:25] - INFO: Epoch: 2, Batch[2630/4186], Train loss :0.069, Train acc: 1.000
[2022-10-13 16:00:28] - INFO: Epoch: 2, Batch[2640/4186], Train loss :0.115, Train acc: 0.969
[2022-10-13 16:00:30] - INFO: Epoch: 2, Batch[2650/4186], Train loss :0.279, Train acc: 0.891
[2022-10-13 16:00:33] - INFO: Epoch: 2, Batch[2660/4186], Train loss :0.385, Train acc: 0.859
[2022-10-13 16:00:35] - INFO: Epoch: 2, Batch[2670/4186], Train loss :0.190, Train acc: 0.922
[2022-10-13 16:00:38] - INFO: Epoch: 2, Batch[2680/4186], Train loss :0.159, Train acc: 0.953
[2022-10-13 16:00:40] - INFO: Epoch: 2, Batch[2690/4186], Train loss :0.219, Train acc: 0.953
[2022-10-13 16:00:42] - INFO: Epoch: 2, Batch[2700/4186], Train loss :0.315, Train acc: 0.891
[2022-10-13 16:00:45] - INFO: Epoch: 2, Batch[2710/4186], Train loss :0.172, Train acc: 0.953
[2022-10-13 16:00:47] - INFO: Epoch: 2, Batch[2720/4186], Train loss :0.115, Train acc: 0.969
[2022-10-13 16:00:50] - INFO: Epoch: 2, Batch[2730/4186], Train loss :0.230, Train acc: 0.906
[2022-10-13 16:00:52] - INFO: Epoch: 2, Batch[2740/4186], Train loss :0.223, Train acc: 0.906
[2022-10-13 16:00:54] - INFO: Epoch: 2, Batch[2750/4186], Train loss :0.219, Train acc: 0.938
[2022-10-13 16:00:57] - INFO: Epoch: 2, Batch[2760/4186], Train loss :0.265, Train acc: 0.922
[2022-10-13 16:00:59] - INFO: Epoch: 2, Batch[2770/4186], Train loss :0.146, Train acc: 0.969
[2022-10-13 16:01:01] - INFO: Epoch: 2, Batch[2780/4186], Train loss :0.318, Train acc: 0.891
[2022-10-13 16:01:04] - INFO: Epoch: 2, Batch[2790/4186], Train loss :0.148, Train acc: 0.984
[2022-10-13 16:01:23] - INFO: ÊàêÂäüÂØºÂÖ•BERTÈÖçÁΩÆÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/config.json
[2022-10-13 16:01:23] - INFO:  ### Â∞ÜÂΩìÂâçÈÖçÁΩÆÊâìÂç∞Âà∞Êó•ÂøóÊñá‰ª∂‰∏≠ 
[2022-10-13 16:01:23] - INFO: ###  project_dir = /home3/Tpandeng/mxy/BertWithPretrained
[2022-10-13 16:01:23] - INFO: ###  dataset_dir = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification
[2022-10-13 16:01:23] - INFO: ###  pretrained_model_dir = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese
[2022-10-13 16:01:23] - INFO: ###  vocab_path = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/vocab.txt
[2022-10-13 16:01:23] - INFO: ###  device = cuda:0
[2022-10-13 16:01:23] - INFO: ###  train_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_train.txt
[2022-10-13 16:01:23] - INFO: ###  val_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_val.txt
[2022-10-13 16:01:23] - INFO: ###  test_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_test.txt
[2022-10-13 16:01:23] - INFO: ###  model_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/cache
[2022-10-13 16:01:23] - INFO: ###  logs_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/logs
[2022-10-13 16:01:23] - INFO: ###  split_sep = _!_
[2022-10-13 16:01:23] - INFO: ###  is_sample_shuffle = True
[2022-10-13 16:01:23] - INFO: ###  batch_size = 64
[2022-10-13 16:01:23] - INFO: ###  max_sen_len = None
[2022-10-13 16:01:23] - INFO: ###  num_labels = 15
[2022-10-13 16:01:23] - INFO: ###  epochs = 1
[2022-10-13 16:01:23] - INFO: ###  model_val_per_epoch = 2
[2022-10-13 16:01:23] - INFO: ###  vocab_size = 21128
[2022-10-13 16:01:23] - INFO: ###  hidden_size = 768
[2022-10-13 16:01:23] - INFO: ###  num_hidden_layers = 12
[2022-10-13 16:01:23] - INFO: ###  num_attention_heads = 12
[2022-10-13 16:01:23] - INFO: ###  hidden_act = gelu
[2022-10-13 16:01:23] - INFO: ###  intermediate_size = 3072
[2022-10-13 16:01:23] - INFO: ###  pad_token_id = 0
[2022-10-13 16:01:23] - INFO: ###  hidden_dropout_prob = 0.1
[2022-10-13 16:01:23] - INFO: ###  attention_probs_dropout_prob = 0.1
[2022-10-13 16:01:23] - INFO: ###  max_position_embeddings = 512
[2022-10-13 16:01:23] - INFO: ###  type_vocab_size = 2
[2022-10-13 16:01:23] - INFO: ###  initializer_range = 0.02
[2022-10-13 16:01:23] - INFO: ###  directionality = bidi
[2022-10-13 16:01:23] - INFO: ###  pooler_fc_size = 768
[2022-10-13 16:01:23] - INFO: ###  pooler_num_attention_heads = 12
[2022-10-13 16:01:23] - INFO: ###  pooler_num_fc_layers = 3
[2022-10-13 16:01:23] - INFO: ###  pooler_size_per_head = 128
[2022-10-13 16:01:23] - INFO: ###  pooler_type = first_token_transform
[2022-10-13 16:01:25] - INFO: ## Ê≥®ÊÑèÔºåÊ≠£Âú®‰ΩøÁî®Êú¨Âú∞MyTransformer‰∏≠ÁöÑMyMultiHeadAttentionÂÆûÁé∞
[2022-10-13 16:01:27] - INFO: ## ÊàêÂäüËΩΩÂÖ•Â∑≤ÊúâÊ®°ÂûãÔºåËøõË°åËøΩÂä†ËÆ≠ÁªÉ......
[2022-10-13 16:01:27] - INFO: ÁºìÂ≠òÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_test_None.pt Â≠òÂú®ÔºåÁõ¥Êé•ËΩΩÂÖ•ÁºìÂ≠òÊñá‰ª∂ÔºÅ
[2022-10-13 16:01:30] - INFO: ÁºìÂ≠òÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_train_None.pt Â≠òÂú®ÔºåÁõ¥Êé•ËΩΩÂÖ•ÁºìÂ≠òÊñá‰ª∂ÔºÅ
[2022-10-13 16:01:51] - INFO: ÁºìÂ≠òÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_val_None.pt Â≠òÂú®ÔºåÁõ¥Êé•ËΩΩÂÖ•ÁºìÂ≠òÊñá‰ª∂ÔºÅ
[2022-10-13 16:01:58] - INFO: Epoch: 0, Batch[0/4186], Train loss :0.314, Train acc: 0.938
[2022-10-13 16:02:00] - INFO: Epoch: 0, Batch[10/4186], Train loss :0.266, Train acc: 0.906
[2022-10-13 16:02:02] - INFO: Epoch: 0, Batch[20/4186], Train loss :0.273, Train acc: 0.953
[2022-10-13 16:02:05] - INFO: Epoch: 0, Batch[30/4186], Train loss :0.323, Train acc: 0.938
[2022-10-13 16:02:07] - INFO: Epoch: 0, Batch[40/4186], Train loss :0.200, Train acc: 0.922
[2022-10-13 16:02:09] - INFO: Epoch: 0, Batch[50/4186], Train loss :0.361, Train acc: 0.891
[2022-10-13 16:02:12] - INFO: Epoch: 0, Batch[60/4186], Train loss :0.326, Train acc: 0.891
[2022-10-13 16:02:14] - INFO: Epoch: 0, Batch[70/4186], Train loss :0.282, Train acc: 0.891
[2022-10-13 16:02:16] - INFO: Epoch: 0, Batch[80/4186], Train loss :0.397, Train acc: 0.875
[2022-10-13 16:02:18] - INFO: Epoch: 0, Batch[90/4186], Train loss :0.366, Train acc: 0.906
[2022-10-13 16:02:21] - INFO: Epoch: 0, Batch[100/4186], Train loss :0.132, Train acc: 0.953
[2022-10-13 16:02:23] - INFO: Epoch: 0, Batch[110/4186], Train loss :0.441, Train acc: 0.875
[2022-10-13 16:02:25] - INFO: Epoch: 0, Batch[120/4186], Train loss :0.209, Train acc: 0.953
[2022-10-13 16:02:28] - INFO: Epoch: 0, Batch[130/4186], Train loss :0.198, Train acc: 0.938
[2022-10-13 16:02:30] - INFO: Epoch: 0, Batch[140/4186], Train loss :0.227, Train acc: 0.922
[2022-10-13 16:02:32] - INFO: Epoch: 0, Batch[150/4186], Train loss :0.504, Train acc: 0.859
[2022-10-13 16:02:35] - INFO: Epoch: 0, Batch[160/4186], Train loss :0.281, Train acc: 0.922
[2022-10-13 16:02:37] - INFO: Epoch: 0, Batch[170/4186], Train loss :0.202, Train acc: 0.906
[2022-10-13 16:02:39] - INFO: Epoch: 0, Batch[180/4186], Train loss :0.124, Train acc: 0.953
[2022-10-13 16:02:41] - INFO: Epoch: 0, Batch[190/4186], Train loss :0.296, Train acc: 0.906
[2022-10-13 16:02:44] - INFO: Epoch: 0, Batch[200/4186], Train loss :0.511, Train acc: 0.859
[2022-10-13 16:02:46] - INFO: Epoch: 0, Batch[210/4186], Train loss :0.520, Train acc: 0.891
[2022-10-13 16:02:49] - INFO: Epoch: 0, Batch[220/4186], Train loss :0.124, Train acc: 0.953
[2022-10-13 16:02:51] - INFO: Epoch: 0, Batch[230/4186], Train loss :0.342, Train acc: 0.906
[2022-10-13 16:02:54] - INFO: Epoch: 0, Batch[240/4186], Train loss :0.151, Train acc: 0.969
[2022-10-13 16:02:56] - INFO: Epoch: 0, Batch[250/4186], Train loss :0.254, Train acc: 0.922
[2022-10-13 16:02:58] - INFO: Epoch: 0, Batch[260/4186], Train loss :0.310, Train acc: 0.938
[2022-10-13 16:03:01] - INFO: Epoch: 0, Batch[270/4186], Train loss :0.340, Train acc: 0.906
[2022-10-13 16:03:03] - INFO: Epoch: 0, Batch[280/4186], Train loss :0.240, Train acc: 0.906
[2022-10-13 16:03:05] - INFO: Epoch: 0, Batch[290/4186], Train loss :0.177, Train acc: 0.953
[2022-10-13 16:03:08] - INFO: Epoch: 0, Batch[300/4186], Train loss :0.306, Train acc: 0.906
[2022-10-13 16:03:10] - INFO: Epoch: 0, Batch[310/4186], Train loss :0.369, Train acc: 0.938
[2022-10-13 16:03:12] - INFO: Epoch: 0, Batch[320/4186], Train loss :0.227, Train acc: 0.938
[2022-10-13 16:03:14] - INFO: Epoch: 0, Batch[330/4186], Train loss :0.354, Train acc: 0.875
[2022-10-13 16:03:17] - INFO: Epoch: 0, Batch[340/4186], Train loss :0.084, Train acc: 0.953
[2022-10-13 16:03:19] - INFO: Epoch: 0, Batch[350/4186], Train loss :0.098, Train acc: 0.969
[2022-10-13 16:03:22] - INFO: Epoch: 0, Batch[360/4186], Train loss :0.443, Train acc: 0.891
[2022-10-13 16:03:24] - INFO: Epoch: 0, Batch[370/4186], Train loss :0.411, Train acc: 0.906
[2022-10-13 16:03:26] - INFO: Epoch: 0, Batch[380/4186], Train loss :0.141, Train acc: 0.953
[2022-10-13 16:03:29] - INFO: Epoch: 0, Batch[390/4186], Train loss :0.238, Train acc: 0.906
[2022-10-13 16:03:31] - INFO: Epoch: 0, Batch[400/4186], Train loss :0.274, Train acc: 0.922
[2022-10-13 16:03:33] - INFO: Epoch: 0, Batch[410/4186], Train loss :0.444, Train acc: 0.891
[2022-10-13 16:03:35] - INFO: Epoch: 0, Batch[420/4186], Train loss :0.397, Train acc: 0.891
[2022-10-13 16:03:38] - INFO: Epoch: 0, Batch[430/4186], Train loss :0.254, Train acc: 0.938
[2022-10-13 16:03:40] - INFO: Epoch: 0, Batch[440/4186], Train loss :0.329, Train acc: 0.906
[2022-10-13 16:03:42] - INFO: Epoch: 0, Batch[450/4186], Train loss :0.199, Train acc: 0.922
[2022-10-13 16:03:45] - INFO: Epoch: 0, Batch[460/4186], Train loss :0.263, Train acc: 0.922
[2022-10-13 16:03:47] - INFO: Epoch: 0, Batch[470/4186], Train loss :0.210, Train acc: 0.906
[2022-10-13 16:03:49] - INFO: Epoch: 0, Batch[480/4186], Train loss :0.201, Train acc: 0.938
[2022-10-13 16:03:52] - INFO: Epoch: 0, Batch[490/4186], Train loss :0.210, Train acc: 0.953
[2022-10-13 16:03:54] - INFO: Epoch: 0, Batch[500/4186], Train loss :0.240, Train acc: 0.891
[2022-10-13 16:03:57] - INFO: Epoch: 0, Batch[510/4186], Train loss :0.267, Train acc: 0.953
[2022-10-13 16:03:59] - INFO: Epoch: 0, Batch[520/4186], Train loss :0.164, Train acc: 0.922
[2022-10-13 16:04:01] - INFO: Epoch: 0, Batch[530/4186], Train loss :0.358, Train acc: 0.875
[2022-10-13 16:04:04] - INFO: Epoch: 0, Batch[540/4186], Train loss :0.094, Train acc: 0.969
[2022-10-13 16:04:06] - INFO: Epoch: 0, Batch[550/4186], Train loss :0.072, Train acc: 0.984
[2022-10-13 16:04:08] - INFO: Epoch: 0, Batch[560/4186], Train loss :0.145, Train acc: 0.953
[2022-10-13 16:04:10] - INFO: Epoch: 0, Batch[570/4186], Train loss :0.307, Train acc: 0.875
[2022-10-13 16:04:13] - INFO: Epoch: 0, Batch[580/4186], Train loss :0.505, Train acc: 0.828
[2022-10-13 16:04:15] - INFO: Epoch: 0, Batch[590/4186], Train loss :0.158, Train acc: 0.953
[2022-10-13 16:04:17] - INFO: Epoch: 0, Batch[600/4186], Train loss :0.091, Train acc: 0.953
[2022-10-13 16:04:20] - INFO: Epoch: 0, Batch[610/4186], Train loss :0.166, Train acc: 0.938
[2022-10-13 16:04:22] - INFO: Epoch: 0, Batch[620/4186], Train loss :0.303, Train acc: 0.906
[2022-10-13 16:04:24] - INFO: Epoch: 0, Batch[630/4186], Train loss :0.280, Train acc: 0.922
[2022-10-13 16:04:27] - INFO: Epoch: 0, Batch[640/4186], Train loss :0.406, Train acc: 0.859
[2022-10-13 16:04:29] - INFO: Epoch: 0, Batch[650/4186], Train loss :0.356, Train acc: 0.891
[2022-10-13 16:04:31] - INFO: Epoch: 0, Batch[660/4186], Train loss :0.219, Train acc: 0.953
[2022-10-13 16:04:34] - INFO: Epoch: 0, Batch[670/4186], Train loss :0.458, Train acc: 0.859
[2022-10-13 16:04:36] - INFO: Epoch: 0, Batch[680/4186], Train loss :0.302, Train acc: 0.938
[2022-10-13 16:04:38] - INFO: Epoch: 0, Batch[690/4186], Train loss :0.467, Train acc: 0.891
[2022-10-13 16:04:41] - INFO: Epoch: 0, Batch[700/4186], Train loss :0.245, Train acc: 0.906
[2022-10-13 16:04:43] - INFO: Epoch: 0, Batch[710/4186], Train loss :0.305, Train acc: 0.875
[2022-10-13 16:04:45] - INFO: Epoch: 0, Batch[720/4186], Train loss :0.316, Train acc: 0.891
[2022-10-13 16:04:47] - INFO: Epoch: 0, Batch[730/4186], Train loss :0.343, Train acc: 0.891
[2022-10-13 16:04:50] - INFO: Epoch: 0, Batch[740/4186], Train loss :0.509, Train acc: 0.797
[2022-10-13 16:04:52] - INFO: Epoch: 0, Batch[750/4186], Train loss :0.096, Train acc: 0.969
[2022-10-13 16:04:54] - INFO: Epoch: 0, Batch[760/4186], Train loss :0.151, Train acc: 0.938
[2022-10-13 16:04:57] - INFO: Epoch: 0, Batch[770/4186], Train loss :0.165, Train acc: 0.969
[2022-10-13 16:04:59] - INFO: Epoch: 0, Batch[780/4186], Train loss :0.283, Train acc: 0.922
[2022-10-13 16:05:01] - INFO: Epoch: 0, Batch[790/4186], Train loss :0.230, Train acc: 0.953
[2022-10-13 16:05:03] - INFO: Epoch: 0, Batch[800/4186], Train loss :0.184, Train acc: 0.922
[2022-10-13 16:05:06] - INFO: Epoch: 0, Batch[810/4186], Train loss :0.254, Train acc: 0.906
[2022-10-13 16:05:08] - INFO: Epoch: 0, Batch[820/4186], Train loss :0.227, Train acc: 0.906
[2022-10-13 16:05:11] - INFO: Epoch: 0, Batch[830/4186], Train loss :0.271, Train acc: 0.891
[2022-10-13 16:05:13] - INFO: Epoch: 0, Batch[840/4186], Train loss :0.172, Train acc: 0.953
[2022-10-13 16:05:16] - INFO: Epoch: 0, Batch[850/4186], Train loss :0.217, Train acc: 0.922
[2022-10-13 16:05:18] - INFO: Epoch: 0, Batch[860/4186], Train loss :0.081, Train acc: 1.000
[2022-10-13 16:05:21] - INFO: Epoch: 0, Batch[870/4186], Train loss :0.300, Train acc: 0.922
[2022-10-13 16:05:23] - INFO: Epoch: 0, Batch[880/4186], Train loss :0.230, Train acc: 0.938
[2022-10-13 16:05:25] - INFO: Epoch: 0, Batch[890/4186], Train loss :0.329, Train acc: 0.906
[2022-10-13 16:05:28] - INFO: Epoch: 0, Batch[900/4186], Train loss :0.223, Train acc: 0.938
[2022-10-13 16:05:30] - INFO: Epoch: 0, Batch[910/4186], Train loss :0.341, Train acc: 0.922
[2022-10-13 16:05:32] - INFO: Epoch: 0, Batch[920/4186], Train loss :0.194, Train acc: 0.953
[2022-10-13 16:05:35] - INFO: Epoch: 0, Batch[930/4186], Train loss :0.238, Train acc: 0.922
[2022-10-13 16:05:37] - INFO: Epoch: 0, Batch[940/4186], Train loss :0.483, Train acc: 0.875
[2022-10-13 16:05:39] - INFO: Epoch: 0, Batch[950/4186], Train loss :0.302, Train acc: 0.906
[2022-10-13 16:05:42] - INFO: Epoch: 0, Batch[960/4186], Train loss :0.105, Train acc: 0.984
[2022-10-13 16:05:44] - INFO: Epoch: 0, Batch[970/4186], Train loss :0.258, Train acc: 0.922
[2022-10-13 16:05:46] - INFO: Epoch: 0, Batch[980/4186], Train loss :0.134, Train acc: 0.938
[2022-10-13 16:05:48] - INFO: Epoch: 0, Batch[990/4186], Train loss :0.254, Train acc: 0.891
[2022-10-13 16:05:51] - INFO: Epoch: 0, Batch[1000/4186], Train loss :0.258, Train acc: 0.953
[2022-10-13 16:05:53] - INFO: Epoch: 0, Batch[1010/4186], Train loss :0.613, Train acc: 0.828
[2022-10-13 16:05:55] - INFO: Epoch: 0, Batch[1020/4186], Train loss :0.126, Train acc: 0.953
[2022-10-13 16:05:58] - INFO: Epoch: 0, Batch[1030/4186], Train loss :0.424, Train acc: 0.891
[2022-10-13 16:06:00] - INFO: Epoch: 0, Batch[1040/4186], Train loss :0.222, Train acc: 0.953
[2022-10-13 16:06:02] - INFO: Epoch: 0, Batch[1050/4186], Train loss :0.289, Train acc: 0.953
[2022-10-13 16:06:05] - INFO: Epoch: 0, Batch[1060/4186], Train loss :0.328, Train acc: 0.859
[2022-10-13 16:06:07] - INFO: Epoch: 0, Batch[1070/4186], Train loss :0.260, Train acc: 0.891
[2022-10-13 16:06:09] - INFO: Epoch: 0, Batch[1080/4186], Train loss :0.204, Train acc: 0.938
[2022-10-13 16:06:11] - INFO: Epoch: 0, Batch[1090/4186], Train loss :0.277, Train acc: 0.891
[2022-10-13 16:06:14] - INFO: Epoch: 0, Batch[1100/4186], Train loss :0.318, Train acc: 0.938
[2022-10-13 16:06:16] - INFO: Epoch: 0, Batch[1110/4186], Train loss :0.230, Train acc: 0.922
[2022-10-13 16:06:18] - INFO: Epoch: 0, Batch[1120/4186], Train loss :0.169, Train acc: 0.906
[2022-10-13 16:06:21] - INFO: Epoch: 0, Batch[1130/4186], Train loss :0.211, Train acc: 0.984
[2022-10-13 16:06:23] - INFO: Epoch: 0, Batch[1140/4186], Train loss :0.261, Train acc: 0.922
[2022-10-13 16:06:26] - INFO: Epoch: 0, Batch[1150/4186], Train loss :0.349, Train acc: 0.891
[2022-10-13 16:06:28] - INFO: Epoch: 0, Batch[1160/4186], Train loss :0.176, Train acc: 0.938
[2022-10-13 16:06:30] - INFO: Epoch: 0, Batch[1170/4186], Train loss :0.280, Train acc: 0.891
[2022-10-13 16:06:33] - INFO: Epoch: 0, Batch[1180/4186], Train loss :0.426, Train acc: 0.844
[2022-10-13 16:06:35] - INFO: Epoch: 0, Batch[1190/4186], Train loss :0.428, Train acc: 0.844
[2022-10-13 16:06:37] - INFO: Epoch: 0, Batch[1200/4186], Train loss :0.328, Train acc: 0.891
[2022-10-13 16:06:40] - INFO: Epoch: 0, Batch[1210/4186], Train loss :0.147, Train acc: 0.969
[2022-10-13 16:06:42] - INFO: Epoch: 0, Batch[1220/4186], Train loss :0.074, Train acc: 0.969
[2022-10-13 16:06:44] - INFO: Epoch: 0, Batch[1230/4186], Train loss :0.300, Train acc: 0.891
[2022-10-13 16:06:47] - INFO: Epoch: 0, Batch[1240/4186], Train loss :0.238, Train acc: 0.938
[2022-10-13 16:06:49] - INFO: Epoch: 0, Batch[1250/4186], Train loss :0.467, Train acc: 0.859
[2022-10-13 16:06:51] - INFO: Epoch: 0, Batch[1260/4186], Train loss :0.349, Train acc: 0.906
[2022-10-13 16:06:54] - INFO: Epoch: 0, Batch[1270/4186], Train loss :0.200, Train acc: 0.969
[2022-10-13 16:06:56] - INFO: Epoch: 0, Batch[1280/4186], Train loss :0.328, Train acc: 0.922
[2022-10-13 16:06:58] - INFO: Epoch: 0, Batch[1290/4186], Train loss :0.169, Train acc: 0.969
[2022-10-13 16:07:00] - INFO: Epoch: 0, Batch[1300/4186], Train loss :0.232, Train acc: 0.953
[2022-10-13 16:07:03] - INFO: Epoch: 0, Batch[1310/4186], Train loss :0.178, Train acc: 0.953
[2022-10-13 16:07:05] - INFO: Epoch: 0, Batch[1320/4186], Train loss :0.123, Train acc: 0.953
[2022-10-13 16:07:07] - INFO: Epoch: 0, Batch[1330/4186], Train loss :0.089, Train acc: 0.969
[2022-10-13 16:07:09] - INFO: Epoch: 0, Batch[1340/4186], Train loss :0.306, Train acc: 0.906
[2022-10-13 16:07:12] - INFO: Epoch: 0, Batch[1350/4186], Train loss :0.306, Train acc: 0.891
[2022-10-13 16:07:14] - INFO: Epoch: 0, Batch[1360/4186], Train loss :0.521, Train acc: 0.875
[2022-10-13 16:07:16] - INFO: Epoch: 0, Batch[1370/4186], Train loss :0.596, Train acc: 0.844
[2022-10-13 16:07:19] - INFO: Epoch: 0, Batch[1380/4186], Train loss :0.197, Train acc: 0.953
[2022-10-13 16:07:21] - INFO: Epoch: 0, Batch[1390/4186], Train loss :0.261, Train acc: 0.906
[2022-10-13 16:07:23] - INFO: Epoch: 0, Batch[1400/4186], Train loss :0.127, Train acc: 0.969
[2022-10-13 16:07:25] - INFO: Epoch: 0, Batch[1410/4186], Train loss :0.372, Train acc: 0.844
[2022-10-13 16:07:28] - INFO: Epoch: 0, Batch[1420/4186], Train loss :0.451, Train acc: 0.875
[2022-10-13 16:07:30] - INFO: Epoch: 0, Batch[1430/4186], Train loss :0.484, Train acc: 0.859
[2022-10-13 16:07:32] - INFO: Epoch: 0, Batch[1440/4186], Train loss :0.202, Train acc: 0.938
[2022-10-13 16:07:35] - INFO: Epoch: 0, Batch[1450/4186], Train loss :0.276, Train acc: 0.891
[2022-10-13 16:07:37] - INFO: Epoch: 0, Batch[1460/4186], Train loss :0.400, Train acc: 0.875
[2022-10-13 16:07:39] - INFO: Epoch: 0, Batch[1470/4186], Train loss :0.296, Train acc: 0.938
[2022-10-13 16:07:42] - INFO: Epoch: 0, Batch[1480/4186], Train loss :0.307, Train acc: 0.891
[2022-10-13 16:07:44] - INFO: Epoch: 0, Batch[1490/4186], Train loss :0.252, Train acc: 0.938
[2022-10-13 16:07:46] - INFO: Epoch: 0, Batch[1500/4186], Train loss :0.137, Train acc: 0.938
[2022-10-13 16:07:49] - INFO: Epoch: 0, Batch[1510/4186], Train loss :0.392, Train acc: 0.875
[2022-10-13 16:07:51] - INFO: Epoch: 0, Batch[1520/4186], Train loss :0.172, Train acc: 0.969
[2022-10-13 16:07:53] - INFO: Epoch: 0, Batch[1530/4186], Train loss :0.167, Train acc: 0.906
[2022-10-13 16:07:55] - INFO: Epoch: 0, Batch[1540/4186], Train loss :0.243, Train acc: 0.953
[2022-10-13 16:07:58] - INFO: Epoch: 0, Batch[1550/4186], Train loss :0.297, Train acc: 0.906
[2022-10-13 16:08:00] - INFO: Epoch: 0, Batch[1560/4186], Train loss :0.204, Train acc: 0.953
[2022-10-13 16:08:02] - INFO: Epoch: 0, Batch[1570/4186], Train loss :0.383, Train acc: 0.906
[2022-10-13 16:08:05] - INFO: Epoch: 0, Batch[1580/4186], Train loss :0.248, Train acc: 0.922
[2022-10-13 16:08:07] - INFO: Epoch: 0, Batch[1590/4186], Train loss :0.283, Train acc: 0.906
[2022-10-13 16:08:09] - INFO: Epoch: 0, Batch[1600/4186], Train loss :0.347, Train acc: 0.922
[2022-10-13 16:08:12] - INFO: Epoch: 0, Batch[1610/4186], Train loss :0.277, Train acc: 0.906
[2022-10-13 16:08:14] - INFO: Epoch: 0, Batch[1620/4186], Train loss :0.161, Train acc: 0.938
[2022-10-13 16:08:16] - INFO: Epoch: 0, Batch[1630/4186], Train loss :0.176, Train acc: 0.953
[2022-10-13 16:08:19] - INFO: Epoch: 0, Batch[1640/4186], Train loss :0.185, Train acc: 0.938
[2022-10-13 16:08:21] - INFO: Epoch: 0, Batch[1650/4186], Train loss :0.256, Train acc: 0.953
[2022-10-13 16:08:23] - INFO: Epoch: 0, Batch[1660/4186], Train loss :0.160, Train acc: 0.969
[2022-10-13 16:08:26] - INFO: Epoch: 0, Batch[1670/4186], Train loss :0.323, Train acc: 0.875
[2022-10-13 16:08:28] - INFO: Epoch: 0, Batch[1680/4186], Train loss :0.198, Train acc: 0.953
[2022-10-13 16:08:30] - INFO: Epoch: 0, Batch[1690/4186], Train loss :0.133, Train acc: 0.938
[2022-10-13 16:08:33] - INFO: Epoch: 0, Batch[1700/4186], Train loss :0.561, Train acc: 0.859
[2022-10-13 16:08:35] - INFO: Epoch: 0, Batch[1710/4186], Train loss :0.192, Train acc: 0.953
[2022-10-13 16:08:37] - INFO: Epoch: 0, Batch[1720/4186], Train loss :0.558, Train acc: 0.875
[2022-10-13 16:08:40] - INFO: Epoch: 0, Batch[1730/4186], Train loss :0.242, Train acc: 0.906
[2022-10-13 16:08:42] - INFO: Epoch: 0, Batch[1740/4186], Train loss :0.262, Train acc: 0.938
[2022-10-13 16:08:44] - INFO: Epoch: 0, Batch[1750/4186], Train loss :0.325, Train acc: 0.906
[2022-10-13 16:08:47] - INFO: Epoch: 0, Batch[1760/4186], Train loss :0.402, Train acc: 0.891
[2022-10-13 16:08:49] - INFO: Epoch: 0, Batch[1770/4186], Train loss :0.325, Train acc: 0.875
[2022-10-13 16:08:51] - INFO: Epoch: 0, Batch[1780/4186], Train loss :0.199, Train acc: 0.953
[2022-10-13 16:08:54] - INFO: Epoch: 0, Batch[1790/4186], Train loss :0.190, Train acc: 0.938
[2022-10-13 16:08:56] - INFO: Epoch: 0, Batch[1800/4186], Train loss :0.171, Train acc: 0.938
[2022-10-13 16:08:58] - INFO: Epoch: 0, Batch[1810/4186], Train loss :0.273, Train acc: 0.922
[2022-10-13 16:09:01] - INFO: Epoch: 0, Batch[1820/4186], Train loss :0.225, Train acc: 0.906
[2022-10-13 16:09:03] - INFO: Epoch: 0, Batch[1830/4186], Train loss :0.345, Train acc: 0.891
[2022-10-13 16:09:05] - INFO: Epoch: 0, Batch[1840/4186], Train loss :0.309, Train acc: 0.891
[2022-10-13 16:09:07] - INFO: Epoch: 0, Batch[1850/4186], Train loss :0.273, Train acc: 0.906
[2022-10-13 16:09:10] - INFO: Epoch: 0, Batch[1860/4186], Train loss :0.160, Train acc: 0.938
[2022-10-13 16:09:12] - INFO: Epoch: 0, Batch[1870/4186], Train loss :0.382, Train acc: 0.875
[2022-10-13 16:09:14] - INFO: Epoch: 0, Batch[1880/4186], Train loss :0.394, Train acc: 0.875
[2022-10-13 16:09:17] - INFO: Epoch: 0, Batch[1890/4186], Train loss :0.197, Train acc: 0.938
[2022-10-13 16:09:19] - INFO: Epoch: 0, Batch[1900/4186], Train loss :0.313, Train acc: 0.891
[2022-10-13 16:09:21] - INFO: Epoch: 0, Batch[1910/4186], Train loss :0.324, Train acc: 0.906
[2022-10-13 16:09:23] - INFO: Epoch: 0, Batch[1920/4186], Train loss :0.430, Train acc: 0.844
[2022-10-13 16:09:26] - INFO: Epoch: 0, Batch[1930/4186], Train loss :0.181, Train acc: 0.969
[2022-10-13 16:09:28] - INFO: Epoch: 0, Batch[1940/4186], Train loss :0.312, Train acc: 0.891
[2022-10-13 16:09:30] - INFO: Epoch: 0, Batch[1950/4186], Train loss :0.302, Train acc: 0.875
[2022-10-13 16:09:33] - INFO: Epoch: 0, Batch[1960/4186], Train loss :0.433, Train acc: 0.875
[2022-10-13 16:09:35] - INFO: Epoch: 0, Batch[1970/4186], Train loss :0.297, Train acc: 0.938
[2022-10-13 16:09:37] - INFO: Epoch: 0, Batch[1980/4186], Train loss :0.246, Train acc: 0.906
[2022-10-13 16:09:39] - INFO: Epoch: 0, Batch[1990/4186], Train loss :0.319, Train acc: 0.922
[2022-10-13 16:09:42] - INFO: Epoch: 0, Batch[2000/4186], Train loss :0.173, Train acc: 0.938
[2022-10-13 16:09:44] - INFO: Epoch: 0, Batch[2010/4186], Train loss :0.209, Train acc: 0.922
[2022-10-13 16:09:46] - INFO: Epoch: 0, Batch[2020/4186], Train loss :0.178, Train acc: 0.938
[2022-10-13 16:09:49] - INFO: Epoch: 0, Batch[2030/4186], Train loss :0.247, Train acc: 0.891
[2022-10-13 16:09:51] - INFO: Epoch: 0, Batch[2040/4186], Train loss :0.226, Train acc: 0.922
[2022-10-13 16:09:53] - INFO: Epoch: 0, Batch[2050/4186], Train loss :0.442, Train acc: 0.891
[2022-10-13 16:09:55] - INFO: Epoch: 0, Batch[2060/4186], Train loss :0.343, Train acc: 0.859
[2022-10-13 16:09:58] - INFO: Epoch: 0, Batch[2070/4186], Train loss :0.131, Train acc: 0.938
[2022-10-13 16:10:00] - INFO: Epoch: 0, Batch[2080/4186], Train loss :0.238, Train acc: 0.938
[2022-10-13 16:10:02] - INFO: Epoch: 0, Batch[2090/4186], Train loss :0.262, Train acc: 0.906
[2022-10-13 16:10:05] - INFO: Epoch: 0, Batch[2100/4186], Train loss :0.049, Train acc: 1.000
[2022-10-13 16:10:07] - INFO: Epoch: 0, Batch[2110/4186], Train loss :0.502, Train acc: 0.875
[2022-10-13 16:10:09] - INFO: Epoch: 0, Batch[2120/4186], Train loss :0.174, Train acc: 0.938
[2022-10-13 16:10:12] - INFO: Epoch: 0, Batch[2130/4186], Train loss :0.371, Train acc: 0.891
[2022-10-13 16:10:14] - INFO: Epoch: 0, Batch[2140/4186], Train loss :0.302, Train acc: 0.938
[2022-10-13 16:10:16] - INFO: Epoch: 0, Batch[2150/4186], Train loss :0.315, Train acc: 0.891
[2022-10-13 16:10:19] - INFO: Epoch: 0, Batch[2160/4186], Train loss :0.269, Train acc: 0.906
[2022-10-13 16:10:21] - INFO: Epoch: 0, Batch[2170/4186], Train loss :0.244, Train acc: 0.906
[2022-10-13 16:10:24] - INFO: Epoch: 0, Batch[2180/4186], Train loss :0.305, Train acc: 0.891
[2022-10-13 16:10:26] - INFO: Epoch: 0, Batch[2190/4186], Train loss :0.236, Train acc: 0.906
[2022-10-13 16:10:28] - INFO: Epoch: 0, Batch[2200/4186], Train loss :0.099, Train acc: 0.953
[2022-10-13 16:10:31] - INFO: Epoch: 0, Batch[2210/4186], Train loss :0.368, Train acc: 0.922
[2022-10-13 16:10:33] - INFO: Epoch: 0, Batch[2220/4186], Train loss :0.387, Train acc: 0.922
[2022-10-13 16:10:35] - INFO: Epoch: 0, Batch[2230/4186], Train loss :0.244, Train acc: 0.938
[2022-10-13 16:10:38] - INFO: Epoch: 0, Batch[2240/4186], Train loss :0.227, Train acc: 0.922
[2022-10-13 16:10:40] - INFO: Epoch: 0, Batch[2250/4186], Train loss :0.118, Train acc: 0.969
[2022-10-13 16:10:42] - INFO: Epoch: 0, Batch[2260/4186], Train loss :0.153, Train acc: 0.938
[2022-10-13 16:10:44] - INFO: Epoch: 0, Batch[2270/4186], Train loss :0.153, Train acc: 0.984
[2022-10-13 16:10:47] - INFO: Epoch: 0, Batch[2280/4186], Train loss :0.394, Train acc: 0.906
[2022-10-13 16:10:49] - INFO: Epoch: 0, Batch[2290/4186], Train loss :0.196, Train acc: 0.938
[2022-10-13 16:10:51] - INFO: Epoch: 0, Batch[2300/4186], Train loss :0.176, Train acc: 0.938
[2022-10-13 16:10:54] - INFO: Epoch: 0, Batch[2310/4186], Train loss :0.249, Train acc: 0.922
[2022-10-13 16:10:56] - INFO: Epoch: 0, Batch[2320/4186], Train loss :0.216, Train acc: 0.922
[2022-10-13 16:10:59] - INFO: Epoch: 0, Batch[2330/4186], Train loss :0.390, Train acc: 0.844
[2022-10-13 16:11:01] - INFO: Epoch: 0, Batch[2340/4186], Train loss :0.181, Train acc: 0.953
[2022-10-13 16:11:03] - INFO: Epoch: 0, Batch[2350/4186], Train loss :0.194, Train acc: 0.922
[2022-10-13 16:11:05] - INFO: Epoch: 0, Batch[2360/4186], Train loss :0.379, Train acc: 0.906
[2022-10-13 16:11:08] - INFO: Epoch: 0, Batch[2370/4186], Train loss :0.189, Train acc: 0.922
[2022-10-13 16:11:10] - INFO: Epoch: 0, Batch[2380/4186], Train loss :0.152, Train acc: 0.969
[2022-10-13 16:11:12] - INFO: Epoch: 0, Batch[2390/4186], Train loss :0.171, Train acc: 0.938
[2022-10-13 16:11:15] - INFO: Epoch: 0, Batch[2400/4186], Train loss :0.179, Train acc: 0.953
[2022-10-13 16:11:17] - INFO: Epoch: 0, Batch[2410/4186], Train loss :0.363, Train acc: 0.906
[2022-10-13 16:11:19] - INFO: Epoch: 0, Batch[2420/4186], Train loss :0.381, Train acc: 0.922
[2022-10-13 16:11:22] - INFO: Epoch: 0, Batch[2430/4186], Train loss :0.254, Train acc: 0.969
[2022-10-13 16:11:24] - INFO: Epoch: 0, Batch[2440/4186], Train loss :0.250, Train acc: 0.953
[2022-10-13 16:11:26] - INFO: Epoch: 0, Batch[2450/4186], Train loss :0.301, Train acc: 0.906
[2022-10-13 16:11:28] - INFO: Epoch: 0, Batch[2460/4186], Train loss :0.281, Train acc: 0.953
[2022-10-13 16:11:31] - INFO: Epoch: 0, Batch[2470/4186], Train loss :0.280, Train acc: 0.922
[2022-10-13 16:11:33] - INFO: Epoch: 0, Batch[2480/4186], Train loss :0.327, Train acc: 0.906
[2022-10-13 16:11:35] - INFO: Epoch: 0, Batch[2490/4186], Train loss :0.158, Train acc: 0.938
[2022-10-13 16:11:38] - INFO: Epoch: 0, Batch[2500/4186], Train loss :0.514, Train acc: 0.875
[2022-10-13 16:11:40] - INFO: Epoch: 0, Batch[2510/4186], Train loss :0.113, Train acc: 0.969
[2022-10-13 16:11:42] - INFO: Epoch: 0, Batch[2520/4186], Train loss :0.690, Train acc: 0.797
[2022-10-13 16:11:45] - INFO: Epoch: 0, Batch[2530/4186], Train loss :0.374, Train acc: 0.922
[2022-10-13 16:11:47] - INFO: Epoch: 0, Batch[2540/4186], Train loss :0.190, Train acc: 0.938
[2022-10-13 16:11:49] - INFO: Epoch: 0, Batch[2550/4186], Train loss :0.436, Train acc: 0.859
[2022-10-13 16:11:52] - INFO: Epoch: 0, Batch[2560/4186], Train loss :0.386, Train acc: 0.891
[2022-10-13 16:11:54] - INFO: Epoch: 0, Batch[2570/4186], Train loss :0.431, Train acc: 0.891
[2022-10-13 16:11:56] - INFO: Epoch: 0, Batch[2580/4186], Train loss :0.451, Train acc: 0.828
[2022-10-13 16:11:59] - INFO: Epoch: 0, Batch[2590/4186], Train loss :0.231, Train acc: 0.922
[2022-10-13 16:12:01] - INFO: Epoch: 0, Batch[2600/4186], Train loss :0.367, Train acc: 0.906
[2022-10-13 16:12:03] - INFO: Epoch: 0, Batch[2610/4186], Train loss :0.453, Train acc: 0.859
[2022-10-13 16:12:06] - INFO: Epoch: 0, Batch[2620/4186], Train loss :0.211, Train acc: 0.938
[2022-10-13 16:12:08] - INFO: Epoch: 0, Batch[2630/4186], Train loss :0.358, Train acc: 0.875
[2022-10-13 16:12:10] - INFO: Epoch: 0, Batch[2640/4186], Train loss :0.316, Train acc: 0.922
[2022-10-13 16:12:13] - INFO: Epoch: 0, Batch[2650/4186], Train loss :0.306, Train acc: 0.922
[2022-10-13 16:12:15] - INFO: Epoch: 0, Batch[2660/4186], Train loss :0.311, Train acc: 0.891
[2022-10-13 16:12:17] - INFO: Epoch: 0, Batch[2670/4186], Train loss :0.165, Train acc: 0.953
[2022-10-13 16:12:20] - INFO: Epoch: 0, Batch[2680/4186], Train loss :0.527, Train acc: 0.875
[2022-10-13 16:12:23] - INFO: Epoch: 0, Batch[2690/4186], Train loss :0.259, Train acc: 0.922
[2022-10-13 16:12:25] - INFO: Epoch: 0, Batch[2700/4186], Train loss :0.229, Train acc: 0.906
[2022-10-13 16:12:27] - INFO: Epoch: 0, Batch[2710/4186], Train loss :0.332, Train acc: 0.891
[2022-10-13 16:12:30] - INFO: Epoch: 0, Batch[2720/4186], Train loss :0.153, Train acc: 0.953
[2022-10-13 16:12:32] - INFO: Epoch: 0, Batch[2730/4186], Train loss :0.167, Train acc: 0.953
[2022-10-13 16:12:34] - INFO: Epoch: 0, Batch[2740/4186], Train loss :0.296, Train acc: 0.906
[2022-10-13 16:12:37] - INFO: Epoch: 0, Batch[2750/4186], Train loss :0.178, Train acc: 0.922
[2022-10-13 16:12:39] - INFO: Epoch: 0, Batch[2760/4186], Train loss :0.446, Train acc: 0.859
[2022-10-13 16:12:41] - INFO: Epoch: 0, Batch[2770/4186], Train loss :0.273, Train acc: 0.938
[2022-10-13 16:12:43] - INFO: Epoch: 0, Batch[2780/4186], Train loss :0.363, Train acc: 0.859
[2022-10-13 16:12:46] - INFO: Epoch: 0, Batch[2790/4186], Train loss :0.260, Train acc: 0.938
[2022-10-13 16:12:48] - INFO: Epoch: 0, Batch[2800/4186], Train loss :0.320, Train acc: 0.891
[2022-10-13 16:12:50] - INFO: Epoch: 0, Batch[2810/4186], Train loss :0.366, Train acc: 0.922
[2022-10-13 16:12:52] - INFO: Epoch: 0, Batch[2820/4186], Train loss :0.369, Train acc: 0.891
[2022-10-13 16:12:55] - INFO: Epoch: 0, Batch[2830/4186], Train loss :0.331, Train acc: 0.906
[2022-10-13 16:12:57] - INFO: Epoch: 0, Batch[2840/4186], Train loss :0.076, Train acc: 0.984
[2022-10-13 16:12:59] - INFO: Epoch: 0, Batch[2850/4186], Train loss :0.448, Train acc: 0.875
[2022-10-13 16:13:01] - INFO: Epoch: 0, Batch[2860/4186], Train loss :0.184, Train acc: 0.922
[2022-10-13 16:13:04] - INFO: Epoch: 0, Batch[2870/4186], Train loss :0.148, Train acc: 0.938
[2022-10-13 16:13:06] - INFO: Epoch: 0, Batch[2880/4186], Train loss :0.185, Train acc: 0.953
[2022-10-13 16:13:08] - INFO: Epoch: 0, Batch[2890/4186], Train loss :0.283, Train acc: 0.891
[2022-10-13 16:13:11] - INFO: Epoch: 0, Batch[2900/4186], Train loss :0.303, Train acc: 0.922
[2022-10-13 16:13:13] - INFO: Epoch: 0, Batch[2910/4186], Train loss :0.230, Train acc: 0.953
[2022-10-13 16:13:15] - INFO: Epoch: 0, Batch[2920/4186], Train loss :0.312, Train acc: 0.922
[2022-10-13 16:13:17] - INFO: Epoch: 0, Batch[2930/4186], Train loss :0.416, Train acc: 0.859
[2022-10-13 16:13:20] - INFO: Epoch: 0, Batch[2940/4186], Train loss :0.275, Train acc: 0.953
[2022-10-13 16:13:22] - INFO: Epoch: 0, Batch[2950/4186], Train loss :0.191, Train acc: 0.938
[2022-10-13 16:13:24] - INFO: Epoch: 0, Batch[2960/4186], Train loss :0.395, Train acc: 0.906
[2022-10-13 16:13:27] - INFO: Epoch: 0, Batch[2970/4186], Train loss :0.202, Train acc: 0.938
[2022-10-13 16:13:29] - INFO: Epoch: 0, Batch[2980/4186], Train loss :0.347, Train acc: 0.891
[2022-10-13 16:13:31] - INFO: Epoch: 0, Batch[2990/4186], Train loss :0.204, Train acc: 0.922
[2022-10-13 16:13:33] - INFO: Epoch: 0, Batch[3000/4186], Train loss :0.126, Train acc: 0.969
[2022-10-13 16:13:36] - INFO: Epoch: 0, Batch[3010/4186], Train loss :0.378, Train acc: 0.875
[2022-10-13 16:13:38] - INFO: Epoch: 0, Batch[3020/4186], Train loss :0.372, Train acc: 0.891
[2022-10-13 16:13:40] - INFO: Epoch: 0, Batch[3030/4186], Train loss :0.168, Train acc: 0.969
[2022-10-13 16:13:43] - INFO: Epoch: 0, Batch[3040/4186], Train loss :0.258, Train acc: 0.953
[2022-10-13 16:13:45] - INFO: Epoch: 0, Batch[3050/4186], Train loss :0.180, Train acc: 0.938
[2022-10-13 16:13:47] - INFO: Epoch: 0, Batch[3060/4186], Train loss :0.281, Train acc: 0.938
[2022-10-13 16:13:50] - INFO: Epoch: 0, Batch[3070/4186], Train loss :0.503, Train acc: 0.859
[2022-10-13 16:13:52] - INFO: Epoch: 0, Batch[3080/4186], Train loss :0.199, Train acc: 0.922
[2022-10-13 16:13:55] - INFO: Epoch: 0, Batch[3090/4186], Train loss :0.326, Train acc: 0.938
[2022-10-13 16:13:57] - INFO: Epoch: 0, Batch[3100/4186], Train loss :0.144, Train acc: 0.969
[2022-10-13 16:13:59] - INFO: Epoch: 0, Batch[3110/4186], Train loss :0.219, Train acc: 0.953
[2022-10-13 16:14:01] - INFO: Epoch: 0, Batch[3120/4186], Train loss :0.477, Train acc: 0.828
[2022-10-13 16:14:04] - INFO: Epoch: 0, Batch[3130/4186], Train loss :0.213, Train acc: 0.938
[2022-10-13 16:14:06] - INFO: Epoch: 0, Batch[3140/4186], Train loss :0.431, Train acc: 0.875
[2022-10-13 16:14:08] - INFO: Epoch: 0, Batch[3150/4186], Train loss :0.288, Train acc: 0.938
[2022-10-13 16:14:11] - INFO: Epoch: 0, Batch[3160/4186], Train loss :0.249, Train acc: 0.938
[2022-10-13 16:14:13] - INFO: Epoch: 0, Batch[3170/4186], Train loss :0.145, Train acc: 0.922
[2022-10-13 16:14:15] - INFO: Epoch: 0, Batch[3180/4186], Train loss :0.315, Train acc: 0.891
[2022-10-13 16:14:17] - INFO: Epoch: 0, Batch[3190/4186], Train loss :0.340, Train acc: 0.875
[2022-10-13 16:14:20] - INFO: Epoch: 0, Batch[3200/4186], Train loss :0.309, Train acc: 0.891
[2022-10-13 16:14:22] - INFO: Epoch: 0, Batch[3210/4186], Train loss :0.303, Train acc: 0.859
[2022-10-13 16:14:24] - INFO: Epoch: 0, Batch[3220/4186], Train loss :0.289, Train acc: 0.938
[2022-10-13 16:14:27] - INFO: Epoch: 0, Batch[3230/4186], Train loss :0.509, Train acc: 0.875
[2022-10-13 16:14:29] - INFO: Epoch: 0, Batch[3240/4186], Train loss :0.182, Train acc: 0.938
[2022-10-13 16:14:31] - INFO: Epoch: 0, Batch[3250/4186], Train loss :0.322, Train acc: 0.891
[2022-10-13 16:14:34] - INFO: Epoch: 0, Batch[3260/4186], Train loss :0.419, Train acc: 0.922
[2022-10-13 16:14:36] - INFO: Epoch: 0, Batch[3270/4186], Train loss :0.219, Train acc: 0.938
[2022-10-13 16:14:38] - INFO: Epoch: 0, Batch[3280/4186], Train loss :0.219, Train acc: 0.922
[2022-10-13 16:14:41] - INFO: Epoch: 0, Batch[3290/4186], Train loss :0.462, Train acc: 0.828
[2022-10-13 16:14:43] - INFO: Epoch: 0, Batch[3300/4186], Train loss :0.398, Train acc: 0.922
[2022-10-13 16:14:45] - INFO: Epoch: 0, Batch[3310/4186], Train loss :0.198, Train acc: 0.953
[2022-10-13 16:14:48] - INFO: Epoch: 0, Batch[3320/4186], Train loss :0.216, Train acc: 0.922
[2022-10-13 16:14:50] - INFO: Epoch: 0, Batch[3330/4186], Train loss :0.285, Train acc: 0.922
[2022-10-13 16:14:52] - INFO: Epoch: 0, Batch[3340/4186], Train loss :0.272, Train acc: 0.875
[2022-10-13 16:14:55] - INFO: Epoch: 0, Batch[3350/4186], Train loss :0.561, Train acc: 0.844
[2022-10-13 16:14:57] - INFO: Epoch: 0, Batch[3360/4186], Train loss :0.375, Train acc: 0.875
[2022-10-13 16:14:59] - INFO: Epoch: 0, Batch[3370/4186], Train loss :0.353, Train acc: 0.906
[2022-10-13 16:15:02] - INFO: Epoch: 0, Batch[3380/4186], Train loss :0.293, Train acc: 0.875
[2022-10-13 16:15:04] - INFO: Epoch: 0, Batch[3390/4186], Train loss :0.160, Train acc: 0.938
[2022-10-13 16:15:06] - INFO: Epoch: 0, Batch[3400/4186], Train loss :0.263, Train acc: 0.891
[2022-10-13 16:15:08] - INFO: Epoch: 0, Batch[3410/4186], Train loss :0.180, Train acc: 0.953
[2022-10-13 16:15:11] - INFO: Epoch: 0, Batch[3420/4186], Train loss :0.260, Train acc: 0.906
[2022-10-13 16:15:13] - INFO: Epoch: 0, Batch[3430/4186], Train loss :0.199, Train acc: 0.938
[2022-10-13 16:15:15] - INFO: Epoch: 0, Batch[3440/4186], Train loss :0.301, Train acc: 0.938
[2022-10-13 16:15:18] - INFO: Epoch: 0, Batch[3450/4186], Train loss :0.085, Train acc: 0.984
[2022-10-13 16:15:20] - INFO: Epoch: 0, Batch[3460/4186], Train loss :0.382, Train acc: 0.906
[2022-10-13 16:15:22] - INFO: Epoch: 0, Batch[3470/4186], Train loss :0.284, Train acc: 0.906
[2022-10-13 16:15:25] - INFO: Epoch: 0, Batch[3480/4186], Train loss :0.412, Train acc: 0.906
[2022-10-13 16:15:27] - INFO: Epoch: 0, Batch[3490/4186], Train loss :0.267, Train acc: 0.906
[2022-10-13 16:15:29] - INFO: Epoch: 0, Batch[3500/4186], Train loss :0.341, Train acc: 0.875
[2022-10-13 16:15:31] - INFO: Epoch: 0, Batch[3510/4186], Train loss :0.137, Train acc: 0.984
[2022-10-13 16:15:34] - INFO: Epoch: 0, Batch[3520/4186], Train loss :0.219, Train acc: 0.953
[2022-10-13 16:15:36] - INFO: Epoch: 0, Batch[3530/4186], Train loss :0.313, Train acc: 0.891
[2022-10-13 16:15:38] - INFO: Epoch: 0, Batch[3540/4186], Train loss :0.399, Train acc: 0.875
[2022-10-13 16:15:41] - INFO: Epoch: 0, Batch[3550/4186], Train loss :0.246, Train acc: 0.938
[2022-10-13 16:15:43] - INFO: Epoch: 0, Batch[3560/4186], Train loss :0.200, Train acc: 0.922
[2022-10-13 16:15:45] - INFO: Epoch: 0, Batch[3570/4186], Train loss :0.194, Train acc: 0.938
[2022-10-13 16:15:47] - INFO: Epoch: 0, Batch[3580/4186], Train loss :0.574, Train acc: 0.797
[2022-10-13 16:15:50] - INFO: Epoch: 0, Batch[3590/4186], Train loss :0.186, Train acc: 0.922
[2022-10-13 16:15:52] - INFO: Epoch: 0, Batch[3600/4186], Train loss :0.320, Train acc: 0.875
[2022-10-13 16:15:54] - INFO: Epoch: 0, Batch[3610/4186], Train loss :0.281, Train acc: 0.906
[2022-10-13 16:15:57] - INFO: Epoch: 0, Batch[3620/4186], Train loss :0.271, Train acc: 0.922
[2022-10-13 16:15:59] - INFO: Epoch: 0, Batch[3630/4186], Train loss :0.271, Train acc: 0.906
[2022-10-13 16:16:01] - INFO: Epoch: 0, Batch[3640/4186], Train loss :0.327, Train acc: 0.922
[2022-10-13 16:16:03] - INFO: Epoch: 0, Batch[3650/4186], Train loss :0.342, Train acc: 0.875
[2022-10-13 16:16:06] - INFO: Epoch: 0, Batch[3660/4186], Train loss :0.335, Train acc: 0.906
[2022-10-13 16:16:08] - INFO: Epoch: 0, Batch[3670/4186], Train loss :0.248, Train acc: 0.906
[2022-10-13 16:16:10] - INFO: Epoch: 0, Batch[3680/4186], Train loss :0.216, Train acc: 0.953
[2022-10-13 16:16:13] - INFO: Epoch: 0, Batch[3690/4186], Train loss :0.284, Train acc: 0.938
[2022-10-13 16:16:15] - INFO: Epoch: 0, Batch[3700/4186], Train loss :0.326, Train acc: 0.891
[2022-10-13 16:16:17] - INFO: Epoch: 0, Batch[3710/4186], Train loss :0.191, Train acc: 0.938
[2022-10-13 16:16:20] - INFO: Epoch: 0, Batch[3720/4186], Train loss :0.341, Train acc: 0.844
[2022-10-13 16:16:22] - INFO: Epoch: 0, Batch[3730/4186], Train loss :0.303, Train acc: 0.875
[2022-10-13 16:16:24] - INFO: Epoch: 0, Batch[3740/4186], Train loss :0.320, Train acc: 0.922
[2022-10-13 16:16:27] - INFO: Epoch: 0, Batch[3750/4186], Train loss :0.255, Train acc: 0.891
[2022-10-13 16:16:29] - INFO: Epoch: 0, Batch[3760/4186], Train loss :0.133, Train acc: 0.953
[2022-10-13 16:16:31] - INFO: Epoch: 0, Batch[3770/4186], Train loss :0.254, Train acc: 0.906
[2022-10-13 16:16:34] - INFO: Epoch: 0, Batch[3780/4186], Train loss :0.344, Train acc: 0.875
[2022-10-13 16:16:36] - INFO: Epoch: 0, Batch[3790/4186], Train loss :0.220, Train acc: 0.938
[2022-10-13 16:16:38] - INFO: Epoch: 0, Batch[3800/4186], Train loss :0.113, Train acc: 0.984
[2022-10-13 16:16:41] - INFO: Epoch: 0, Batch[3810/4186], Train loss :0.327, Train acc: 0.906
[2022-10-13 16:16:43] - INFO: Epoch: 0, Batch[3820/4186], Train loss :0.334, Train acc: 0.922
[2022-10-13 16:16:45] - INFO: Epoch: 0, Batch[3830/4186], Train loss :0.262, Train acc: 0.906
[2022-10-13 16:16:47] - INFO: Epoch: 0, Batch[3840/4186], Train loss :0.147, Train acc: 0.953
[2022-10-13 16:16:50] - INFO: Epoch: 0, Batch[3850/4186], Train loss :0.220, Train acc: 0.953
[2022-10-13 16:16:52] - INFO: Epoch: 0, Batch[3860/4186], Train loss :0.276, Train acc: 0.906
[2022-10-13 16:16:54] - INFO: Epoch: 0, Batch[3870/4186], Train loss :0.191, Train acc: 0.953
[2022-10-13 16:16:57] - INFO: Epoch: 0, Batch[3880/4186], Train loss :0.242, Train acc: 0.922
[2022-10-13 16:16:59] - INFO: Epoch: 0, Batch[3890/4186], Train loss :0.112, Train acc: 0.969
[2022-10-13 16:17:02] - INFO: Epoch: 0, Batch[3900/4186], Train loss :0.333, Train acc: 0.938
[2022-10-13 16:17:04] - INFO: Epoch: 0, Batch[3910/4186], Train loss :0.205, Train acc: 0.938
[2022-10-13 16:17:06] - INFO: Epoch: 0, Batch[3920/4186], Train loss :0.467, Train acc: 0.875
[2022-10-13 16:17:08] - INFO: Epoch: 0, Batch[3930/4186], Train loss :0.458, Train acc: 0.859
[2022-10-13 16:17:11] - INFO: Epoch: 0, Batch[3940/4186], Train loss :0.258, Train acc: 0.922
[2022-10-13 16:17:13] - INFO: Epoch: 0, Batch[3950/4186], Train loss :0.227, Train acc: 0.938
[2022-10-13 16:17:15] - INFO: Epoch: 0, Batch[3960/4186], Train loss :0.354, Train acc: 0.938
[2022-10-13 16:17:18] - INFO: Epoch: 0, Batch[3970/4186], Train loss :0.623, Train acc: 0.828
[2022-10-13 16:17:20] - INFO: Epoch: 0, Batch[3980/4186], Train loss :0.253, Train acc: 0.891
[2022-10-13 16:17:22] - INFO: Epoch: 0, Batch[3990/4186], Train loss :0.255, Train acc: 0.922
[2022-10-13 16:17:25] - INFO: Epoch: 0, Batch[4000/4186], Train loss :0.420, Train acc: 0.844
[2022-10-13 16:17:27] - INFO: Epoch: 0, Batch[4010/4186], Train loss :0.276, Train acc: 0.906
[2022-10-13 16:17:30] - INFO: Epoch: 0, Batch[4020/4186], Train loss :0.502, Train acc: 0.891
[2022-10-13 16:17:32] - INFO: Epoch: 0, Batch[4030/4186], Train loss :0.189, Train acc: 0.922
[2022-10-13 16:17:34] - INFO: Epoch: 0, Batch[4040/4186], Train loss :0.387, Train acc: 0.891
[2022-10-13 16:17:37] - INFO: Epoch: 0, Batch[4050/4186], Train loss :0.397, Train acc: 0.875
[2022-10-13 16:17:39] - INFO: Epoch: 0, Batch[4060/4186], Train loss :0.187, Train acc: 0.938
[2022-10-13 16:17:41] - INFO: Epoch: 0, Batch[4070/4186], Train loss :0.392, Train acc: 0.859
[2022-10-13 16:17:44] - INFO: Epoch: 0, Batch[4080/4186], Train loss :0.168, Train acc: 0.938
[2022-10-13 16:17:46] - INFO: Epoch: 0, Batch[4090/4186], Train loss :0.451, Train acc: 0.891
[2022-10-13 16:17:48] - INFO: Epoch: 0, Batch[4100/4186], Train loss :0.248, Train acc: 0.938
[2022-10-13 16:17:50] - INFO: Epoch: 0, Batch[4110/4186], Train loss :0.380, Train acc: 0.891
[2022-10-13 16:17:53] - INFO: Epoch: 0, Batch[4120/4186], Train loss :0.577, Train acc: 0.844
[2022-10-13 16:17:55] - INFO: Epoch: 0, Batch[4130/4186], Train loss :0.352, Train acc: 0.922
[2022-10-13 16:17:57] - INFO: Epoch: 0, Batch[4140/4186], Train loss :0.495, Train acc: 0.891
[2022-10-13 16:18:00] - INFO: Epoch: 0, Batch[4150/4186], Train loss :0.357, Train acc: 0.906
[2022-10-13 16:18:02] - INFO: Epoch: 0, Batch[4160/4186], Train loss :0.481, Train acc: 0.844
[2022-10-13 16:18:04] - INFO: Epoch: 0, Batch[4170/4186], Train loss :0.347, Train acc: 0.891
[2022-10-13 16:18:07] - INFO: Epoch: 0, Batch[4180/4186], Train loss :0.199, Train acc: 0.969
[2022-10-13 16:18:08] - INFO: Epoch: 0, Train loss: 0.280, Epoch time = 970.498s
[2022-10-13 16:18:11] - INFO: ## Ê≥®ÊÑèÔºåÊ≠£Âú®‰ΩøÁî®Êú¨Âú∞MyTransformer‰∏≠ÁöÑMyMultiHeadAttentionÂÆûÁé∞
[2022-10-13 16:18:11] - INFO: ## ÊàêÂäüËΩΩÂÖ•Â∑≤ÊúâÊ®°ÂûãÔºåËøõË°åÈ¢ÑÊµã......
[2022-10-13 16:18:11] - INFO: ÁºìÂ≠òÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_test_None.pt Â≠òÂú®ÔºåÁõ¥Êé•ËΩΩÂÖ•ÁºìÂ≠òÊñá‰ª∂ÔºÅ
[2022-10-13 16:18:14] - INFO: ÁºìÂ≠òÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_train_None.pt Â≠òÂú®ÔºåÁõ¥Êé•ËΩΩÂÖ•ÁºìÂ≠òÊñá‰ª∂ÔºÅ
[2022-10-13 16:18:35] - INFO: ÁºìÂ≠òÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_val_None.pt Â≠òÂú®ÔºåÁõ¥Êé•ËΩΩÂÖ•ÁºìÂ≠òÊñá‰ª∂ÔºÅ
[2022-10-13 16:19:24] - INFO: Acc on test:0.890
[2022-10-13 16:53:46] - INFO: ÊàêÂäüÂØºÂÖ•BERTÈÖçÁΩÆÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/config.json
[2022-10-13 16:53:46] - INFO:  ### Â∞ÜÂΩìÂâçÈÖçÁΩÆÊâìÂç∞Âà∞Êó•ÂøóÊñá‰ª∂‰∏≠ 
[2022-10-13 16:53:46] - INFO: ###  project_dir = /home3/Tpandeng/mxy/BertWithPretrained
[2022-10-13 16:53:46] - INFO: ###  dataset_dir = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification
[2022-10-13 16:53:46] - INFO: ###  pretrained_model_dir = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese
[2022-10-13 16:53:46] - INFO: ###  vocab_path = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/vocab.txt
[2022-10-13 16:53:46] - INFO: ###  device = cuda:0
[2022-10-13 16:53:46] - INFO: ###  train_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_train.txt
[2022-10-13 16:53:46] - INFO: ###  val_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_val.txt
[2022-10-13 16:53:46] - INFO: ###  test_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_test.txt
[2022-10-13 16:53:46] - INFO: ###  model_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/cache
[2022-10-13 16:53:46] - INFO: ###  logs_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/logs
[2022-10-13 16:53:46] - INFO: ###  split_sep = _!_
[2022-10-13 16:53:46] - INFO: ###  is_sample_shuffle = True
[2022-10-13 16:53:46] - INFO: ###  batch_size = 64
[2022-10-13 16:53:46] - INFO: ###  max_sen_len = None
[2022-10-13 16:53:46] - INFO: ###  num_labels = 15
[2022-10-13 16:53:46] - INFO: ###  epochs = 1
[2022-10-13 16:53:46] - INFO: ###  model_val_per_epoch = 2
[2022-10-13 16:53:46] - INFO: ###  vocab_size = 21128
[2022-10-13 16:53:46] - INFO: ###  hidden_size = 768
[2022-10-13 16:53:46] - INFO: ###  num_hidden_layers = 12
[2022-10-13 16:53:46] - INFO: ###  num_attention_heads = 12
[2022-10-13 16:53:46] - INFO: ###  hidden_act = gelu
[2022-10-13 16:53:46] - INFO: ###  intermediate_size = 3072
[2022-10-13 16:53:46] - INFO: ###  pad_token_id = 0
[2022-10-13 16:53:46] - INFO: ###  hidden_dropout_prob = 0.1
[2022-10-13 16:53:46] - INFO: ###  attention_probs_dropout_prob = 0.1
[2022-10-13 16:53:46] - INFO: ###  max_position_embeddings = 512
[2022-10-13 16:53:46] - INFO: ###  type_vocab_size = 2
[2022-10-13 16:53:46] - INFO: ###  initializer_range = 0.02
[2022-10-13 16:53:46] - INFO: ###  directionality = bidi
[2022-10-13 16:53:46] - INFO: ###  pooler_fc_size = 768
[2022-10-13 16:53:46] - INFO: ###  pooler_num_attention_heads = 12
[2022-10-13 16:53:46] - INFO: ###  pooler_num_fc_layers = 3
[2022-10-13 16:53:46] - INFO: ###  pooler_size_per_head = 128
[2022-10-13 16:53:46] - INFO: ###  pooler_type = first_token_transform
[2022-10-13 16:53:46] - INFO: ÁºìÂ≠òÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_test_None.pt Â≠òÂú®ÔºåÁõ¥Êé•ËΩΩÂÖ•ÁºìÂ≠òÊñá‰ª∂ÔºÅ
[2022-10-13 16:53:49] - INFO: ÁºìÂ≠òÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_train_None.pt Â≠òÂú®ÔºåÁõ¥Êé•ËΩΩÂÖ•ÁºìÂ≠òÊñá‰ª∂ÔºÅ
[2022-10-13 16:54:11] - INFO: ÁºìÂ≠òÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_val_None.pt Â≠òÂú®ÔºåÁõ¥Êé•ËΩΩÂÖ•ÁºìÂ≠òÊñá‰ª∂ÔºÅ
[2022-10-13 17:03:09] - INFO: ÊàêÂäüÂØºÂÖ•BERTÈÖçÁΩÆÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/config.json
[2022-10-13 17:03:09] - INFO:  ### Â∞ÜÂΩìÂâçÈÖçÁΩÆÊâìÂç∞Âà∞Êó•ÂøóÊñá‰ª∂‰∏≠ 
[2022-10-13 17:03:09] - INFO: ###  project_dir = /home3/Tpandeng/mxy/BertWithPretrained
[2022-10-13 17:03:09] - INFO: ###  dataset_dir = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification
[2022-10-13 17:03:09] - INFO: ###  pretrained_model_dir = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese
[2022-10-13 17:03:09] - INFO: ###  vocab_path = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/vocab.txt
[2022-10-13 17:03:09] - INFO: ###  device = cuda:0
[2022-10-13 17:03:09] - INFO: ###  train_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_train.txt
[2022-10-13 17:03:09] - INFO: ###  val_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_val.txt
[2022-10-13 17:03:09] - INFO: ###  test_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_test.txt
[2022-10-13 17:03:09] - INFO: ###  model_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/cache
[2022-10-13 17:03:09] - INFO: ###  logs_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/logs
[2022-10-13 17:03:09] - INFO: ###  split_sep = _!_
[2022-10-13 17:03:09] - INFO: ###  is_sample_shuffle = True
[2022-10-13 17:03:09] - INFO: ###  batch_size = 64
[2022-10-13 17:03:09] - INFO: ###  max_sen_len = None
[2022-10-13 17:03:09] - INFO: ###  num_labels = 15
[2022-10-13 17:03:09] - INFO: ###  epochs = 1
[2022-10-13 17:03:09] - INFO: ###  model_val_per_epoch = 2
[2022-10-13 17:03:09] - INFO: ###  vocab_size = 21128
[2022-10-13 17:03:09] - INFO: ###  hidden_size = 768
[2022-10-13 17:03:09] - INFO: ###  num_hidden_layers = 12
[2022-10-13 17:03:09] - INFO: ###  num_attention_heads = 12
[2022-10-13 17:03:09] - INFO: ###  hidden_act = gelu
[2022-10-13 17:03:09] - INFO: ###  intermediate_size = 3072
[2022-10-13 17:03:09] - INFO: ###  pad_token_id = 0
[2022-10-13 17:03:09] - INFO: ###  hidden_dropout_prob = 0.1
[2022-10-13 17:03:09] - INFO: ###  attention_probs_dropout_prob = 0.1
[2022-10-13 17:03:09] - INFO: ###  max_position_embeddings = 512
[2022-10-13 17:03:09] - INFO: ###  type_vocab_size = 2
[2022-10-13 17:03:09] - INFO: ###  initializer_range = 0.02
[2022-10-13 17:03:09] - INFO: ###  directionality = bidi
[2022-10-13 17:03:09] - INFO: ###  pooler_fc_size = 768
[2022-10-13 17:03:09] - INFO: ###  pooler_num_attention_heads = 12
[2022-10-13 17:03:09] - INFO: ###  pooler_num_fc_layers = 3
[2022-10-13 17:03:09] - INFO: ###  pooler_size_per_head = 128
[2022-10-13 17:03:09] - INFO: ###  pooler_type = first_token_transform
[2022-10-13 17:04:57] - INFO: ÊàêÂäüÂØºÂÖ•BERTÈÖçÁΩÆÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/config.json
[2022-10-13 17:04:57] - INFO:  ### Â∞ÜÂΩìÂâçÈÖçÁΩÆÊâìÂç∞Âà∞Êó•ÂøóÊñá‰ª∂‰∏≠ 
[2022-10-13 17:04:57] - INFO: ###  project_dir = /home3/Tpandeng/mxy/BertWithPretrained
[2022-10-13 17:04:57] - INFO: ###  dataset_dir = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification
[2022-10-13 17:04:57] - INFO: ###  pretrained_model_dir = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese
[2022-10-13 17:04:57] - INFO: ###  vocab_path = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/vocab.txt
[2022-10-13 17:04:57] - INFO: ###  device = cuda:0
[2022-10-13 17:04:57] - INFO: ###  train_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_train.txt
[2022-10-13 17:04:57] - INFO: ###  val_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_val.txt
[2022-10-13 17:04:57] - INFO: ###  test_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_test.txt
[2022-10-13 17:04:57] - INFO: ###  model_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/cache
[2022-10-13 17:04:57] - INFO: ###  logs_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/logs
[2022-10-13 17:04:57] - INFO: ###  split_sep = _!_
[2022-10-13 17:04:57] - INFO: ###  is_sample_shuffle = True
[2022-10-13 17:04:57] - INFO: ###  batch_size = 64
[2022-10-13 17:04:57] - INFO: ###  max_sen_len = None
[2022-10-13 17:04:57] - INFO: ###  num_labels = 15
[2022-10-13 17:04:57] - INFO: ###  epochs = 1
[2022-10-13 17:04:57] - INFO: ###  model_val_per_epoch = 2
[2022-10-13 17:04:57] - INFO: ###  vocab_size = 21128
[2022-10-13 17:04:57] - INFO: ###  hidden_size = 768
[2022-10-13 17:04:57] - INFO: ###  num_hidden_layers = 12
[2022-10-13 17:04:57] - INFO: ###  num_attention_heads = 12
[2022-10-13 17:04:57] - INFO: ###  hidden_act = gelu
[2022-10-13 17:04:57] - INFO: ###  intermediate_size = 3072
[2022-10-13 17:04:57] - INFO: ###  pad_token_id = 0
[2022-10-13 17:04:57] - INFO: ###  hidden_dropout_prob = 0.1
[2022-10-13 17:04:57] - INFO: ###  attention_probs_dropout_prob = 0.1
[2022-10-13 17:04:57] - INFO: ###  max_position_embeddings = 512
[2022-10-13 17:04:57] - INFO: ###  type_vocab_size = 2
[2022-10-13 17:04:57] - INFO: ###  initializer_range = 0.02
[2022-10-13 17:04:57] - INFO: ###  directionality = bidi
[2022-10-13 17:04:57] - INFO: ###  pooler_fc_size = 768
[2022-10-13 17:04:57] - INFO: ###  pooler_num_attention_heads = 12
[2022-10-13 17:04:57] - INFO: ###  pooler_num_fc_layers = 3
[2022-10-13 17:04:57] - INFO: ###  pooler_size_per_head = 128
[2022-10-13 17:04:57] - INFO: ###  pooler_type = first_token_transform
[2022-10-13 17:04:58] - INFO: ## Ê≥®ÊÑèÔºåÊ≠£Âú®‰ΩøÁî®Êú¨Âú∞MyTransformer‰∏≠ÁöÑMyMultiHeadAttentionÂÆûÁé∞
[2022-10-13 17:05:00] - INFO: ## ÊàêÂäüËΩΩÂÖ•Â∑≤ÊúâÊ®°ÂûãÔºåËøõË°åÈ¢ÑÊµã......
[2022-10-13 17:05:01] - INFO: ÁºìÂ≠òÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_test_None.pt Â≠òÂú®ÔºåÁõ¥Êé•ËΩΩÂÖ•ÁºìÂ≠òÊñá‰ª∂ÔºÅ
[2022-10-13 17:05:04] - INFO: ÁºìÂ≠òÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_train_None.pt Â≠òÂú®ÔºåÁõ¥Êé•ËΩΩÂÖ•ÁºìÂ≠òÊñá‰ª∂ÔºÅ
[2022-10-13 17:05:26] - INFO: ÁºìÂ≠òÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_val_None.pt Â≠òÂú®ÔºåÁõ¥Êé•ËΩΩÂÖ•ÁºìÂ≠òÊñá‰ª∂ÔºÅ
[2022-10-13 17:22:35] - INFO: ÊàêÂäüÂØºÂÖ•BERTÈÖçÁΩÆÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/config.json
[2022-10-13 17:22:35] - INFO:  ### Â∞ÜÂΩìÂâçÈÖçÁΩÆÊâìÂç∞Âà∞Êó•ÂøóÊñá‰ª∂‰∏≠ 
[2022-10-13 17:22:35] - INFO: ###  project_dir = /home3/Tpandeng/mxy/BertWithPretrained
[2022-10-13 17:22:35] - INFO: ###  dataset_dir = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification
[2022-10-13 17:22:35] - INFO: ###  pretrained_model_dir = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese
[2022-10-13 17:22:35] - INFO: ###  vocab_path = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/vocab.txt
[2022-10-13 17:22:35] - INFO: ###  device = cuda:0
[2022-10-13 17:22:35] - INFO: ###  train_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_train.txt
[2022-10-13 17:22:35] - INFO: ###  val_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_val.txt
[2022-10-13 17:22:35] - INFO: ###  test_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_test.txt
[2022-10-13 17:22:35] - INFO: ###  model_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/cache
[2022-10-13 17:22:35] - INFO: ###  logs_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/logs
[2022-10-13 17:22:35] - INFO: ###  split_sep = _!_
[2022-10-13 17:22:35] - INFO: ###  is_sample_shuffle = True
[2022-10-13 17:22:35] - INFO: ###  batch_size = 64
[2022-10-13 17:22:35] - INFO: ###  max_sen_len = None
[2022-10-13 17:22:35] - INFO: ###  num_labels = 15
[2022-10-13 17:22:35] - INFO: ###  epochs = 1
[2022-10-13 17:22:35] - INFO: ###  model_val_per_epoch = 2
[2022-10-13 17:22:35] - INFO: ###  vocab_size = 21128
[2022-10-13 17:22:35] - INFO: ###  hidden_size = 768
[2022-10-13 17:22:35] - INFO: ###  num_hidden_layers = 12
[2022-10-13 17:22:35] - INFO: ###  num_attention_heads = 12
[2022-10-13 17:22:35] - INFO: ###  hidden_act = gelu
[2022-10-13 17:22:35] - INFO: ###  intermediate_size = 3072
[2022-10-13 17:22:35] - INFO: ###  pad_token_id = 0
[2022-10-13 17:22:35] - INFO: ###  hidden_dropout_prob = 0.1
[2022-10-13 17:22:35] - INFO: ###  attention_probs_dropout_prob = 0.1
[2022-10-13 17:22:35] - INFO: ###  max_position_embeddings = 512
[2022-10-13 17:22:35] - INFO: ###  type_vocab_size = 2
[2022-10-13 17:22:35] - INFO: ###  initializer_range = 0.02
[2022-10-13 17:22:35] - INFO: ###  directionality = bidi
[2022-10-13 17:22:35] - INFO: ###  pooler_fc_size = 768
[2022-10-13 17:22:35] - INFO: ###  pooler_num_attention_heads = 12
[2022-10-13 17:22:35] - INFO: ###  pooler_num_fc_layers = 3
[2022-10-13 17:22:35] - INFO: ###  pooler_size_per_head = 128
[2022-10-13 17:22:35] - INFO: ###  pooler_type = first_token_transform
[2022-10-13 17:22:37] - INFO: ## Ê≥®ÊÑèÔºåÊ≠£Âú®‰ΩøÁî®Êú¨Âú∞MyTransformer‰∏≠ÁöÑMyMultiHeadAttentionÂÆûÁé∞
[2022-10-13 17:22:39] - INFO: ## ÊàêÂäüËΩΩÂÖ•Â∑≤ÊúâÊ®°ÂûãÔºåËøõË°åÈ¢ÑÊµã......
[2022-10-13 17:22:39] - INFO: ÁºìÂ≠òÊñá‰ª∂ predict_None.pt ‰∏çÂ≠òÂú®ÔºåÈáçÊñ∞Â§ÑÁêÜÂπ∂ÁºìÂ≠òÔºÅ
[2022-10-13 17:23:07] - INFO: ÊàêÂäüÂØºÂÖ•BERTÈÖçÁΩÆÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/config.json
[2022-10-13 17:23:07] - INFO:  ### Â∞ÜÂΩìÂâçÈÖçÁΩÆÊâìÂç∞Âà∞Êó•ÂøóÊñá‰ª∂‰∏≠ 
[2022-10-13 17:23:07] - INFO: ###  project_dir = /home3/Tpandeng/mxy/BertWithPretrained
[2022-10-13 17:23:07] - INFO: ###  dataset_dir = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification
[2022-10-13 17:23:07] - INFO: ###  pretrained_model_dir = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese
[2022-10-13 17:23:07] - INFO: ###  vocab_path = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/vocab.txt
[2022-10-13 17:23:07] - INFO: ###  device = cuda:0
[2022-10-13 17:23:07] - INFO: ###  train_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_train.txt
[2022-10-13 17:23:07] - INFO: ###  val_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_val.txt
[2022-10-13 17:23:07] - INFO: ###  test_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_test.txt
[2022-10-13 17:23:07] - INFO: ###  model_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/cache
[2022-10-13 17:23:07] - INFO: ###  logs_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/logs
[2022-10-13 17:23:07] - INFO: ###  split_sep = _!_
[2022-10-13 17:23:07] - INFO: ###  is_sample_shuffle = True
[2022-10-13 17:23:07] - INFO: ###  batch_size = 64
[2022-10-13 17:23:07] - INFO: ###  max_sen_len = None
[2022-10-13 17:23:07] - INFO: ###  num_labels = 15
[2022-10-13 17:23:07] - INFO: ###  epochs = 1
[2022-10-13 17:23:07] - INFO: ###  model_val_per_epoch = 2
[2022-10-13 17:23:07] - INFO: ###  vocab_size = 21128
[2022-10-13 17:23:07] - INFO: ###  hidden_size = 768
[2022-10-13 17:23:07] - INFO: ###  num_hidden_layers = 12
[2022-10-13 17:23:07] - INFO: ###  num_attention_heads = 12
[2022-10-13 17:23:07] - INFO: ###  hidden_act = gelu
[2022-10-13 17:23:07] - INFO: ###  intermediate_size = 3072
[2022-10-13 17:23:07] - INFO: ###  pad_token_id = 0
[2022-10-13 17:23:07] - INFO: ###  hidden_dropout_prob = 0.1
[2022-10-13 17:23:07] - INFO: ###  attention_probs_dropout_prob = 0.1
[2022-10-13 17:23:07] - INFO: ###  max_position_embeddings = 512
[2022-10-13 17:23:07] - INFO: ###  type_vocab_size = 2
[2022-10-13 17:23:07] - INFO: ###  initializer_range = 0.02
[2022-10-13 17:23:07] - INFO: ###  directionality = bidi
[2022-10-13 17:23:07] - INFO: ###  pooler_fc_size = 768
[2022-10-13 17:23:07] - INFO: ###  pooler_num_attention_heads = 12
[2022-10-13 17:23:07] - INFO: ###  pooler_num_fc_layers = 3
[2022-10-13 17:23:07] - INFO: ###  pooler_size_per_head = 128
[2022-10-13 17:23:07] - INFO: ###  pooler_type = first_token_transform
[2022-10-13 17:23:09] - INFO: ## Ê≥®ÊÑèÔºåÊ≠£Âú®‰ΩøÁî®Êú¨Âú∞MyTransformer‰∏≠ÁöÑMyMultiHeadAttentionÂÆûÁé∞
[2022-10-13 17:23:11] - INFO: ## ÊàêÂäüËΩΩÂÖ•Â∑≤ÊúâÊ®°ÂûãÔºåËøõË°åÈ¢ÑÊµã......
[2022-10-13 17:23:11] - INFO: ÁºìÂ≠òÊñá‰ª∂ predict_None.pt Â≠òÂú®ÔºåÁõ¥Êé•ËΩΩÂÖ•ÁºìÂ≠òÊñá‰ª∂ÔºÅ
[2022-10-13 17:24:19] - INFO: ÊàêÂäüÂØºÂÖ•BERTÈÖçÁΩÆÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/config.json
[2022-10-13 17:24:19] - INFO:  ### Â∞ÜÂΩìÂâçÈÖçÁΩÆÊâìÂç∞Âà∞Êó•ÂøóÊñá‰ª∂‰∏≠ 
[2022-10-13 17:24:19] - INFO: ###  project_dir = /home3/Tpandeng/mxy/BertWithPretrained
[2022-10-13 17:24:19] - INFO: ###  dataset_dir = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification
[2022-10-13 17:24:19] - INFO: ###  pretrained_model_dir = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese
[2022-10-13 17:24:19] - INFO: ###  vocab_path = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/vocab.txt
[2022-10-13 17:24:19] - INFO: ###  device = cuda:0
[2022-10-13 17:24:19] - INFO: ###  train_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_train.txt
[2022-10-13 17:24:19] - INFO: ###  val_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_val.txt
[2022-10-13 17:24:19] - INFO: ###  test_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_test.txt
[2022-10-13 17:24:19] - INFO: ###  model_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/cache
[2022-10-13 17:24:19] - INFO: ###  logs_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/logs
[2022-10-13 17:24:19] - INFO: ###  split_sep = _!_
[2022-10-13 17:24:19] - INFO: ###  is_sample_shuffle = True
[2022-10-13 17:24:19] - INFO: ###  batch_size = 64
[2022-10-13 17:24:19] - INFO: ###  max_sen_len = None
[2022-10-13 17:24:19] - INFO: ###  num_labels = 15
[2022-10-13 17:24:19] - INFO: ###  epochs = 1
[2022-10-13 17:24:19] - INFO: ###  model_val_per_epoch = 2
[2022-10-13 17:24:19] - INFO: ###  vocab_size = 21128
[2022-10-13 17:24:19] - INFO: ###  hidden_size = 768
[2022-10-13 17:24:19] - INFO: ###  num_hidden_layers = 12
[2022-10-13 17:24:19] - INFO: ###  num_attention_heads = 12
[2022-10-13 17:24:19] - INFO: ###  hidden_act = gelu
[2022-10-13 17:24:19] - INFO: ###  intermediate_size = 3072
[2022-10-13 17:24:19] - INFO: ###  pad_token_id = 0
[2022-10-13 17:24:19] - INFO: ###  hidden_dropout_prob = 0.1
[2022-10-13 17:24:19] - INFO: ###  attention_probs_dropout_prob = 0.1
[2022-10-13 17:24:19] - INFO: ###  max_position_embeddings = 512
[2022-10-13 17:24:19] - INFO: ###  type_vocab_size = 2
[2022-10-13 17:24:19] - INFO: ###  initializer_range = 0.02
[2022-10-13 17:24:19] - INFO: ###  directionality = bidi
[2022-10-13 17:24:19] - INFO: ###  pooler_fc_size = 768
[2022-10-13 17:24:19] - INFO: ###  pooler_num_attention_heads = 12
[2022-10-13 17:24:19] - INFO: ###  pooler_num_fc_layers = 3
[2022-10-13 17:24:19] - INFO: ###  pooler_size_per_head = 128
[2022-10-13 17:24:19] - INFO: ###  pooler_type = first_token_transform
[2022-10-13 17:24:21] - INFO: ## Ê≥®ÊÑèÔºåÊ≠£Âú®‰ΩøÁî®Êú¨Âú∞MyTransformer‰∏≠ÁöÑMyMultiHeadAttentionÂÆûÁé∞
[2022-10-13 17:24:23] - INFO: ## ÊàêÂäüËΩΩÂÖ•Â∑≤ÊúâÊ®°ÂûãÔºåËøõË°åÈ¢ÑÊµã......
[2022-10-13 17:24:23] - INFO: ÁºìÂ≠òÊñá‰ª∂ predict_None.pt Â≠òÂú®ÔºåÁõ¥Êé•ËΩΩÂÖ•ÁºìÂ≠òÊñá‰ª∂ÔºÅ
[2022-10-13 17:26:47] - INFO: ÊàêÂäüÂØºÂÖ•BERTÈÖçÁΩÆÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/config.json
[2022-10-13 17:26:47] - INFO:  ### Â∞ÜÂΩìÂâçÈÖçÁΩÆÊâìÂç∞Âà∞Êó•ÂøóÊñá‰ª∂‰∏≠ 
[2022-10-13 17:26:47] - INFO: ###  project_dir = /home3/Tpandeng/mxy/BertWithPretrained
[2022-10-13 17:26:47] - INFO: ###  dataset_dir = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification
[2022-10-13 17:26:47] - INFO: ###  pretrained_model_dir = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese
[2022-10-13 17:26:47] - INFO: ###  vocab_path = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/vocab.txt
[2022-10-13 17:26:47] - INFO: ###  device = cuda:0
[2022-10-13 17:26:47] - INFO: ###  train_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_train.txt
[2022-10-13 17:26:47] - INFO: ###  val_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_val.txt
[2022-10-13 17:26:47] - INFO: ###  test_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_test.txt
[2022-10-13 17:26:47] - INFO: ###  model_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/cache
[2022-10-13 17:26:47] - INFO: ###  logs_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/logs
[2022-10-13 17:26:47] - INFO: ###  split_sep = _!_
[2022-10-13 17:26:47] - INFO: ###  is_sample_shuffle = True
[2022-10-13 17:26:47] - INFO: ###  batch_size = 64
[2022-10-13 17:26:47] - INFO: ###  max_sen_len = None
[2022-10-13 17:26:47] - INFO: ###  num_labels = 15
[2022-10-13 17:26:47] - INFO: ###  epochs = 1
[2022-10-13 17:26:47] - INFO: ###  model_val_per_epoch = 2
[2022-10-13 17:26:47] - INFO: ###  vocab_size = 21128
[2022-10-13 17:26:47] - INFO: ###  hidden_size = 768
[2022-10-13 17:26:47] - INFO: ###  num_hidden_layers = 12
[2022-10-13 17:26:47] - INFO: ###  num_attention_heads = 12
[2022-10-13 17:26:47] - INFO: ###  hidden_act = gelu
[2022-10-13 17:26:47] - INFO: ###  intermediate_size = 3072
[2022-10-13 17:26:47] - INFO: ###  pad_token_id = 0
[2022-10-13 17:26:47] - INFO: ###  hidden_dropout_prob = 0.1
[2022-10-13 17:26:47] - INFO: ###  attention_probs_dropout_prob = 0.1
[2022-10-13 17:26:47] - INFO: ###  max_position_embeddings = 512
[2022-10-13 17:26:47] - INFO: ###  type_vocab_size = 2
[2022-10-13 17:26:47] - INFO: ###  initializer_range = 0.02
[2022-10-13 17:26:47] - INFO: ###  directionality = bidi
[2022-10-13 17:26:47] - INFO: ###  pooler_fc_size = 768
[2022-10-13 17:26:47] - INFO: ###  pooler_num_attention_heads = 12
[2022-10-13 17:26:47] - INFO: ###  pooler_num_fc_layers = 3
[2022-10-13 17:26:47] - INFO: ###  pooler_size_per_head = 128
[2022-10-13 17:26:47] - INFO: ###  pooler_type = first_token_transform
[2022-10-13 17:26:49] - INFO: ## Ê≥®ÊÑèÔºåÊ≠£Âú®‰ΩøÁî®Êú¨Âú∞MyTransformer‰∏≠ÁöÑMyMultiHeadAttentionÂÆûÁé∞
[2022-10-13 17:26:51] - INFO: ## ÊàêÂäüËΩΩÂÖ•Â∑≤ÊúâÊ®°ÂûãÔºåËøõË°åÈ¢ÑÊµã......
[2022-10-13 17:26:51] - INFO: ÁºìÂ≠òÊñá‰ª∂ predict_None.pt Â≠òÂú®ÔºåÁõ¥Êé•ËΩΩÂÖ•ÁºìÂ≠òÊñá‰ª∂ÔºÅ
[2022-10-13 17:28:10] - INFO: ÊàêÂäüÂØºÂÖ•BERTÈÖçÁΩÆÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/config.json
[2022-10-13 17:28:10] - INFO:  ### Â∞ÜÂΩìÂâçÈÖçÁΩÆÊâìÂç∞Âà∞Êó•ÂøóÊñá‰ª∂‰∏≠ 
[2022-10-13 17:28:10] - INFO: ###  project_dir = /home3/Tpandeng/mxy/BertWithPretrained
[2022-10-13 17:28:10] - INFO: ###  dataset_dir = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification
[2022-10-13 17:28:10] - INFO: ###  pretrained_model_dir = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese
[2022-10-13 17:28:10] - INFO: ###  vocab_path = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/vocab.txt
[2022-10-13 17:28:10] - INFO: ###  device = cuda:0
[2022-10-13 17:28:10] - INFO: ###  train_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_train.txt
[2022-10-13 17:28:10] - INFO: ###  val_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_val.txt
[2022-10-13 17:28:10] - INFO: ###  test_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_test.txt
[2022-10-13 17:28:10] - INFO: ###  model_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/cache
[2022-10-13 17:28:10] - INFO: ###  logs_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/logs
[2022-10-13 17:28:10] - INFO: ###  split_sep = _!_
[2022-10-13 17:28:10] - INFO: ###  is_sample_shuffle = True
[2022-10-13 17:28:10] - INFO: ###  batch_size = 64
[2022-10-13 17:28:10] - INFO: ###  max_sen_len = None
[2022-10-13 17:28:10] - INFO: ###  num_labels = 15
[2022-10-13 17:28:10] - INFO: ###  epochs = 1
[2022-10-13 17:28:10] - INFO: ###  model_val_per_epoch = 2
[2022-10-13 17:28:10] - INFO: ###  vocab_size = 21128
[2022-10-13 17:28:10] - INFO: ###  hidden_size = 768
[2022-10-13 17:28:10] - INFO: ###  num_hidden_layers = 12
[2022-10-13 17:28:10] - INFO: ###  num_attention_heads = 12
[2022-10-13 17:28:10] - INFO: ###  hidden_act = gelu
[2022-10-13 17:28:10] - INFO: ###  intermediate_size = 3072
[2022-10-13 17:28:10] - INFO: ###  pad_token_id = 0
[2022-10-13 17:28:10] - INFO: ###  hidden_dropout_prob = 0.1
[2022-10-13 17:28:10] - INFO: ###  attention_probs_dropout_prob = 0.1
[2022-10-13 17:28:10] - INFO: ###  max_position_embeddings = 512
[2022-10-13 17:28:10] - INFO: ###  type_vocab_size = 2
[2022-10-13 17:28:10] - INFO: ###  initializer_range = 0.02
[2022-10-13 17:28:10] - INFO: ###  directionality = bidi
[2022-10-13 17:28:10] - INFO: ###  pooler_fc_size = 768
[2022-10-13 17:28:10] - INFO: ###  pooler_num_attention_heads = 12
[2022-10-13 17:28:10] - INFO: ###  pooler_num_fc_layers = 3
[2022-10-13 17:28:10] - INFO: ###  pooler_size_per_head = 128
[2022-10-13 17:28:10] - INFO: ###  pooler_type = first_token_transform
[2022-10-13 17:28:12] - INFO: ## Ê≥®ÊÑèÔºåÊ≠£Âú®‰ΩøÁî®Êú¨Âú∞MyTransformer‰∏≠ÁöÑMyMultiHeadAttentionÂÆûÁé∞
[2022-10-13 17:28:14] - INFO: ## ÊàêÂäüËΩΩÂÖ•Â∑≤ÊúâÊ®°ÂûãÔºåËøõË°åÈ¢ÑÊµã......
[2022-10-13 17:28:14] - INFO: ÁºìÂ≠òÊñá‰ª∂ predict_None.pt Â≠òÂú®ÔºåÁõ¥Êé•ËΩΩÂÖ•ÁºìÂ≠òÊñá‰ª∂ÔºÅ
[2022-10-13 17:29:12] - INFO: ÊàêÂäüÂØºÂÖ•BERTÈÖçÁΩÆÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/config.json
[2022-10-13 17:29:12] - INFO:  ### Â∞ÜÂΩìÂâçÈÖçÁΩÆÊâìÂç∞Âà∞Êó•ÂøóÊñá‰ª∂‰∏≠ 
[2022-10-13 17:29:12] - INFO: ###  project_dir = /home3/Tpandeng/mxy/BertWithPretrained
[2022-10-13 17:29:12] - INFO: ###  dataset_dir = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification
[2022-10-13 17:29:12] - INFO: ###  pretrained_model_dir = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese
[2022-10-13 17:29:12] - INFO: ###  vocab_path = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/vocab.txt
[2022-10-13 17:29:12] - INFO: ###  device = cuda:0
[2022-10-13 17:29:12] - INFO: ###  train_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_train.txt
[2022-10-13 17:29:12] - INFO: ###  val_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_val.txt
[2022-10-13 17:29:12] - INFO: ###  test_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_test.txt
[2022-10-13 17:29:12] - INFO: ###  model_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/cache
[2022-10-13 17:29:12] - INFO: ###  logs_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/logs
[2022-10-13 17:29:12] - INFO: ###  split_sep = _!_
[2022-10-13 17:29:12] - INFO: ###  is_sample_shuffle = True
[2022-10-13 17:29:12] - INFO: ###  batch_size = 64
[2022-10-13 17:29:12] - INFO: ###  max_sen_len = None
[2022-10-13 17:29:12] - INFO: ###  num_labels = 15
[2022-10-13 17:29:12] - INFO: ###  epochs = 1
[2022-10-13 17:29:12] - INFO: ###  model_val_per_epoch = 2
[2022-10-13 17:29:12] - INFO: ###  vocab_size = 21128
[2022-10-13 17:29:12] - INFO: ###  hidden_size = 768
[2022-10-13 17:29:12] - INFO: ###  num_hidden_layers = 12
[2022-10-13 17:29:12] - INFO: ###  num_attention_heads = 12
[2022-10-13 17:29:12] - INFO: ###  hidden_act = gelu
[2022-10-13 17:29:12] - INFO: ###  intermediate_size = 3072
[2022-10-13 17:29:12] - INFO: ###  pad_token_id = 0
[2022-10-13 17:29:12] - INFO: ###  hidden_dropout_prob = 0.1
[2022-10-13 17:29:12] - INFO: ###  attention_probs_dropout_prob = 0.1
[2022-10-13 17:29:12] - INFO: ###  max_position_embeddings = 512
[2022-10-13 17:29:12] - INFO: ###  type_vocab_size = 2
[2022-10-13 17:29:12] - INFO: ###  initializer_range = 0.02
[2022-10-13 17:29:12] - INFO: ###  directionality = bidi
[2022-10-13 17:29:12] - INFO: ###  pooler_fc_size = 768
[2022-10-13 17:29:12] - INFO: ###  pooler_num_attention_heads = 12
[2022-10-13 17:29:12] - INFO: ###  pooler_num_fc_layers = 3
[2022-10-13 17:29:12] - INFO: ###  pooler_size_per_head = 128
[2022-10-13 17:29:12] - INFO: ###  pooler_type = first_token_transform
[2022-10-13 17:29:14] - INFO: ## Ê≥®ÊÑèÔºåÊ≠£Âú®‰ΩøÁî®Êú¨Âú∞MyTransformer‰∏≠ÁöÑMyMultiHeadAttentionÂÆûÁé∞
[2022-10-13 17:29:16] - INFO: ## ÊàêÂäüËΩΩÂÖ•Â∑≤ÊúâÊ®°ÂûãÔºåËøõË°åÈ¢ÑÊµã......
[2022-10-13 17:29:16] - INFO: ÁºìÂ≠òÊñá‰ª∂ predict_None.pt Â≠òÂú®ÔºåÁõ¥Êé•ËΩΩÂÖ•ÁºìÂ≠òÊñá‰ª∂ÔºÅ
[2022-10-13 17:31:04] - INFO: ÊàêÂäüÂØºÂÖ•BERTÈÖçÁΩÆÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/config.json
[2022-10-13 17:31:04] - INFO:  ### Â∞ÜÂΩìÂâçÈÖçÁΩÆÊâìÂç∞Âà∞Êó•ÂøóÊñá‰ª∂‰∏≠ 
[2022-10-13 17:31:04] - INFO: ###  project_dir = /home3/Tpandeng/mxy/BertWithPretrained
[2022-10-13 17:31:04] - INFO: ###  dataset_dir = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification
[2022-10-13 17:31:04] - INFO: ###  pretrained_model_dir = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese
[2022-10-13 17:31:04] - INFO: ###  vocab_path = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/vocab.txt
[2022-10-13 17:31:04] - INFO: ###  device = cuda:0
[2022-10-13 17:31:04] - INFO: ###  train_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_train.txt
[2022-10-13 17:31:04] - INFO: ###  val_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_val.txt
[2022-10-13 17:31:04] - INFO: ###  test_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_test.txt
[2022-10-13 17:31:04] - INFO: ###  model_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/cache
[2022-10-13 17:31:04] - INFO: ###  logs_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/logs
[2022-10-13 17:31:04] - INFO: ###  split_sep = _!_
[2022-10-13 17:31:04] - INFO: ###  is_sample_shuffle = True
[2022-10-13 17:31:04] - INFO: ###  batch_size = 64
[2022-10-13 17:31:04] - INFO: ###  max_sen_len = None
[2022-10-13 17:31:04] - INFO: ###  num_labels = 15
[2022-10-13 17:31:04] - INFO: ###  epochs = 1
[2022-10-13 17:31:04] - INFO: ###  model_val_per_epoch = 2
[2022-10-13 17:31:04] - INFO: ###  vocab_size = 21128
[2022-10-13 17:31:04] - INFO: ###  hidden_size = 768
[2022-10-13 17:31:04] - INFO: ###  num_hidden_layers = 12
[2022-10-13 17:31:04] - INFO: ###  num_attention_heads = 12
[2022-10-13 17:31:04] - INFO: ###  hidden_act = gelu
[2022-10-13 17:31:04] - INFO: ###  intermediate_size = 3072
[2022-10-13 17:31:04] - INFO: ###  pad_token_id = 0
[2022-10-13 17:31:04] - INFO: ###  hidden_dropout_prob = 0.1
[2022-10-13 17:31:04] - INFO: ###  attention_probs_dropout_prob = 0.1
[2022-10-13 17:31:04] - INFO: ###  max_position_embeddings = 512
[2022-10-13 17:31:04] - INFO: ###  type_vocab_size = 2
[2022-10-13 17:31:04] - INFO: ###  initializer_range = 0.02
[2022-10-13 17:31:04] - INFO: ###  directionality = bidi
[2022-10-13 17:31:04] - INFO: ###  pooler_fc_size = 768
[2022-10-13 17:31:04] - INFO: ###  pooler_num_attention_heads = 12
[2022-10-13 17:31:04] - INFO: ###  pooler_num_fc_layers = 3
[2022-10-13 17:31:04] - INFO: ###  pooler_size_per_head = 128
[2022-10-13 17:31:04] - INFO: ###  pooler_type = first_token_transform
[2022-10-13 17:31:06] - INFO: ## Ê≥®ÊÑèÔºåÊ≠£Âú®‰ΩøÁî®Êú¨Âú∞MyTransformer‰∏≠ÁöÑMyMultiHeadAttentionÂÆûÁé∞
[2022-10-13 17:31:07] - INFO: ## ÊàêÂäüËΩΩÂÖ•Â∑≤ÊúâÊ®°ÂûãÔºåËøõË°åÈ¢ÑÊµã......
[2022-10-13 17:31:08] - INFO: ÁºìÂ≠òÊñá‰ª∂ predict_None.pt Â≠òÂú®ÔºåÁõ¥Êé•ËΩΩÂÖ•ÁºìÂ≠òÊñá‰ª∂ÔºÅ
[2022-10-13 17:33:30] - INFO: ÊàêÂäüÂØºÂÖ•BERTÈÖçÁΩÆÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/config.json
[2022-10-13 17:33:30] - INFO:  ### Â∞ÜÂΩìÂâçÈÖçÁΩÆÊâìÂç∞Âà∞Êó•ÂøóÊñá‰ª∂‰∏≠ 
[2022-10-13 17:33:30] - INFO: ###  project_dir = /home3/Tpandeng/mxy/BertWithPretrained
[2022-10-13 17:33:30] - INFO: ###  dataset_dir = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification
[2022-10-13 17:33:30] - INFO: ###  pretrained_model_dir = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese
[2022-10-13 17:33:30] - INFO: ###  vocab_path = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/vocab.txt
[2022-10-13 17:33:30] - INFO: ###  device = cuda:0
[2022-10-13 17:33:30] - INFO: ###  train_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_train.txt
[2022-10-13 17:33:30] - INFO: ###  val_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_val.txt
[2022-10-13 17:33:30] - INFO: ###  test_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_test.txt
[2022-10-13 17:33:30] - INFO: ###  model_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/cache
[2022-10-13 17:33:30] - INFO: ###  logs_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/logs
[2022-10-13 17:33:30] - INFO: ###  split_sep = _!_
[2022-10-13 17:33:30] - INFO: ###  is_sample_shuffle = True
[2022-10-13 17:33:30] - INFO: ###  batch_size = 64
[2022-10-13 17:33:30] - INFO: ###  max_sen_len = None
[2022-10-13 17:33:30] - INFO: ###  num_labels = 15
[2022-10-13 17:33:30] - INFO: ###  epochs = 1
[2022-10-13 17:33:30] - INFO: ###  model_val_per_epoch = 2
[2022-10-13 17:33:30] - INFO: ###  vocab_size = 21128
[2022-10-13 17:33:30] - INFO: ###  hidden_size = 768
[2022-10-13 17:33:30] - INFO: ###  num_hidden_layers = 12
[2022-10-13 17:33:30] - INFO: ###  num_attention_heads = 12
[2022-10-13 17:33:30] - INFO: ###  hidden_act = gelu
[2022-10-13 17:33:30] - INFO: ###  intermediate_size = 3072
[2022-10-13 17:33:30] - INFO: ###  pad_token_id = 0
[2022-10-13 17:33:30] - INFO: ###  hidden_dropout_prob = 0.1
[2022-10-13 17:33:30] - INFO: ###  attention_probs_dropout_prob = 0.1
[2022-10-13 17:33:30] - INFO: ###  max_position_embeddings = 512
[2022-10-13 17:33:30] - INFO: ###  type_vocab_size = 2
[2022-10-13 17:33:30] - INFO: ###  initializer_range = 0.02
[2022-10-13 17:33:30] - INFO: ###  directionality = bidi
[2022-10-13 17:33:30] - INFO: ###  pooler_fc_size = 768
[2022-10-13 17:33:30] - INFO: ###  pooler_num_attention_heads = 12
[2022-10-13 17:33:30] - INFO: ###  pooler_num_fc_layers = 3
[2022-10-13 17:33:30] - INFO: ###  pooler_size_per_head = 128
[2022-10-13 17:33:30] - INFO: ###  pooler_type = first_token_transform
[2022-10-13 17:33:32] - INFO: ## Ê≥®ÊÑèÔºåÊ≠£Âú®‰ΩøÁî®Êú¨Âú∞MyTransformer‰∏≠ÁöÑMyMultiHeadAttentionÂÆûÁé∞
[2022-10-13 17:33:34] - INFO: ## ÊàêÂäüËΩΩÂÖ•Â∑≤ÊúâÊ®°ÂûãÔºåËøõË°åÈ¢ÑÊµã......
[2022-10-13 17:33:34] - INFO: ÁºìÂ≠òÊñá‰ª∂ predict_None.pt ‰∏çÂ≠òÂú®ÔºåÈáçÊñ∞Â§ÑÁêÜÂπ∂ÁºìÂ≠òÔºÅ
[2022-10-13 17:34:45] - INFO: ÊàêÂäüÂØºÂÖ•BERTÈÖçÁΩÆÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/config.json
[2022-10-13 17:34:45] - INFO:  ### Â∞ÜÂΩìÂâçÈÖçÁΩÆÊâìÂç∞Âà∞Êó•ÂøóÊñá‰ª∂‰∏≠ 
[2022-10-13 17:34:45] - INFO: ###  project_dir = /home3/Tpandeng/mxy/BertWithPretrained
[2022-10-13 17:34:45] - INFO: ###  dataset_dir = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification
[2022-10-13 17:34:45] - INFO: ###  pretrained_model_dir = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese
[2022-10-13 17:34:45] - INFO: ###  vocab_path = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/vocab.txt
[2022-10-13 17:34:45] - INFO: ###  device = cuda:0
[2022-10-13 17:34:45] - INFO: ###  train_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_train.txt
[2022-10-13 17:34:45] - INFO: ###  val_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_val.txt
[2022-10-13 17:34:45] - INFO: ###  test_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_test.txt
[2022-10-13 17:34:45] - INFO: ###  model_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/cache
[2022-10-13 17:34:45] - INFO: ###  logs_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/logs
[2022-10-13 17:34:45] - INFO: ###  split_sep = _!_
[2022-10-13 17:34:45] - INFO: ###  is_sample_shuffle = True
[2022-10-13 17:34:45] - INFO: ###  batch_size = 64
[2022-10-13 17:34:45] - INFO: ###  max_sen_len = None
[2022-10-13 17:34:45] - INFO: ###  num_labels = 15
[2022-10-13 17:34:45] - INFO: ###  epochs = 1
[2022-10-13 17:34:45] - INFO: ###  model_val_per_epoch = 2
[2022-10-13 17:34:45] - INFO: ###  vocab_size = 21128
[2022-10-13 17:34:45] - INFO: ###  hidden_size = 768
[2022-10-13 17:34:45] - INFO: ###  num_hidden_layers = 12
[2022-10-13 17:34:45] - INFO: ###  num_attention_heads = 12
[2022-10-13 17:34:45] - INFO: ###  hidden_act = gelu
[2022-10-13 17:34:45] - INFO: ###  intermediate_size = 3072
[2022-10-13 17:34:45] - INFO: ###  pad_token_id = 0
[2022-10-13 17:34:45] - INFO: ###  hidden_dropout_prob = 0.1
[2022-10-13 17:34:45] - INFO: ###  attention_probs_dropout_prob = 0.1
[2022-10-13 17:34:45] - INFO: ###  max_position_embeddings = 512
[2022-10-13 17:34:45] - INFO: ###  type_vocab_size = 2
[2022-10-13 17:34:45] - INFO: ###  initializer_range = 0.02
[2022-10-13 17:34:45] - INFO: ###  directionality = bidi
[2022-10-13 17:34:45] - INFO: ###  pooler_fc_size = 768
[2022-10-13 17:34:45] - INFO: ###  pooler_num_attention_heads = 12
[2022-10-13 17:34:45] - INFO: ###  pooler_num_fc_layers = 3
[2022-10-13 17:34:45] - INFO: ###  pooler_size_per_head = 128
[2022-10-13 17:34:45] - INFO: ###  pooler_type = first_token_transform
[2022-10-13 17:34:47] - INFO: ## Ê≥®ÊÑèÔºåÊ≠£Âú®‰ΩøÁî®Êú¨Âú∞MyTransformer‰∏≠ÁöÑMyMultiHeadAttentionÂÆûÁé∞
[2022-10-13 17:34:49] - INFO: ## ÊàêÂäüËΩΩÂÖ•Â∑≤ÊúâÊ®°ÂûãÔºåËøõË°åÈ¢ÑÊµã......
[2022-10-13 17:34:50] - INFO: ÁºìÂ≠òÊñá‰ª∂ predict_None.pt ‰∏çÂ≠òÂú®ÔºåÈáçÊñ∞Â§ÑÁêÜÂπ∂ÁºìÂ≠òÔºÅ
[2022-10-13 17:35:47] - INFO: ÊàêÂäüÂØºÂÖ•BERTÈÖçÁΩÆÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/config.json
[2022-10-13 17:35:47] - INFO:  ### Â∞ÜÂΩìÂâçÈÖçÁΩÆÊâìÂç∞Âà∞Êó•ÂøóÊñá‰ª∂‰∏≠ 
[2022-10-13 17:35:47] - INFO: ###  project_dir = /home3/Tpandeng/mxy/BertWithPretrained
[2022-10-13 17:35:47] - INFO: ###  dataset_dir = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification
[2022-10-13 17:35:47] - INFO: ###  pretrained_model_dir = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese
[2022-10-13 17:35:47] - INFO: ###  vocab_path = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/vocab.txt
[2022-10-13 17:35:47] - INFO: ###  device = cuda:0
[2022-10-13 17:35:47] - INFO: ###  train_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_train.txt
[2022-10-13 17:35:47] - INFO: ###  val_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_val.txt
[2022-10-13 17:35:47] - INFO: ###  test_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_test.txt
[2022-10-13 17:35:47] - INFO: ###  model_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/cache
[2022-10-13 17:35:47] - INFO: ###  logs_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/logs
[2022-10-13 17:35:47] - INFO: ###  split_sep = _!_
[2022-10-13 17:35:47] - INFO: ###  is_sample_shuffle = True
[2022-10-13 17:35:47] - INFO: ###  batch_size = 64
[2022-10-13 17:35:47] - INFO: ###  max_sen_len = None
[2022-10-13 17:35:47] - INFO: ###  num_labels = 15
[2022-10-13 17:35:47] - INFO: ###  epochs = 1
[2022-10-13 17:35:47] - INFO: ###  model_val_per_epoch = 2
[2022-10-13 17:35:47] - INFO: ###  vocab_size = 21128
[2022-10-13 17:35:47] - INFO: ###  hidden_size = 768
[2022-10-13 17:35:47] - INFO: ###  num_hidden_layers = 12
[2022-10-13 17:35:47] - INFO: ###  num_attention_heads = 12
[2022-10-13 17:35:47] - INFO: ###  hidden_act = gelu
[2022-10-13 17:35:47] - INFO: ###  intermediate_size = 3072
[2022-10-13 17:35:47] - INFO: ###  pad_token_id = 0
[2022-10-13 17:35:47] - INFO: ###  hidden_dropout_prob = 0.1
[2022-10-13 17:35:47] - INFO: ###  attention_probs_dropout_prob = 0.1
[2022-10-13 17:35:47] - INFO: ###  max_position_embeddings = 512
[2022-10-13 17:35:47] - INFO: ###  type_vocab_size = 2
[2022-10-13 17:35:47] - INFO: ###  initializer_range = 0.02
[2022-10-13 17:35:47] - INFO: ###  directionality = bidi
[2022-10-13 17:35:47] - INFO: ###  pooler_fc_size = 768
[2022-10-13 17:35:47] - INFO: ###  pooler_num_attention_heads = 12
[2022-10-13 17:35:47] - INFO: ###  pooler_num_fc_layers = 3
[2022-10-13 17:35:47] - INFO: ###  pooler_size_per_head = 128
[2022-10-13 17:35:47] - INFO: ###  pooler_type = first_token_transform
[2022-10-13 17:35:49] - INFO: ## Ê≥®ÊÑèÔºåÊ≠£Âú®‰ΩøÁî®Êú¨Âú∞MyTransformer‰∏≠ÁöÑMyMultiHeadAttentionÂÆûÁé∞
[2022-10-13 17:35:51] - INFO: ## ÊàêÂäüËΩΩÂÖ•Â∑≤ÊúâÊ®°ÂûãÔºåËøõË°åÈ¢ÑÊµã......
[2022-10-13 17:35:51] - INFO: ÁºìÂ≠òÊñá‰ª∂ predict_None.pt ‰∏çÂ≠òÂú®ÔºåÈáçÊñ∞Â§ÑÁêÜÂπ∂ÁºìÂ≠òÔºÅ
[2022-10-13 17:40:20] - INFO: ÊàêÂäüÂØºÂÖ•BERTÈÖçÁΩÆÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/config.json
[2022-10-13 17:40:20] - INFO:  ### Â∞ÜÂΩìÂâçÈÖçÁΩÆÊâìÂç∞Âà∞Êó•ÂøóÊñá‰ª∂‰∏≠ 
[2022-10-13 17:40:20] - INFO: ###  project_dir = /home3/Tpandeng/mxy/BertWithPretrained
[2022-10-13 17:40:20] - INFO: ###  dataset_dir = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification
[2022-10-13 17:40:20] - INFO: ###  pretrained_model_dir = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese
[2022-10-13 17:40:20] - INFO: ###  vocab_path = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/vocab.txt
[2022-10-13 17:40:20] - INFO: ###  device = cuda:0
[2022-10-13 17:40:20] - INFO: ###  train_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_train.txt
[2022-10-13 17:40:20] - INFO: ###  val_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_val.txt
[2022-10-13 17:40:20] - INFO: ###  test_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_test.txt
[2022-10-13 17:40:20] - INFO: ###  model_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/cache
[2022-10-13 17:40:20] - INFO: ###  logs_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/logs
[2022-10-13 17:40:20] - INFO: ###  split_sep = _!_
[2022-10-13 17:40:20] - INFO: ###  is_sample_shuffle = True
[2022-10-13 17:40:20] - INFO: ###  batch_size = 64
[2022-10-13 17:40:20] - INFO: ###  max_sen_len = None
[2022-10-13 17:40:20] - INFO: ###  num_labels = 15
[2022-10-13 17:40:20] - INFO: ###  epochs = 1
[2022-10-13 17:40:20] - INFO: ###  model_val_per_epoch = 2
[2022-10-13 17:40:20] - INFO: ###  vocab_size = 21128
[2022-10-13 17:40:20] - INFO: ###  hidden_size = 768
[2022-10-13 17:40:20] - INFO: ###  num_hidden_layers = 12
[2022-10-13 17:40:20] - INFO: ###  num_attention_heads = 12
[2022-10-13 17:40:20] - INFO: ###  hidden_act = gelu
[2022-10-13 17:40:20] - INFO: ###  intermediate_size = 3072
[2022-10-13 17:40:20] - INFO: ###  pad_token_id = 0
[2022-10-13 17:40:20] - INFO: ###  hidden_dropout_prob = 0.1
[2022-10-13 17:40:20] - INFO: ###  attention_probs_dropout_prob = 0.1
[2022-10-13 17:40:20] - INFO: ###  max_position_embeddings = 512
[2022-10-13 17:40:20] - INFO: ###  type_vocab_size = 2
[2022-10-13 17:40:20] - INFO: ###  initializer_range = 0.02
[2022-10-13 17:40:20] - INFO: ###  directionality = bidi
[2022-10-13 17:40:20] - INFO: ###  pooler_fc_size = 768
[2022-10-13 17:40:20] - INFO: ###  pooler_num_attention_heads = 12
[2022-10-13 17:40:20] - INFO: ###  pooler_num_fc_layers = 3
[2022-10-13 17:40:20] - INFO: ###  pooler_size_per_head = 128
[2022-10-13 17:40:20] - INFO: ###  pooler_type = first_token_transform
[2022-10-13 17:40:22] - INFO: ## Ê≥®ÊÑèÔºåÊ≠£Âú®‰ΩøÁî®Êú¨Âú∞MyTransformer‰∏≠ÁöÑMyMultiHeadAttentionÂÆûÁé∞
[2022-10-13 17:40:24] - INFO: ## ÊàêÂäüËΩΩÂÖ•Â∑≤ÊúâÊ®°ÂûãÔºåËøõË°åÈ¢ÑÊµã......
[2022-10-13 17:40:24] - INFO: ÁºìÂ≠òÊñá‰ª∂ predict_None.pt ‰∏çÂ≠òÂú®ÔºåÈáçÊñ∞Â§ÑÁêÜÂπ∂ÁºìÂ≠òÔºÅ
[2022-10-13 17:41:06] - INFO: ÊàêÂäüÂØºÂÖ•BERTÈÖçÁΩÆÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/config.json
[2022-10-13 17:41:06] - INFO:  ### Â∞ÜÂΩìÂâçÈÖçÁΩÆÊâìÂç∞Âà∞Êó•ÂøóÊñá‰ª∂‰∏≠ 
[2022-10-13 17:41:06] - INFO: ###  project_dir = /home3/Tpandeng/mxy/BertWithPretrained
[2022-10-13 17:41:06] - INFO: ###  dataset_dir = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification
[2022-10-13 17:41:06] - INFO: ###  pretrained_model_dir = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese
[2022-10-13 17:41:06] - INFO: ###  vocab_path = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/vocab.txt
[2022-10-13 17:41:06] - INFO: ###  device = cuda:0
[2022-10-13 17:41:06] - INFO: ###  train_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_train.txt
[2022-10-13 17:41:06] - INFO: ###  val_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_val.txt
[2022-10-13 17:41:06] - INFO: ###  test_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_test.txt
[2022-10-13 17:41:06] - INFO: ###  model_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/cache
[2022-10-13 17:41:06] - INFO: ###  logs_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/logs
[2022-10-13 17:41:06] - INFO: ###  split_sep = _!_
[2022-10-13 17:41:06] - INFO: ###  is_sample_shuffle = True
[2022-10-13 17:41:06] - INFO: ###  batch_size = 64
[2022-10-13 17:41:06] - INFO: ###  max_sen_len = None
[2022-10-13 17:41:06] - INFO: ###  num_labels = 15
[2022-10-13 17:41:06] - INFO: ###  epochs = 1
[2022-10-13 17:41:06] - INFO: ###  model_val_per_epoch = 2
[2022-10-13 17:41:06] - INFO: ###  vocab_size = 21128
[2022-10-13 17:41:06] - INFO: ###  hidden_size = 768
[2022-10-13 17:41:06] - INFO: ###  num_hidden_layers = 12
[2022-10-13 17:41:06] - INFO: ###  num_attention_heads = 12
[2022-10-13 17:41:06] - INFO: ###  hidden_act = gelu
[2022-10-13 17:41:06] - INFO: ###  intermediate_size = 3072
[2022-10-13 17:41:06] - INFO: ###  pad_token_id = 0
[2022-10-13 17:41:06] - INFO: ###  hidden_dropout_prob = 0.1
[2022-10-13 17:41:06] - INFO: ###  attention_probs_dropout_prob = 0.1
[2022-10-13 17:41:06] - INFO: ###  max_position_embeddings = 512
[2022-10-13 17:41:06] - INFO: ###  type_vocab_size = 2
[2022-10-13 17:41:06] - INFO: ###  initializer_range = 0.02
[2022-10-13 17:41:06] - INFO: ###  directionality = bidi
[2022-10-13 17:41:06] - INFO: ###  pooler_fc_size = 768
[2022-10-13 17:41:06] - INFO: ###  pooler_num_attention_heads = 12
[2022-10-13 17:41:06] - INFO: ###  pooler_num_fc_layers = 3
[2022-10-13 17:41:06] - INFO: ###  pooler_size_per_head = 128
[2022-10-13 17:41:06] - INFO: ###  pooler_type = first_token_transform
[2022-10-13 17:41:08] - INFO: ## Ê≥®ÊÑèÔºåÊ≠£Âú®‰ΩøÁî®Êú¨Âú∞MyTransformer‰∏≠ÁöÑMyMultiHeadAttentionÂÆûÁé∞
[2022-10-13 17:41:10] - INFO: ## ÊàêÂäüËΩΩÂÖ•Â∑≤ÊúâÊ®°ÂûãÔºåËøõË°åÈ¢ÑÊµã......
[2022-10-13 17:41:10] - INFO: ÁºìÂ≠òÊñá‰ª∂ predict_None.pt ‰∏çÂ≠òÂú®ÔºåÈáçÊñ∞Â§ÑÁêÜÂπ∂ÁºìÂ≠òÔºÅ
[2022-10-13 17:42:18] - INFO: ÊàêÂäüÂØºÂÖ•BERTÈÖçÁΩÆÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/config.json
[2022-10-13 17:42:18] - INFO:  ### Â∞ÜÂΩìÂâçÈÖçÁΩÆÊâìÂç∞Âà∞Êó•ÂøóÊñá‰ª∂‰∏≠ 
[2022-10-13 17:42:18] - INFO: ###  project_dir = /home3/Tpandeng/mxy/BertWithPretrained
[2022-10-13 17:42:18] - INFO: ###  dataset_dir = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification
[2022-10-13 17:42:18] - INFO: ###  pretrained_model_dir = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese
[2022-10-13 17:42:18] - INFO: ###  vocab_path = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/vocab.txt
[2022-10-13 17:42:18] - INFO: ###  device = cuda:0
[2022-10-13 17:42:18] - INFO: ###  train_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_train.txt
[2022-10-13 17:42:18] - INFO: ###  val_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_val.txt
[2022-10-13 17:42:18] - INFO: ###  test_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_test.txt
[2022-10-13 17:42:18] - INFO: ###  model_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/cache
[2022-10-13 17:42:18] - INFO: ###  logs_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/logs
[2022-10-13 17:42:18] - INFO: ###  split_sep = _!_
[2022-10-13 17:42:18] - INFO: ###  is_sample_shuffle = True
[2022-10-13 17:42:18] - INFO: ###  batch_size = 64
[2022-10-13 17:42:18] - INFO: ###  max_sen_len = None
[2022-10-13 17:42:18] - INFO: ###  num_labels = 15
[2022-10-13 17:42:18] - INFO: ###  epochs = 1
[2022-10-13 17:42:18] - INFO: ###  model_val_per_epoch = 2
[2022-10-13 17:42:18] - INFO: ###  vocab_size = 21128
[2022-10-13 17:42:18] - INFO: ###  hidden_size = 768
[2022-10-13 17:42:18] - INFO: ###  num_hidden_layers = 12
[2022-10-13 17:42:18] - INFO: ###  num_attention_heads = 12
[2022-10-13 17:42:18] - INFO: ###  hidden_act = gelu
[2022-10-13 17:42:18] - INFO: ###  intermediate_size = 3072
[2022-10-13 17:42:18] - INFO: ###  pad_token_id = 0
[2022-10-13 17:42:18] - INFO: ###  hidden_dropout_prob = 0.1
[2022-10-13 17:42:18] - INFO: ###  attention_probs_dropout_prob = 0.1
[2022-10-13 17:42:18] - INFO: ###  max_position_embeddings = 512
[2022-10-13 17:42:18] - INFO: ###  type_vocab_size = 2
[2022-10-13 17:42:18] - INFO: ###  initializer_range = 0.02
[2022-10-13 17:42:18] - INFO: ###  directionality = bidi
[2022-10-13 17:42:18] - INFO: ###  pooler_fc_size = 768
[2022-10-13 17:42:18] - INFO: ###  pooler_num_attention_heads = 12
[2022-10-13 17:42:18] - INFO: ###  pooler_num_fc_layers = 3
[2022-10-13 17:42:18] - INFO: ###  pooler_size_per_head = 128
[2022-10-13 17:42:18] - INFO: ###  pooler_type = first_token_transform
[2022-10-13 17:42:20] - INFO: ## Ê≥®ÊÑèÔºåÊ≠£Âú®‰ΩøÁî®Êú¨Âú∞MyTransformer‰∏≠ÁöÑMyMultiHeadAttentionÂÆûÁé∞
[2022-10-13 17:42:22] - INFO: ## ÊàêÂäüËΩΩÂÖ•Â∑≤ÊúâÊ®°ÂûãÔºåËøõË°åÈ¢ÑÊµã......
[2022-10-13 17:42:22] - INFO: ÁºìÂ≠òÊñá‰ª∂ predict_None.pt ‰∏çÂ≠òÂú®ÔºåÈáçÊñ∞Â§ÑÁêÜÂπ∂ÁºìÂ≠òÔºÅ
[2022-10-13 17:42:55] - INFO: ÊàêÂäüÂØºÂÖ•BERTÈÖçÁΩÆÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/config.json
[2022-10-13 17:42:55] - INFO:  ### Â∞ÜÂΩìÂâçÈÖçÁΩÆÊâìÂç∞Âà∞Êó•ÂøóÊñá‰ª∂‰∏≠ 
[2022-10-13 17:42:55] - INFO: ###  project_dir = /home3/Tpandeng/mxy/BertWithPretrained
[2022-10-13 17:42:55] - INFO: ###  dataset_dir = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification
[2022-10-13 17:42:55] - INFO: ###  pretrained_model_dir = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese
[2022-10-13 17:42:55] - INFO: ###  vocab_path = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/vocab.txt
[2022-10-13 17:42:55] - INFO: ###  device = cuda:0
[2022-10-13 17:42:55] - INFO: ###  train_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_train.txt
[2022-10-13 17:42:55] - INFO: ###  val_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_val.txt
[2022-10-13 17:42:55] - INFO: ###  test_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_test.txt
[2022-10-13 17:42:55] - INFO: ###  model_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/cache
[2022-10-13 17:42:55] - INFO: ###  logs_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/logs
[2022-10-13 17:42:55] - INFO: ###  split_sep = _!_
[2022-10-13 17:42:55] - INFO: ###  is_sample_shuffle = True
[2022-10-13 17:42:55] - INFO: ###  batch_size = 64
[2022-10-13 17:42:55] - INFO: ###  max_sen_len = None
[2022-10-13 17:42:55] - INFO: ###  num_labels = 15
[2022-10-13 17:42:55] - INFO: ###  epochs = 1
[2022-10-13 17:42:55] - INFO: ###  model_val_per_epoch = 2
[2022-10-13 17:42:55] - INFO: ###  vocab_size = 21128
[2022-10-13 17:42:55] - INFO: ###  hidden_size = 768
[2022-10-13 17:42:55] - INFO: ###  num_hidden_layers = 12
[2022-10-13 17:42:55] - INFO: ###  num_attention_heads = 12
[2022-10-13 17:42:55] - INFO: ###  hidden_act = gelu
[2022-10-13 17:42:55] - INFO: ###  intermediate_size = 3072
[2022-10-13 17:42:55] - INFO: ###  pad_token_id = 0
[2022-10-13 17:42:55] - INFO: ###  hidden_dropout_prob = 0.1
[2022-10-13 17:42:55] - INFO: ###  attention_probs_dropout_prob = 0.1
[2022-10-13 17:42:55] - INFO: ###  max_position_embeddings = 512
[2022-10-13 17:42:55] - INFO: ###  type_vocab_size = 2
[2022-10-13 17:42:55] - INFO: ###  initializer_range = 0.02
[2022-10-13 17:42:55] - INFO: ###  directionality = bidi
[2022-10-13 17:42:55] - INFO: ###  pooler_fc_size = 768
[2022-10-13 17:42:55] - INFO: ###  pooler_num_attention_heads = 12
[2022-10-13 17:42:55] - INFO: ###  pooler_num_fc_layers = 3
[2022-10-13 17:42:55] - INFO: ###  pooler_size_per_head = 128
[2022-10-13 17:42:55] - INFO: ###  pooler_type = first_token_transform
[2022-10-13 17:42:57] - INFO: ## Ê≥®ÊÑèÔºåÊ≠£Âú®‰ΩøÁî®Êú¨Âú∞MyTransformer‰∏≠ÁöÑMyMultiHeadAttentionÂÆûÁé∞
[2022-10-13 17:42:59] - INFO: ## ÊàêÂäüËΩΩÂÖ•Â∑≤ÊúâÊ®°ÂûãÔºåËøõË°åÈ¢ÑÊµã......
[2022-10-13 17:42:59] - INFO: ÁºìÂ≠òÊñá‰ª∂ predict_None.pt ‰∏çÂ≠òÂú®ÔºåÈáçÊñ∞Â§ÑÁêÜÂπ∂ÁºìÂ≠òÔºÅ
[2022-10-13 19:10:44] - INFO: ÊàêÂäüÂØºÂÖ•BERTÈÖçÁΩÆÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/config.json
[2022-10-13 19:10:44] - INFO:  ### Â∞ÜÂΩìÂâçÈÖçÁΩÆÊâìÂç∞Âà∞Êó•ÂøóÊñá‰ª∂‰∏≠ 
[2022-10-13 19:10:44] - INFO: ###  project_dir = /home3/Tpandeng/mxy/BertWithPretrained
[2022-10-13 19:10:44] - INFO: ###  dataset_dir = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification
[2022-10-13 19:10:44] - INFO: ###  pretrained_model_dir = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese
[2022-10-13 19:10:44] - INFO: ###  vocab_path = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/vocab.txt
[2022-10-13 19:10:44] - INFO: ###  device = cuda:0
[2022-10-13 19:10:44] - INFO: ###  train_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_train.txt
[2022-10-13 19:10:44] - INFO: ###  val_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_val.txt
[2022-10-13 19:10:44] - INFO: ###  test_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_test.txt
[2022-10-13 19:10:44] - INFO: ###  model_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/cache
[2022-10-13 19:10:44] - INFO: ###  logs_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/logs
[2022-10-13 19:10:44] - INFO: ###  split_sep = _!_
[2022-10-13 19:10:44] - INFO: ###  is_sample_shuffle = True
[2022-10-13 19:10:44] - INFO: ###  batch_size = 64
[2022-10-13 19:10:44] - INFO: ###  max_sen_len = None
[2022-10-13 19:10:44] - INFO: ###  num_labels = 15
[2022-10-13 19:10:44] - INFO: ###  epochs = 1
[2022-10-13 19:10:44] - INFO: ###  model_val_per_epoch = 2
[2022-10-13 19:10:44] - INFO: ###  vocab_size = 21128
[2022-10-13 19:10:44] - INFO: ###  hidden_size = 768
[2022-10-13 19:10:44] - INFO: ###  num_hidden_layers = 12
[2022-10-13 19:10:44] - INFO: ###  num_attention_heads = 12
[2022-10-13 19:10:44] - INFO: ###  hidden_act = gelu
[2022-10-13 19:10:44] - INFO: ###  intermediate_size = 3072
[2022-10-13 19:10:44] - INFO: ###  pad_token_id = 0
[2022-10-13 19:10:44] - INFO: ###  hidden_dropout_prob = 0.1
[2022-10-13 19:10:44] - INFO: ###  attention_probs_dropout_prob = 0.1
[2022-10-13 19:10:44] - INFO: ###  max_position_embeddings = 512
[2022-10-13 19:10:44] - INFO: ###  type_vocab_size = 2
[2022-10-13 19:10:44] - INFO: ###  initializer_range = 0.02
[2022-10-13 19:10:44] - INFO: ###  directionality = bidi
[2022-10-13 19:10:44] - INFO: ###  pooler_fc_size = 768
[2022-10-13 19:10:44] - INFO: ###  pooler_num_attention_heads = 12
[2022-10-13 19:10:44] - INFO: ###  pooler_num_fc_layers = 3
[2022-10-13 19:10:44] - INFO: ###  pooler_size_per_head = 128
[2022-10-13 19:10:44] - INFO: ###  pooler_type = first_token_transform
[2022-10-13 19:10:46] - INFO: ## Ê≥®ÊÑèÔºåÊ≠£Âú®‰ΩøÁî®Êú¨Âú∞MyTransformer‰∏≠ÁöÑMyMultiHeadAttentionÂÆûÁé∞
[2022-10-13 19:10:48] - INFO: ## ÊàêÂäüËΩΩÂÖ•Â∑≤ÊúâÊ®°ÂûãÔºåËøõË°åÈ¢ÑÊµã......
[2022-10-13 19:10:48] - INFO: ÁºìÂ≠òÊñá‰ª∂ predict_None.pt ‰∏çÂ≠òÂú®ÔºåÈáçÊñ∞Â§ÑÁêÜÂπ∂ÁºìÂ≠òÔºÅ
[2022-10-13 19:15:43] - INFO: ÊàêÂäüÂØºÂÖ•BERTÈÖçÁΩÆÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/config.json
[2022-10-13 19:15:43] - INFO:  ### Â∞ÜÂΩìÂâçÈÖçÁΩÆÊâìÂç∞Âà∞Êó•ÂøóÊñá‰ª∂‰∏≠ 
[2022-10-13 19:15:43] - INFO: ###  project_dir = /home3/Tpandeng/mxy/BertWithPretrained
[2022-10-13 19:15:43] - INFO: ###  dataset_dir = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification
[2022-10-13 19:15:43] - INFO: ###  pretrained_model_dir = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese
[2022-10-13 19:15:43] - INFO: ###  vocab_path = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/vocab.txt
[2022-10-13 19:15:43] - INFO: ###  device = cuda:0
[2022-10-13 19:15:43] - INFO: ###  train_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_train.txt
[2022-10-13 19:15:43] - INFO: ###  val_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_val.txt
[2022-10-13 19:15:43] - INFO: ###  test_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_test.txt
[2022-10-13 19:15:43] - INFO: ###  model_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/cache
[2022-10-13 19:15:43] - INFO: ###  logs_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/logs
[2022-10-13 19:15:43] - INFO: ###  split_sep = _!_
[2022-10-13 19:15:43] - INFO: ###  is_sample_shuffle = True
[2022-10-13 19:15:43] - INFO: ###  batch_size = 64
[2022-10-13 19:15:43] - INFO: ###  max_sen_len = None
[2022-10-13 19:15:43] - INFO: ###  num_labels = 15
[2022-10-13 19:15:43] - INFO: ###  epochs = 1
[2022-10-13 19:15:43] - INFO: ###  model_val_per_epoch = 2
[2022-10-13 19:15:43] - INFO: ###  vocab_size = 21128
[2022-10-13 19:15:43] - INFO: ###  hidden_size = 768
[2022-10-13 19:15:43] - INFO: ###  num_hidden_layers = 12
[2022-10-13 19:15:43] - INFO: ###  num_attention_heads = 12
[2022-10-13 19:15:43] - INFO: ###  hidden_act = gelu
[2022-10-13 19:15:43] - INFO: ###  intermediate_size = 3072
[2022-10-13 19:15:43] - INFO: ###  pad_token_id = 0
[2022-10-13 19:15:43] - INFO: ###  hidden_dropout_prob = 0.1
[2022-10-13 19:15:43] - INFO: ###  attention_probs_dropout_prob = 0.1
[2022-10-13 19:15:43] - INFO: ###  max_position_embeddings = 512
[2022-10-13 19:15:43] - INFO: ###  type_vocab_size = 2
[2022-10-13 19:15:43] - INFO: ###  initializer_range = 0.02
[2022-10-13 19:15:43] - INFO: ###  directionality = bidi
[2022-10-13 19:15:43] - INFO: ###  pooler_fc_size = 768
[2022-10-13 19:15:43] - INFO: ###  pooler_num_attention_heads = 12
[2022-10-13 19:15:43] - INFO: ###  pooler_num_fc_layers = 3
[2022-10-13 19:15:43] - INFO: ###  pooler_size_per_head = 128
[2022-10-13 19:15:43] - INFO: ###  pooler_type = first_token_transform
[2022-10-13 19:15:45] - INFO: ## Ê≥®ÊÑèÔºåÊ≠£Âú®‰ΩøÁî®Êú¨Âú∞MyTransformer‰∏≠ÁöÑMyMultiHeadAttentionÂÆûÁé∞
[2022-10-13 19:15:47] - INFO: ## ÊàêÂäüËΩΩÂÖ•Â∑≤ÊúâÊ®°ÂûãÔºåËøõË°åÈ¢ÑÊµã......
[2022-10-13 19:15:47] - INFO: ÁºìÂ≠òÊñá‰ª∂ predict_None.pt ‰∏çÂ≠òÂú®ÔºåÈáçÊñ∞Â§ÑÁêÜÂπ∂ÁºìÂ≠òÔºÅ
[2022-10-13 19:16:00] - INFO: ÊàêÂäüÂØºÂÖ•BERTÈÖçÁΩÆÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/config.json
[2022-10-13 19:16:00] - INFO:  ### Â∞ÜÂΩìÂâçÈÖçÁΩÆÊâìÂç∞Âà∞Êó•ÂøóÊñá‰ª∂‰∏≠ 
[2022-10-13 19:16:00] - INFO: ###  project_dir = /home3/Tpandeng/mxy/BertWithPretrained
[2022-10-13 19:16:00] - INFO: ###  dataset_dir = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification
[2022-10-13 19:16:00] - INFO: ###  pretrained_model_dir = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese
[2022-10-13 19:16:00] - INFO: ###  vocab_path = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/vocab.txt
[2022-10-13 19:16:00] - INFO: ###  device = cuda:0
[2022-10-13 19:16:00] - INFO: ###  train_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_train.txt
[2022-10-13 19:16:00] - INFO: ###  val_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_val.txt
[2022-10-13 19:16:00] - INFO: ###  test_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_test.txt
[2022-10-13 19:16:00] - INFO: ###  model_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/cache
[2022-10-13 19:16:00] - INFO: ###  logs_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/logs
[2022-10-13 19:16:00] - INFO: ###  split_sep = _!_
[2022-10-13 19:16:00] - INFO: ###  is_sample_shuffle = True
[2022-10-13 19:16:00] - INFO: ###  batch_size = 64
[2022-10-13 19:16:00] - INFO: ###  max_sen_len = None
[2022-10-13 19:16:00] - INFO: ###  num_labels = 15
[2022-10-13 19:16:00] - INFO: ###  epochs = 1
[2022-10-13 19:16:00] - INFO: ###  model_val_per_epoch = 2
[2022-10-13 19:16:00] - INFO: ###  vocab_size = 21128
[2022-10-13 19:16:00] - INFO: ###  hidden_size = 768
[2022-10-13 19:16:00] - INFO: ###  num_hidden_layers = 12
[2022-10-13 19:16:00] - INFO: ###  num_attention_heads = 12
[2022-10-13 19:16:00] - INFO: ###  hidden_act = gelu
[2022-10-13 19:16:00] - INFO: ###  intermediate_size = 3072
[2022-10-13 19:16:00] - INFO: ###  pad_token_id = 0
[2022-10-13 19:16:00] - INFO: ###  hidden_dropout_prob = 0.1
[2022-10-13 19:16:00] - INFO: ###  attention_probs_dropout_prob = 0.1
[2022-10-13 19:16:00] - INFO: ###  max_position_embeddings = 512
[2022-10-13 19:16:00] - INFO: ###  type_vocab_size = 2
[2022-10-13 19:16:00] - INFO: ###  initializer_range = 0.02
[2022-10-13 19:16:00] - INFO: ###  directionality = bidi
[2022-10-13 19:16:00] - INFO: ###  pooler_fc_size = 768
[2022-10-13 19:16:00] - INFO: ###  pooler_num_attention_heads = 12
[2022-10-13 19:16:00] - INFO: ###  pooler_num_fc_layers = 3
[2022-10-13 19:16:00] - INFO: ###  pooler_size_per_head = 128
[2022-10-13 19:16:00] - INFO: ###  pooler_type = first_token_transform
[2022-10-13 19:16:01] - INFO: ## Ê≥®ÊÑèÔºåÊ≠£Âú®‰ΩøÁî®Êú¨Âú∞MyTransformer‰∏≠ÁöÑMyMultiHeadAttentionÂÆûÁé∞
[2022-10-13 19:16:03] - INFO: ## ÊàêÂäüËΩΩÂÖ•Â∑≤ÊúâÊ®°ÂûãÔºåËøõË°åÈ¢ÑÊµã......
[2022-10-13 19:16:04] - INFO: ÁºìÂ≠òÊñá‰ª∂ predict_None.pt ‰∏çÂ≠òÂú®ÔºåÈáçÊñ∞Â§ÑÁêÜÂπ∂ÁºìÂ≠òÔºÅ
[2022-10-13 19:16:22] - INFO: ÊàêÂäüÂØºÂÖ•BERTÈÖçÁΩÆÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/config.json
[2022-10-13 19:16:22] - INFO:  ### Â∞ÜÂΩìÂâçÈÖçÁΩÆÊâìÂç∞Âà∞Êó•ÂøóÊñá‰ª∂‰∏≠ 
[2022-10-13 19:16:22] - INFO: ###  project_dir = /home3/Tpandeng/mxy/BertWithPretrained
[2022-10-13 19:16:22] - INFO: ###  dataset_dir = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification
[2022-10-13 19:16:22] - INFO: ###  pretrained_model_dir = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese
[2022-10-13 19:16:22] - INFO: ###  vocab_path = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/vocab.txt
[2022-10-13 19:16:22] - INFO: ###  device = cuda:0
[2022-10-13 19:16:22] - INFO: ###  train_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_train.txt
[2022-10-13 19:16:22] - INFO: ###  val_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_val.txt
[2022-10-13 19:16:22] - INFO: ###  test_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_test.txt
[2022-10-13 19:16:22] - INFO: ###  model_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/cache
[2022-10-13 19:16:22] - INFO: ###  logs_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/logs
[2022-10-13 19:16:22] - INFO: ###  split_sep = _!_
[2022-10-13 19:16:22] - INFO: ###  is_sample_shuffle = True
[2022-10-13 19:16:22] - INFO: ###  batch_size = 64
[2022-10-13 19:16:22] - INFO: ###  max_sen_len = None
[2022-10-13 19:16:22] - INFO: ###  num_labels = 15
[2022-10-13 19:16:22] - INFO: ###  epochs = 1
[2022-10-13 19:16:22] - INFO: ###  model_val_per_epoch = 2
[2022-10-13 19:16:22] - INFO: ###  vocab_size = 21128
[2022-10-13 19:16:22] - INFO: ###  hidden_size = 768
[2022-10-13 19:16:22] - INFO: ###  num_hidden_layers = 12
[2022-10-13 19:16:22] - INFO: ###  num_attention_heads = 12
[2022-10-13 19:16:22] - INFO: ###  hidden_act = gelu
[2022-10-13 19:16:22] - INFO: ###  intermediate_size = 3072
[2022-10-13 19:16:22] - INFO: ###  pad_token_id = 0
[2022-10-13 19:16:22] - INFO: ###  hidden_dropout_prob = 0.1
[2022-10-13 19:16:22] - INFO: ###  attention_probs_dropout_prob = 0.1
[2022-10-13 19:16:22] - INFO: ###  max_position_embeddings = 512
[2022-10-13 19:16:22] - INFO: ###  type_vocab_size = 2
[2022-10-13 19:16:22] - INFO: ###  initializer_range = 0.02
[2022-10-13 19:16:22] - INFO: ###  directionality = bidi
[2022-10-13 19:16:22] - INFO: ###  pooler_fc_size = 768
[2022-10-13 19:16:22] - INFO: ###  pooler_num_attention_heads = 12
[2022-10-13 19:16:22] - INFO: ###  pooler_num_fc_layers = 3
[2022-10-13 19:16:22] - INFO: ###  pooler_size_per_head = 128
[2022-10-13 19:16:22] - INFO: ###  pooler_type = first_token_transform
[2022-10-13 19:16:24] - INFO: ## Ê≥®ÊÑèÔºåÊ≠£Âú®‰ΩøÁî®Êú¨Âú∞MyTransformer‰∏≠ÁöÑMyMultiHeadAttentionÂÆûÁé∞
[2022-10-13 19:16:26] - INFO: ## ÊàêÂäüËΩΩÂÖ•Â∑≤ÊúâÊ®°ÂûãÔºåËøõË°åÈ¢ÑÊµã......
[2022-10-13 19:16:26] - INFO: ÁºìÂ≠òÊñá‰ª∂ predict_None.pt ‰∏çÂ≠òÂú®ÔºåÈáçÊñ∞Â§ÑÁêÜÂπ∂ÁºìÂ≠òÔºÅ
[2022-10-13 19:17:19] - INFO: ÊàêÂäüÂØºÂÖ•BERTÈÖçÁΩÆÊñá‰ª∂ /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/config.json
[2022-10-13 19:17:19] - INFO:  ### Â∞ÜÂΩìÂâçÈÖçÁΩÆÊâìÂç∞Âà∞Êó•ÂøóÊñá‰ª∂‰∏≠ 
[2022-10-13 19:17:19] - INFO: ###  project_dir = /home3/Tpandeng/mxy/BertWithPretrained
[2022-10-13 19:17:19] - INFO: ###  dataset_dir = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification
[2022-10-13 19:17:19] - INFO: ###  pretrained_model_dir = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese
[2022-10-13 19:17:19] - INFO: ###  vocab_path = /home3/Tpandeng/mxy/BertWithPretrained/bert_base_chinese/vocab.txt
[2022-10-13 19:17:19] - INFO: ###  device = cuda:0
[2022-10-13 19:17:19] - INFO: ###  train_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_train.txt
[2022-10-13 19:17:19] - INFO: ###  val_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_val.txt
[2022-10-13 19:17:19] - INFO: ###  test_file_path = /home3/Tpandeng/mxy/BertWithPretrained/data/SingleSentenceClassification/toutiao_test.txt
[2022-10-13 19:17:19] - INFO: ###  model_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/cache
[2022-10-13 19:17:19] - INFO: ###  logs_save_dir = /home3/Tpandeng/mxy/BertWithPretrained/logs
[2022-10-13 19:17:19] - INFO: ###  split_sep = _!_
[2022-10-13 19:17:19] - INFO: ###  is_sample_shuffle = True
[2022-10-13 19:17:19] - INFO: ###  batch_size = 64
[2022-10-13 19:17:19] - INFO: ###  max_sen_len = None
[2022-10-13 19:17:19] - INFO: ###  num_labels = 15
[2022-10-13 19:17:19] - INFO: ###  epochs = 1
[2022-10-13 19:17:19] - INFO: ###  model_val_per_epoch = 2
[2022-10-13 19:17:19] - INFO: ###  vocab_size = 21128
[2022-10-13 19:17:19] - INFO: ###  hidden_size = 768
[2022-10-13 19:17:19] - INFO: ###  num_hidden_layers = 12
[2022-10-13 19:17:19] - INFO: ###  num_attention_heads = 12
[2022-10-13 19:17:19] - INFO: ###  hidden_act = gelu
[2022-10-13 19:17:19] - INFO: ###  intermediate_size = 3072
[2022-10-13 19:17:19] - INFO: ###  pad_token_id = 0
[2022-10-13 19:17:19] - INFO: ###  hidden_dropout_prob = 0.1
[2022-10-13 19:17:19] - INFO: ###  attention_probs_dropout_prob = 0.1
[2022-10-13 19:17:19] - INFO: ###  max_position_embeddings = 512
[2022-10-13 19:17:19] - INFO: ###  type_vocab_size = 2
[2022-10-13 19:17:19] - INFO: ###  initializer_range = 0.02
[2022-10-13 19:17:19] - INFO: ###  directionality = bidi
[2022-10-13 19:17:19] - INFO: ###  pooler_fc_size = 768
[2022-10-13 19:17:19] - INFO: ###  pooler_num_attention_heads = 12
[2022-10-13 19:17:19] - INFO: ###  pooler_num_fc_layers = 3
[2022-10-13 19:17:19] - INFO: ###  pooler_size_per_head = 128
[2022-10-13 19:17:19] - INFO: ###  pooler_type = first_token_transform
[2022-10-13 19:17:21] - INFO: ## Ê≥®ÊÑèÔºåÊ≠£Âú®‰ΩøÁî®Êú¨Âú∞MyTransformer‰∏≠ÁöÑMyMultiHeadAttentionÂÆûÁé∞
[2022-10-13 19:17:23] - INFO: ## ÊàêÂäüËΩΩÂÖ•Â∑≤ÊúâÊ®°ÂûãÔºåËøõË°åÈ¢ÑÊµã......
[2022-10-13 19:17:23] - INFO: ÁºìÂ≠òÊñá‰ª∂ predict_None.pt Â≠òÂú®ÔºåÁõ¥Êé•ËΩΩÂÖ•ÁºìÂ≠òÊñá‰ª∂ÔºÅ
